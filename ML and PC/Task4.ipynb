{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 581)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "file_path = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes\\2_speech_true_Ofen_aus.npy\"\n",
    "file_path1 = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes\\3_speech_true_Radio_an.npy\"\n",
    "file_np = np.load(file_path)\n",
    "np.shape(file_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 1109)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_np1 = np.load(file_path1)\n",
    "np.shape(file_np1)\n",
    "\n",
    "development_scenes_path = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes\"\n",
    "annotations_path = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scene_annotations.csv\"\n",
    "development_scenes_csv = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "development_scenes_path = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes\"\n",
    "annotations_path = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scene_annotations.csv\"\n",
    "development_scenes_csv = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\development_scenes\\development_scenes.csv\"\n",
    "\n",
    "\n",
    "# Load CSV files\n",
    "annotations = pd.read_csv(annotations_path)\n",
    "development_scenes = pd.read_csv(development_scenes_csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of feature arrays: 1437\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    file_path = os.path.join(development_scenes_path, f\"{row['filename']}.npy\")\n",
    "    if os.path.exists(file_path):\n",
    "        data = np.load(file_path)\n",
    "        if data.shape[1] > max_length:\n",
    "            max_length = data.shape[1]\n",
    "\n",
    "print(f\"Maximum length of feature arrays: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1190, 251475)\n",
      "Labels shape: (1190,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Extract features and labels with padding\n",
    "for index, row in annotations.iterrows():\n",
    "    file_path = os.path.join(development_scenes_path, f\"{row['filename']}.npy\")\n",
    "    if os.path.exists(file_path):\n",
    "        data = np.load(file_path)\n",
    "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
    "        features.append(padded_data.flatten())  # Flatten the padded 2D array to 1D\n",
    "        labels.append(row['command'])\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Alarm an       0.06      0.06      0.06        17\n",
      "      Alarm aus       0.00      0.00      0.00        14\n",
      "   Fernseher an       0.07      0.04      0.05        23\n",
      "  Fernseher aus       0.00      0.00      0.00        12\n",
      "     Heizung an       0.03      0.09      0.04        11\n",
      "    Heizung aus       0.05      0.06      0.06        16\n",
      "       Licht an       0.00      0.00      0.00        22\n",
      "      Licht aus       0.05      0.11      0.07        19\n",
      "     Lüftung an       0.00      0.00      0.00        13\n",
      "    Lüftung aus       0.00      0.00      0.00        17\n",
      "        Ofen an       0.00      0.00      0.00         9\n",
      "       Ofen aus       0.00      0.00      0.00        10\n",
      "       Radio an       0.00      0.00      0.00        11\n",
      "      Radio aus       0.06      0.06      0.06        17\n",
      " Staubsauger an       0.00      0.00      0.00        16\n",
      "Staubsauger aus       0.00      0.00      0.00        11\n",
      "\n",
      "       accuracy                           0.03       238\n",
      "      macro avg       0.02      0.03      0.02       238\n",
      "   weighted avg       0.02      0.03      0.02       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Heizung aus' 'Radio aus' 'Fernseher aus' 'Ofen aus' 'Ofen an'\n",
      " 'Heizung an' 'Alarm an' 'Staubsauger an' 'Heizung an' 'Heizung an'\n",
      " 'Staubsauger aus' 'Heizung an' 'Heizung an' 'Licht aus' 'Fernseher an'\n",
      " 'Radio aus' 'Heizung an' 'Alarm aus' 'Lüftung an' 'Heizung aus'\n",
      " 'Radio aus' 'Licht aus' 'Alarm an' 'Staubsauger aus' 'Radio aus'\n",
      " 'Licht aus' 'Licht an' 'Alarm an' 'Fernseher an' 'Lüftung an' 'Licht aus'\n",
      " 'Lüftung aus' 'Heizung an' 'Radio an' 'Heizung aus' 'Licht aus'\n",
      " 'Heizung an' 'Heizung aus' 'Heizung an' 'Licht an' 'Lüftung an'\n",
      " 'Heizung aus' 'Heizung an' 'Licht aus' 'Alarm an' 'Radio an' 'Radio an'\n",
      " 'Radio an' 'Alarm an' 'Licht aus' 'Licht aus' 'Fernseher an'\n",
      " 'Fernseher an' 'Ofen aus' 'Radio an' 'Ofen an' 'Licht an'\n",
      " 'Staubsauger aus' 'Fernseher an' 'Radio aus' 'Alarm aus' 'Licht aus'\n",
      " 'Alarm an' 'Heizung an' 'Staubsauger an' 'Heizung aus' 'Heizung an'\n",
      " 'Licht aus' 'Licht aus' 'Fernseher aus' 'Heizung an' 'Heizung aus'\n",
      " 'Licht aus' 'Lüftung aus' 'Lüftung aus' 'Radio an' 'Staubsauger an'\n",
      " 'Radio an' 'Alarm an' 'Fernseher an' 'Heizung an' 'Licht aus'\n",
      " 'Staubsauger an' 'Radio an' 'Staubsauger aus' 'Heizung aus' 'Alarm an'\n",
      " 'Staubsauger aus' 'Fernseher an' 'Licht an' 'Licht an' 'Staubsauger aus'\n",
      " 'Heizung an' 'Licht aus' 'Radio aus' 'Heizung an' 'Licht aus' 'Radio aus'\n",
      " 'Staubsauger an' 'Radio aus' 'Fernseher an' 'Alarm an' 'Heizung aus'\n",
      " 'Radio aus' 'Heizung an' 'Heizung an' 'Licht an' 'Licht an' 'Heizung an'\n",
      " 'Licht aus' 'Ofen an' 'Radio aus' 'Alarm aus' 'Licht aus' 'Heizung aus'\n",
      " 'Radio an' 'Heizung an' 'Staubsauger aus' 'Fernseher an' 'Licht aus'\n",
      " 'Heizung an' 'Heizung aus' 'Radio an' 'Heizung aus' 'Ofen aus'\n",
      " 'Heizung an' 'Heizung an' 'Heizung an' 'Heizung aus' 'Heizung aus'\n",
      " 'Licht an' 'Radio an' 'Heizung an' 'Licht aus' 'Lüftung aus' 'Alarm an'\n",
      " 'Fernseher an' 'Heizung aus' 'Radio aus' 'Licht aus' 'Alarm an'\n",
      " 'Radio an' 'Heizung aus' 'Alarm aus' 'Licht an' 'Heizung an' 'Ofen aus'\n",
      " 'Alarm aus' 'Ofen an' 'Fernseher an' 'Radio aus' 'Radio an' 'Licht an'\n",
      " 'Licht an' 'Staubsauger an' 'Licht aus' 'Licht aus' 'Heizung an'\n",
      " 'Heizung aus' 'Heizung an' 'Alarm aus' 'Licht aus' 'Licht an' 'Licht aus'\n",
      " 'Alarm an' 'Fernseher an' 'Licht aus' 'Radio an' 'Licht aus' 'Lüftung an'\n",
      " 'Staubsauger aus' 'Staubsauger aus' 'Licht an' 'Staubsauger an'\n",
      " 'Staubsauger aus' 'Ofen an' 'Lüftung an' 'Ofen aus' 'Alarm an'\n",
      " 'Alarm aus' 'Alarm an' 'Alarm an' 'Alarm an' 'Licht aus' 'Licht aus'\n",
      " 'Lüftung aus' 'Ofen aus' 'Heizung an' 'Radio an' 'Heizung an' 'Licht aus'\n",
      " 'Heizung an' 'Radio aus' 'Fernseher an' 'Ofen an' 'Licht aus' 'Radio aus'\n",
      " 'Radio an' 'Licht aus' 'Heizung an' 'Staubsauger aus' 'Heizung aus'\n",
      " 'Radio aus' 'Alarm an' 'Alarm aus' 'Licht an' 'Licht aus' 'Radio an'\n",
      " 'Alarm aus' 'Lüftung aus' 'Ofen aus' 'Licht aus' 'Heizung aus'\n",
      " 'Radio aus' 'Radio an' 'Licht aus' 'Lüftung an' 'Ofen an' 'Lüftung an'\n",
      " 'Ofen aus' 'Heizung an' 'Licht aus' 'Heizung aus' 'Licht an' 'Alarm an'\n",
      " 'Ofen aus' 'Staubsauger aus' 'Fernseher an' 'Staubsauger an'\n",
      " 'Lüftung aus' 'Licht aus' 'Licht aus' 'Heizung an' 'Licht aus'\n",
      " 'Licht aus' 'Lüftung aus' 'Fernseher an' 'Heizung an']\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (1594419, 174, 1), Shape y_train: (1594419, 21)\n",
      "Shape X_valid: (398605, 174, 1), Shape y_valid: (398605, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = r'C:\\Users\\azatv\\Jupyter\\JupyterProjects\\ML and PC\\new_df1.4.csv'\n",
    "df = pd.read_csv(dataset_path, index_col=0)\n",
    "\n",
    "X = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape data to fit the CNN + LSTM model (batch_size, timesteps, features)\n",
    "X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_scaled, y_onehot, test_size=0.2, stratify=y_onehot, random_state=42)\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}, Shape y_train: {y_train.shape}\")\n",
    "print(f\"Shape X_valid: {X_valid.shape}, Shape y_valid: {y_valid.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 172, 32)           128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 86, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 86, 32)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 84, 64)            6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 42, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 42, 64)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 40, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 20, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2560)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               327808    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,557\n",
      "Trainable params: 361,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 170, 6)            36        \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 170, 6)           24        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 170, 6)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 85, 6)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 81, 16)            496       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 81, 16)           64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 81, 16)            0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 40, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 640)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 640)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                10256     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                357       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,297\n",
      "Trainable params: 11,221\n",
      "Non-trainable params: 76\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, LeakyReLU, Flatten, Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first convolution layer\n",
    "model.add(Conv1D(6, kernel_size=5, strides=1, padding='valid', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "\n",
    "# Add the second convolution layer\n",
    "model.add(Conv1D(16, kernel_size=5, strides=1, padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "\n",
    "# # Add the second convolution layer\n",
    "# model.add(Conv1D(32, kernel_size=5, strides=1, padding='valid'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add dropout\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add the final dense layer\n",
    "model.add(Dense(16, activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dense(y_onehot.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 172, 64)           256       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 172, 64)          256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 86, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 86, 64)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 84, 128)           24704     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 84, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 42, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 42, 128)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 40, 256)           98560     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 20, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20, 256)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 18, 512)           393728    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 18, 512)          2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 9, 512)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9, 512)            0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                10773     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,891,669\n",
      "Trainable params: 2,889,749\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Flatten, Dense, Dropout\n",
    "\n",
    "def create_vggish_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # VGGish architecture\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(512, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Flatten before Dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "num_classes = y_onehot.shape[1]\n",
    "model = create_vggish_model(input_shape, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 2.5517 - accuracy: 0.2007 - val_loss: 2.0626 - val_accuracy: 0.3443\n",
      "Epoch 2/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 2.1298 - accuracy: 0.3240 - val_loss: 1.7928 - val_accuracy: 0.4209\n",
      "Epoch 3/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.9825 - accuracy: 0.3680 - val_loss: 1.7289 - val_accuracy: 0.4384\n",
      "Epoch 4/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.9252 - accuracy: 0.3850 - val_loss: 1.6784 - val_accuracy: 0.4539\n",
      "Epoch 5/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.8905 - accuracy: 0.3952 - val_loss: 1.6554 - val_accuracy: 0.4598\n",
      "Epoch 6/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.8672 - accuracy: 0.4012 - val_loss: 1.6543 - val_accuracy: 0.4647\n",
      "Epoch 7/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8517 - accuracy: 0.4060 - val_loss: 1.6373 - val_accuracy: 0.4690\n",
      "Epoch 8/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8391 - accuracy: 0.4095 - val_loss: 1.6112 - val_accuracy: 0.4730\n",
      "Epoch 9/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8270 - accuracy: 0.4127 - val_loss: 1.6355 - val_accuracy: 0.4720\n",
      "Epoch 10/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.8194 - accuracy: 0.4148 - val_loss: 1.6257 - val_accuracy: 0.4726\n",
      "Epoch 11/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8120 - accuracy: 0.4167 - val_loss: 1.6025 - val_accuracy: 0.4767\n",
      "Epoch 12/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.8060 - accuracy: 0.4187 - val_loss: 1.5979 - val_accuracy: 0.4802\n",
      "Epoch 13/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.8017 - accuracy: 0.4198 - val_loss: 1.5791 - val_accuracy: 0.4813\n",
      "Epoch 14/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7969 - accuracy: 0.4209 - val_loss: 1.5717 - val_accuracy: 0.4843\n",
      "Epoch 15/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7941 - accuracy: 0.4220 - val_loss: 1.5717 - val_accuracy: 0.4856\n",
      "Epoch 16/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7903 - accuracy: 0.4230 - val_loss: 1.5886 - val_accuracy: 0.4825\n",
      "Epoch 17/200\n",
      "24913/24913 [==============================] - 205s 8ms/step - loss: 1.7869 - accuracy: 0.4238 - val_loss: 1.5673 - val_accuracy: 0.4868\n",
      "Epoch 18/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7840 - accuracy: 0.4248 - val_loss: 1.5760 - val_accuracy: 0.4838\n",
      "Epoch 19/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7825 - accuracy: 0.4255 - val_loss: 1.5622 - val_accuracy: 0.4884\n",
      "Epoch 20/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7809 - accuracy: 0.4256 - val_loss: 1.5680 - val_accuracy: 0.4883\n",
      "Epoch 21/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7789 - accuracy: 0.4264 - val_loss: 1.5632 - val_accuracy: 0.4888\n",
      "Epoch 22/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7776 - accuracy: 0.4268 - val_loss: 1.5550 - val_accuracy: 0.4901\n",
      "Epoch 23/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7763 - accuracy: 0.4271 - val_loss: 1.5611 - val_accuracy: 0.4891\n",
      "Epoch 24/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7731 - accuracy: 0.4281 - val_loss: 1.5685 - val_accuracy: 0.4858\n",
      "Epoch 25/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7741 - accuracy: 0.4277 - val_loss: 1.5614 - val_accuracy: 0.4885\n",
      "Epoch 26/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7721 - accuracy: 0.4283 - val_loss: 1.5640 - val_accuracy: 0.4897\n",
      "Epoch 27/200\n",
      "24913/24913 [==============================] - 193s 8ms/step - loss: 1.7741 - accuracy: 0.4284 - val_loss: 1.5599 - val_accuracy: 0.4884\n",
      "Epoch 28/200\n",
      "24913/24913 [==============================] - 193s 8ms/step - loss: 1.7707 - accuracy: 0.4281 - val_loss: 1.5686 - val_accuracy: 0.4881\n",
      "Epoch 29/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7714 - accuracy: 0.4286 - val_loss: 1.5758 - val_accuracy: 0.4871\n",
      "Epoch 30/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7715 - accuracy: 0.4284 - val_loss: 1.5610 - val_accuracy: 0.4893\n",
      "Epoch 31/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7710 - accuracy: 0.4294 - val_loss: 1.5621 - val_accuracy: 0.4882\n",
      "Epoch 32/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7702 - accuracy: 0.4291 - val_loss: 1.5509 - val_accuracy: 0.4907\n",
      "Epoch 33/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7693 - accuracy: 0.4300 - val_loss: 1.5611 - val_accuracy: 0.4906\n",
      "Epoch 34/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7691 - accuracy: 0.4292 - val_loss: 1.5574 - val_accuracy: 0.4895\n",
      "Epoch 35/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7689 - accuracy: 0.4298 - val_loss: 1.5519 - val_accuracy: 0.4919\n",
      "Epoch 36/200\n",
      "24913/24913 [==============================] - 201s 8ms/step - loss: 1.7681 - accuracy: 0.4296 - val_loss: 1.5551 - val_accuracy: 0.4909\n",
      "Epoch 37/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7684 - accuracy: 0.4299 - val_loss: 1.5483 - val_accuracy: 0.4918\n",
      "Epoch 38/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7689 - accuracy: 0.4295 - val_loss: 1.5560 - val_accuracy: 0.4905\n",
      "Epoch 39/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7698 - accuracy: 0.4294 - val_loss: 1.5421 - val_accuracy: 0.4944\n",
      "Epoch 40/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7685 - accuracy: 0.4295 - val_loss: 1.5465 - val_accuracy: 0.4916\n",
      "Epoch 41/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7681 - accuracy: 0.4295 - val_loss: 1.5407 - val_accuracy: 0.4936\n",
      "Epoch 42/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7689 - accuracy: 0.4294 - val_loss: 1.5508 - val_accuracy: 0.4914\n",
      "Epoch 43/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7704 - accuracy: 0.4291 - val_loss: 1.5686 - val_accuracy: 0.4878\n",
      "Epoch 44/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7698 - accuracy: 0.4290 - val_loss: 1.5451 - val_accuracy: 0.4922\n",
      "Epoch 45/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7685 - accuracy: 0.4298 - val_loss: 1.5574 - val_accuracy: 0.4901\n",
      "Epoch 46/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7690 - accuracy: 0.4297 - val_loss: 1.5482 - val_accuracy: 0.4920\n",
      "Epoch 47/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7691 - accuracy: 0.4299 - val_loss: 1.5451 - val_accuracy: 0.4922\n",
      "Epoch 48/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7698 - accuracy: 0.4298 - val_loss: 1.5472 - val_accuracy: 0.4936\n",
      "Epoch 49/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7698 - accuracy: 0.4296 - val_loss: 1.5399 - val_accuracy: 0.4937\n",
      "Epoch 50/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7717 - accuracy: 0.4290 - val_loss: 1.5513 - val_accuracy: 0.4916\n",
      "Epoch 51/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7705 - accuracy: 0.4290 - val_loss: 1.5467 - val_accuracy: 0.4923\n",
      "Epoch 52/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7715 - accuracy: 0.4289 - val_loss: 1.5516 - val_accuracy: 0.4908\n",
      "Epoch 53/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7705 - accuracy: 0.4292 - val_loss: 1.5584 - val_accuracy: 0.4904\n",
      "Epoch 54/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7723 - accuracy: 0.4289 - val_loss: 1.5505 - val_accuracy: 0.4907\n",
      "Epoch 55/200\n",
      "24913/24913 [==============================] - 202s 8ms/step - loss: 1.7718 - accuracy: 0.4292 - val_loss: 1.5457 - val_accuracy: 0.4929\n",
      "Epoch 56/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7733 - accuracy: 0.4285 - val_loss: 1.5413 - val_accuracy: 0.4942\n",
      "Epoch 57/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7740 - accuracy: 0.4284 - val_loss: 1.5350 - val_accuracy: 0.4957\n",
      "Epoch 58/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.7730 - accuracy: 0.4284 - val_loss: 1.5619 - val_accuracy: 0.4908\n",
      "Epoch 59/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7748 - accuracy: 0.4280 - val_loss: 1.5648 - val_accuracy: 0.4897\n",
      "Epoch 60/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7736 - accuracy: 0.4290 - val_loss: 1.5415 - val_accuracy: 0.4933\n",
      "Epoch 61/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7755 - accuracy: 0.4280 - val_loss: 1.5594 - val_accuracy: 0.4915\n",
      "Epoch 62/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7757 - accuracy: 0.4281 - val_loss: 1.5522 - val_accuracy: 0.4931\n",
      "Epoch 63/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7763 - accuracy: 0.4276 - val_loss: 1.5492 - val_accuracy: 0.4943\n",
      "Epoch 64/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7767 - accuracy: 0.4278 - val_loss: 1.5918 - val_accuracy: 0.4849\n",
      "Epoch 65/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7769 - accuracy: 0.4277 - val_loss: 1.5648 - val_accuracy: 0.4934\n",
      "Epoch 66/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7782 - accuracy: 0.4273 - val_loss: 1.5457 - val_accuracy: 0.4946\n",
      "Epoch 67/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7769 - accuracy: 0.4276 - val_loss: 1.5415 - val_accuracy: 0.4941\n",
      "Epoch 68/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7781 - accuracy: 0.4273 - val_loss: 1.5672 - val_accuracy: 0.4899\n",
      "Epoch 69/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7770 - accuracy: 0.4280 - val_loss: 1.5459 - val_accuracy: 0.4938\n",
      "Epoch 70/200\n",
      "24913/24913 [==============================] - 201s 8ms/step - loss: 1.7777 - accuracy: 0.4276 - val_loss: 1.5528 - val_accuracy: 0.4922\n",
      "Epoch 71/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7779 - accuracy: 0.4276 - val_loss: 1.5512 - val_accuracy: 0.4892\n",
      "Epoch 72/200\n",
      "24913/24913 [==============================] - 208s 8ms/step - loss: 1.7776 - accuracy: 0.4275 - val_loss: 1.5493 - val_accuracy: 0.4900\n",
      "Epoch 73/200\n",
      "24913/24913 [==============================] - 205s 8ms/step - loss: 1.7789 - accuracy: 0.4268 - val_loss: 1.5530 - val_accuracy: 0.4921\n",
      "Epoch 74/200\n",
      "24913/24913 [==============================] - 202s 8ms/step - loss: 1.7806 - accuracy: 0.4268 - val_loss: 1.5587 - val_accuracy: 0.4910\n",
      "Epoch 75/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7814 - accuracy: 0.4265 - val_loss: 1.5580 - val_accuracy: 0.4900\n",
      "Epoch 76/200\n",
      "24913/24913 [==============================] - 202s 8ms/step - loss: 1.7812 - accuracy: 0.4268 - val_loss: 1.5693 - val_accuracy: 0.4872\n",
      "Epoch 77/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7816 - accuracy: 0.4266 - val_loss: 1.5580 - val_accuracy: 0.4918\n",
      "Epoch 78/200\n",
      "24913/24913 [==============================] - 202s 8ms/step - loss: 1.7810 - accuracy: 0.4270 - val_loss: 1.5463 - val_accuracy: 0.4945\n",
      "Epoch 79/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7824 - accuracy: 0.4267 - val_loss: 1.5595 - val_accuracy: 0.4907\n",
      "Epoch 80/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7820 - accuracy: 0.4263 - val_loss: 1.5611 - val_accuracy: 0.4924\n",
      "Epoch 81/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.7833 - accuracy: 0.4261 - val_loss: 1.5653 - val_accuracy: 0.4917\n",
      "Epoch 82/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7826 - accuracy: 0.4259 - val_loss: 1.5447 - val_accuracy: 0.4952\n",
      "Epoch 83/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7809 - accuracy: 0.4268 - val_loss: 1.5693 - val_accuracy: 0.4916\n",
      "Epoch 84/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7828 - accuracy: 0.4260 - val_loss: 1.5982 - val_accuracy: 0.4829\n",
      "Epoch 85/200\n",
      "24913/24913 [==============================] - 205s 8ms/step - loss: 1.7834 - accuracy: 0.4263 - val_loss: 1.5959 - val_accuracy: 0.4868\n",
      "Epoch 86/200\n",
      "24913/24913 [==============================] - 199s 8ms/step - loss: 1.7862 - accuracy: 0.4252 - val_loss: 1.5835 - val_accuracy: 0.4958\n",
      "Epoch 87/200\n",
      "24913/24913 [==============================] - 223s 9ms/step - loss: 1.7835 - accuracy: 0.4261 - val_loss: 1.5628 - val_accuracy: 0.4898\n",
      "Epoch 88/200\n",
      "24913/24913 [==============================] - 206s 8ms/step - loss: 1.7837 - accuracy: 0.4259 - val_loss: 1.5582 - val_accuracy: 0.4900\n",
      "Epoch 89/200\n",
      "24913/24913 [==============================] - 200s 8ms/step - loss: 1.7858 - accuracy: 0.4261 - val_loss: 1.5499 - val_accuracy: 0.4944\n",
      "Epoch 90/200\n",
      "24913/24913 [==============================] - 204s 8ms/step - loss: 1.7857 - accuracy: 0.4254 - val_loss: 1.5928 - val_accuracy: 0.4904\n",
      "Epoch 91/200\n",
      "24913/24913 [==============================] - 194s 8ms/step - loss: 1.7863 - accuracy: 0.4254 - val_loss: 1.5579 - val_accuracy: 0.4927\n",
      "Epoch 92/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7866 - accuracy: 0.4253 - val_loss: 1.5730 - val_accuracy: 0.4891\n",
      "Epoch 93/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7879 - accuracy: 0.4246 - val_loss: 1.5622 - val_accuracy: 0.4906\n",
      "Epoch 94/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7891 - accuracy: 0.4246 - val_loss: 1.5650 - val_accuracy: 0.4888\n",
      "Epoch 95/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7888 - accuracy: 0.4245 - val_loss: 1.5595 - val_accuracy: 0.4897\n",
      "Epoch 96/200\n",
      "24913/24913 [==============================] - 188s 8ms/step - loss: 1.7884 - accuracy: 0.4249 - val_loss: 1.5825 - val_accuracy: 0.4917\n",
      "Epoch 97/200\n",
      "24913/24913 [==============================] - 189s 8ms/step - loss: 1.7897 - accuracy: 0.4241 - val_loss: 1.5846 - val_accuracy: 0.4856\n",
      "Epoch 98/200\n",
      "24913/24913 [==============================] - 191s 8ms/step - loss: 1.7897 - accuracy: 0.4247 - val_loss: 1.5734 - val_accuracy: 0.4869\n",
      "Epoch 99/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7920 - accuracy: 0.4243 - val_loss: 1.5657 - val_accuracy: 0.4878\n",
      "Epoch 100/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7925 - accuracy: 0.4238 - val_loss: 1.5560 - val_accuracy: 0.4899\n",
      "Epoch 101/200\n",
      "24913/24913 [==============================] - 193s 8ms/step - loss: 1.7919 - accuracy: 0.4239 - val_loss: 1.5712 - val_accuracy: 0.4881\n",
      "Epoch 102/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7916 - accuracy: 0.4241 - val_loss: 1.5701 - val_accuracy: 0.4882\n",
      "Epoch 103/200\n",
      "24913/24913 [==============================] - 192s 8ms/step - loss: 1.7934 - accuracy: 0.4234 - val_loss: 1.5607 - val_accuracy: 0.4902\n",
      "Epoch 104/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7954 - accuracy: 0.4232 - val_loss: 1.5745 - val_accuracy: 0.4856\n",
      "Epoch 105/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7940 - accuracy: 0.4235 - val_loss: 1.5645 - val_accuracy: 0.4909\n",
      "Epoch 106/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7942 - accuracy: 0.4233 - val_loss: 1.5635 - val_accuracy: 0.4902\n",
      "Epoch 107/200\n",
      "24913/24913 [==============================] - 201s 8ms/step - loss: 1.7947 - accuracy: 0.4234 - val_loss: 1.5672 - val_accuracy: 0.4909\n",
      "Epoch 108/200\n",
      "24913/24913 [==============================] - 208s 8ms/step - loss: 1.7930 - accuracy: 0.4240 - val_loss: 1.5529 - val_accuracy: 0.4911\n",
      "Epoch 109/200\n",
      "24913/24913 [==============================] - 201s 8ms/step - loss: 1.7951 - accuracy: 0.4235 - val_loss: 1.5648 - val_accuracy: 0.4889\n",
      "Epoch 110/200\n",
      "24913/24913 [==============================] - 198s 8ms/step - loss: 1.7957 - accuracy: 0.4233 - val_loss: 1.5723 - val_accuracy: 0.4874\n",
      "Epoch 111/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7973 - accuracy: 0.4225 - val_loss: 1.5632 - val_accuracy: 0.4900\n",
      "Epoch 112/200\n",
      "24913/24913 [==============================] - 195s 8ms/step - loss: 1.7952 - accuracy: 0.4232 - val_loss: 1.5533 - val_accuracy: 0.4916\n",
      "Epoch 113/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7949 - accuracy: 0.4236 - val_loss: 1.5515 - val_accuracy: 0.4924\n",
      "Epoch 114/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7962 - accuracy: 0.4231 - val_loss: 1.5669 - val_accuracy: 0.4897\n",
      "Epoch 115/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7974 - accuracy: 0.4226 - val_loss: 1.5666 - val_accuracy: 0.4895\n",
      "Epoch 116/200\n",
      "24913/24913 [==============================] - 196s 8ms/step - loss: 1.7985 - accuracy: 0.4227 - val_loss: 1.5577 - val_accuracy: 0.4920\n",
      "Epoch 117/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8019 - accuracy: 0.4219 - val_loss: 1.5589 - val_accuracy: 0.4891\n",
      "Epoch 118/200\n",
      "24913/24913 [==============================] - 197s 8ms/step - loss: 1.8005 - accuracy: 0.4215 - val_loss: 1.5665 - val_accuracy: 0.4898\n",
      "Epoch 119/200\n",
      "24913/24913 [==============================] - 203s 8ms/step - loss: 1.8023 - accuracy: 0.4214 - val_loss: 1.5564 - val_accuracy: 0.4883\n",
      "Epoch 120/200\n",
      "  932/24913 [>.............................] - ETA: 3:18 - loss: 1.7965 - accuracy: 0.4222"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_valid, y_valid), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "valid_loss, valid_accuracy = model.evaluate(X_valid, y_valid, verbose=0)\n",
    "print(f\"Validation Accuracy: {valid_accuracy*100:.2f}%\")\n",
    "\n",
    "model_path = 'improved_cnn_model.keras'\n",
    "model.save(model_path)\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "pred_probs = model.predict(X_valid)\n",
    "pred_labels = np.argmax(pred_probs, axis=1)\n",
    "true_labels = np.argmax(y_valid, axis=1)\n",
    "\n",
    "valid_accuracy = np.mean(pred_labels == true_labels)\n",
    "print(f\"Validation Accuracy: {valid_accuracy*100:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 172, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 86, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 86, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 86, 100)           66000     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 21)                2121      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 158,877\n",
      "Trainable params: 158,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 1D convolutional layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(100, activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model.add(LSTM(100, activation='tanh', recurrent_activation='sigmoid'))\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(y_onehot.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 174, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 174, 64)      256         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 174, 64)     256         ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 174, 64)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 87, 64)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 87, 64)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 87, 128)      24704       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 87, 128)     512         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 87, 128)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 43, 128)     0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 43, 128)      0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 43, 128)      49280       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 43, 128)     512         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 43, 128)      0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 43, 128)      49280       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 43, 128)     512         ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 43, 128)      0           ['dropout_1[0][0]',              \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 43, 128)      0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 21, 128)     0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 21, 128)      0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 21, 128)      49280       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 21, 128)     512         ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 21, 128)      0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 21, 128)      49280       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 21, 128)     512         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 21, 128)      0           ['dropout_2[0][0]',              \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 21, 128)      0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 10, 128)     0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 10, 128)      0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1280)         0           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          327936      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 21)           5397        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 558,229\n",
      "Trainable params: 556,821\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Activation, Add, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def residual_block(x, filters, kernel_size):\n",
    "    shortcut = x\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([shortcut, x])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Adding Residual Blocks\n",
    "x = residual_block(x, filters=128, kernel_size=3)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = residual_block(x, filters=128, kernel_size=3)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(y_onehot.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
