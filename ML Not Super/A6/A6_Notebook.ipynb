{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 6: Factor Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1 (20 Points): Prove the following Lemma</h2>\n",
    "\n",
    "Show that for any matrix $X$ and any differentiable scalar-valued function $f$ it holds that \n",
    "$$\n",
    "\\frac{\\partial f(X^\\top X)}{\\partial X} = 2{X}\\frac{\\partial f(X^\\top X)}{\\partial (X^\\top X)}.\n",
    "$$\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2 (25 Points): Derive GD for FA</h2>\n",
    "\n",
    "Construct the negative log-likelihood $\\ell(\\mathbf{U}, \\mathbf{\\Psi}) = -\\log\\mathcal{L}(\\mathbf{X}; \\mathbf{U}, \\mathbf{\\Psi})$, according to the FA model assumptions and show that its gradients with respect to the model parameters are \n",
    "$$\n",
    "\\nabla_{\\mathbf{U}} \\ell(\\mathbf{U}, \\mathbf{\\Psi}) = 2 \\mathbf{P}\\mathbf{U}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla_{\\mathbf{\\Psi}} \\ell(\\mathbf{U}, \\mathbf{\\Psi}) = \\operatorname{diag}(\\mathbf{P})\n",
    "$$\n",
    ", where \n",
    "$$\n",
    "\\mathbf{P} = \\nabla_{\\mathbf{U}\\mathbf{U}^\\top + \\mathbf{\\Psi}} \\ell(\\mathbf{U}, \\mathbf{\\Psi}) = \\frac12 \\mathbf{Q} - \\frac{1}{2n} \\mathbf{Q} \\mathbf{X}^\\top \\mathbf{X} \\mathbf{Q}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{Q} = (\\mathbf{U}\\mathbf{U}^\\top + \\mathbf{\\Psi})^{-1}.\n",
    "$$ \n",
    "\n",
    "Hint: use the fact that if $f(\\mathbf{A})=f(\\mathbf{A}^T)$ holds for a scalar function $f$ and a real square matrix $\\mathbf{A}$ it holds that $\\nabla_{\\mathbf{X}} f(\\mathbf{X}^\\top \\mathbf{X}) = 2 \\mathbf{X} \\nabla_{\\mathbf{X}^\\top \\mathbf{X}} f(\\mathbf{X}^\\top \\mathbf{X})$. \n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 3 (20 Points): Generate toy data for FA</h2>\n",
    "\n",
    "Generate an $n \\times m$ data matrix $\\mathbf{X}$, i.e. $n=100$ samples with $m=5$ features, according to the generative factor analysis model with $l=3$ factors. Further, fill $\\mathbf{U}$ with random integers from the set $\\{-3, \\dots, 3\\}$ and $\\mathbf{\\Psi}$ with random integers from the set $\\{1, \\dots, 3\\}$. To save memory, implement $\\mathbf{\\Psi}$ as a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 4 (20 Points): Implement GD for FA</h2>\n",
    "\n",
    "Use your results from the previous task to implement gradient descent on the negative log-likelihood for factor analysis. Initialize $\\mathbf{U}$ and $\\mathbf{\\Psi}$ randomly uniform in $[-0.1, 0.1]$. To speed up computations, use the matrix inversion lemma $(\\mathbf{UU}^\\top + \\mathbf{\\Psi})^{-1} = \\mathbf{\\Psi}^{-1} - \\mathbf{\\Psi}^{-1}\\mathbf{U}(\\mathbf{I} + \\mathbf{U^\\top \\mathbf{\\Psi}^{-1} \\mathbf{U}})^{-1}\\mathbf{U}^\\top \\mathbf{\\Psi}^{-1}$ wherever possible and adjust the learning rate properly. Visualize the learning progress over 100 update steps in terms of the loss function $\\ell(\\mathbf{U},\\mathbf{\\Psi})$ for 10 runs which differ only by initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 5 (20 Points): Implement EM for FA</h2>\n",
    "\n",
    "Implement expectation maximization for factor analysis. Use the same data and initialization scheme as in the previous task. Visualize the learning progress over 100 update steps in terms of the loss function $\\ell(\\mathbf{U},\\mathbf{\\Psi})$ for 10 runs which differ only by initialization. Compare your results to those in the previous exercise and interpret the ovserved differences. \n",
    "\n",
    "Use Speedups from lecture notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
