{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI I</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 4 &ndash; Logistic Regression as a Door Opener to Deep Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> Brandstetter, Schäfl, Winter, Schörgenhumer<br>\n",
    "<b>Date:</b> 21-11-2022\n",
    "\n",
    "This file is part of the \"Hands-on AI I\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u4_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u4_utils.py</code> need to be installed.\n",
    "\n",
    "In this notebook, you will need to install PyTorch. Follow the instructions on https://pytorch.org/get-started/locally/. Note that there is no need to also install the CUDA-version, the CPU-version is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import pre-defined utilities specific to this notebook.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mu4_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mu4\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Import additional utilities needed in this notebook.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\JupyterProjects\\U4\\u4_utils.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import pre-defined utilities specific to this notebook.\n",
    "import u4_utils as u4\n",
    "\n",
    "# Import additional utilities needed in this notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from itertools import islice\n",
    "from torchvision.datasets import MNIST\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u4.setup_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Module versions</h3>\n",
    "\n",
    "As mentioned in the introductory slides, specific minimum versions of Python itself as well as of used modules are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u4.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Linear regression</h1>\n",
    "\n",
    "Given a dataset $S$ with two variables, $x$ and $y$ (often termed <i>feature</i> and <i>target</i>):\n",
    "\n",
    "$$S = \\{(x_i, y_i)\\}_{i=1}^{n}\\quad{}\\text{with}\\quad{}x, y\\in{}\\mathbb{R}^1$$\n",
    "\n",
    "We want to find a linear model which best describes the relationship between $x$ and $y$. But before starting to experiment with such a dataset, we have to generate one in the first place. This can be done using <code>get_dataset(...)</code> from <code>u4_utils.py</code>.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter</th>\n",
    "        <th>Value (used in this notebook)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>num_pairs</code></td>\n",
    "        <td>50</td>\n",
    "        <td>amount of $(x, y)$ pairs to generate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>variance</code></td>\n",
    "        <td>0.05</td>\n",
    "        <td>variance within $y$ w.r.t. defining function</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Create dataset conisting of random (x, y) pairs.\n",
    "dataset = u4.get_dataset(\n",
    "    num_pairs=50,\n",
    "    variance=0.05\n",
    "    #coefficients=(0.241, 0.422)  # these are the true coefficients used for the data generation\n",
    ")\n",
    "\n",
    "sns.scatterplot(data=dataset, x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that $x$ and $y$ have a <i>linear</i> dependency. Therefore, we select a <i>linear</i> model $g$ with which we want to describe the dataset:\n",
    "\n",
    "$$g(x; w) = g(x; d, k) = d \\cdot x^0 + k \\cdot x^1 = d + k \\cdot x$$\n",
    "\n",
    "The function $g(x; d, k)$ will be an approximation of our true labels $y$, as there was an additional <i>variance</i> term involved when generating the features $\\{x_i\\}_{i=1}^{n}$. All in all there are $2$ unknown parameters $d$ and $k$ (termed <i>intercept</i>/<i>offset</i> and <i>slope</i>, respectively, or polynomial coefficients in the general case), which can be found in various ways. The most simple but rather impractical one is a <i>manual</i> search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define coefficients of an underlying linear model (here: a simple straight line, i.e., a polynomial of degree 1).\n",
    "coefficients = (\n",
    "    0.2,  # intercept\n",
    "    0.4   # slope\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Try out different coefficients (intercept and slope). Can you find optimal values for them?</li>\n",
    "        <li>Change the number of data pairs and the corresponding variance. Are the previously found optimal values still optimal?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Find optimal values</h2>\n",
    "We can automatically (analytically in the linear case) find the optimal values for our intercept $d$ and slope $k$. The idea is to set the parameters in a way that the <b>Mean Squared Error</b> between our linear model and our data pairs is minimized:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=0}^{n}{\\left(y_i - g(x_i; d, k)\\right)^2} = \\frac{1}{n}\\sum_{i=0}^{n}{\\left(y_i - g(x_i; w)\\right)^2}$$\n",
    "\n",
    "Luckily, there is a **closed-form solution** that only requires a matrix inversion (so we do not have to use iterative methods to minimize the MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=1  # just as above, use a polynomial of degree 1, i.e., a straight line\n",
    ")\n",
    "\n",
    "print(\"The coefficients minimizing the MSE are:\")\n",
    "for i, c in enumerate(coefficients):\n",
    "    print(f\"    coefficient {i}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Background information</h3>\n",
    "But why do we use the squared error and not, e.g., the absolute error? It turns out that <b>minimizing the squared error is equivalent to maximizing the likelihood P($y_i \\mid x_i$)</b> of observing our data (i.e., how plausible the label $y_i$ is given the sample $x_i$ and fixed model parameters). In practice, the negative log-likelihood is minimized. We can use the same minimization algorithm to calculate the optimal parameters for more complicated models.\n",
    "\n",
    "<br>Next, we load a new dataset by using <code>get_dataset_unknown(...)</code> from <code>u4_utils.py</code> and try to find the model degree which best describes the data.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter</th>\n",
    "        <th>Value (used in this notebook)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>num_pairs</code></td>\n",
    "        <td>50</td>\n",
    "        <td>amount of $(x, y)$ pairs to generate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>variance</code></td>\n",
    "        <td>0.9</td>\n",
    "        <td>variance within $y$ w.r.t. defining function</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>degree_max</code> (optional)</td>\n",
    "        <td>6 (default)</td>\n",
    "        <td>maximum degree of the random underlying polynomial which is used to generate the data</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Create dataset conisting of random (x, y) pairs.\n",
    "dataset = u4.get_dataset_unknown(\n",
    "    num_pairs=50,\n",
    "    variance=0.9\n",
    ")\n",
    "\n",
    "sns.scatterplot(data=dataset, x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=5\n",
    ")\n",
    "\n",
    "print(\"The coefficients minimizing the MSE are:\")\n",
    "for i, c in enumerate(coefficients):\n",
    "    print(f\"    coefficient {i}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Try out different degrees. Can you find a model which seems to be plausibly describing the data?</li>\n",
    "        <li>Change the upper bound of the randomly chosen degree in the function <code>get_dataset_unknown(...)</code> (parameter <code>degree_max</code>) and repeat the previous experiments. Is it always equally difficult to guess the degree of the polynomial?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Over- and underfitting revisited</h2>\n",
    "Next, we load a dataset with a known degree. We specify the number of samples, the variance of the Gaussian noise and the corresponding coefficients of the underlying model.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter</th>\n",
    "        <th>Value (used in this notebook)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>num_pairs</code></td>\n",
    "        <td>50</td>\n",
    "        <td>amount of $(x, y)$ pairs to generate</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>variance</code></td>\n",
    "        <td>0.25</td>\n",
    "        <td>variance within $y$ w.r.t. defining function</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td><code>coefficients</code></td>\n",
    "        <td>(0.41, 0.42, 0.43)</td>\n",
    "        <td>coefficients of the underlying polynomial which is used to generate the data</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "What happens if we fit a model with a higher degree polynomial on the data as was used in its generation process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Create dataset conisting of random (x, y) pairs.\n",
    "dataset = u4.get_dataset(\n",
    "    num_pairs=50,\n",
    "    variance=0.25,\n",
    "    coefficients=(0.41, 0.42, 0.43)  # polynomial of degree 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize mean squared error given a specific degree.\n",
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=10\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complicated model <b>overfits on the data</b>, desperately trying to describe the random noise. We assume that the model will perform badly on unseen data points. Therefore, we will decrease the model complexity to a degree of 2 again. We follow <b>Occams razor<b>:\n",
    "    \n",
    "<cite>The simplest solution is usually the best.</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize mean squared error given a specific degree.\n",
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=2\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Increase the amount of sample pairs and again fit the higher order polynomial. What happens with the resulting polynomial?</li>\n",
    "        <li>Change the variance to $0$ and fit a polynomial with degree $2$. Do the found coefficients look familiar?</li>\n",
    "        <li>Change the variance to $0$ and fit a polynomial with degree $5$. How do the additional coefficients look like?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Logistic regression</h1>\n",
    "\n",
    "Given a dataset $S$ with two variables, $x$ and $y$ (often termed <i>feature</i> and <i>label/class</i>):\n",
    "\n",
    "$$S = \\{(x_i, y_i)\\}_{i=1}^{n}\\quad{}\\text{with}\\quad{}x\\in{}\\mathbb{R}^1,\\;y\\in{}\\{0,1\\}$$\n",
    "\n",
    "We want to find a model which best describes the relationship between $x$ and $y$, i.e., a model which best <i>classifies</i> a sample $x_i$. But before starting to experiment with such a dataset, we have to generate one in the first place. This can be done using <code>get_dataset_logistic(...)</code> from <code>u4_utils.py</code>.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter</th>\n",
    "        <th>Value (used in this notebook)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>num_pairs</code></td>\n",
    "        <td>50</td>\n",
    "        <td>amount of $(x, y)$ pairs to generate</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Create dataset conisting of random (x, y) pairs.\n",
    "dataset = u4.get_dataset_logistic(\n",
    "    num_pairs=50\n",
    ")\n",
    "\n",
    "sns.scatterplot(data=dataset, x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply <i>linear regression</i> on a classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize mean squared error given a specific degree.\n",
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=1\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a higher order polynomial, the data is fit in a more reasonable way. Nonetheless, we already get the feeling, that this direction might not be the most optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize mean squared error given a specific degree.\n",
    "coefficients = u4.minimize_mse(\n",
    "    dataset=dataset,\n",
    "    degree=5\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined linear model.\n",
    "u4.plot_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instead use a method specifically designed for categorical data: <b>logistic regression</b>. In addition to our linear regression model, we apply the <b>logistic function</b> $\\sigma$ (sigmoid function) to the result of $g\\left(x; w{}\\right)$.\n",
    "\n",
    "$$\\sigma{}\\left(g\\left(x; w{}\\right)\\right) = \\frac{1}{1 + \\exp{}\\left({-g\\left(x; w{}\\right)}\\right)}$$\n",
    "\n",
    "First, let's define this function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.linspace(-5.0, 5.0, 100)\n",
    "\n",
    "ax = sns.lineplot(x=samples, y=sigmoid(samples))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>What is $\\sigma{}\\left(x\\right)$ for $x\\in{}\\{-2.5, 2.5, 10.0\\}$?</li>\n",
    "        <li>At which value $x$ is $\\sigma{}\\left(x\\right) = 0$?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic function, we can now efficiently perform classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Create dataset conisting of random (x, y) pairs.\n",
    "dataset = u4.get_dataset_logistic(\n",
    "    num_pairs=50\n",
    ")\n",
    "\n",
    "# Minimize cross-entropy loss.\n",
    "coefficients = u4.minimize_ce(\n",
    "    dataset=dataset,\n",
    "    iterations=5000,\n",
    "    learning_rate=10,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "# Plot data pairs as well as the defined logistic model.\n",
    "u4.plot_logistic_model(\n",
    "    dataset=dataset,\n",
    "    coefficients=coefficients\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first (<span style=\"color:rgb(70,104,164)\">blue</span>) line indicates the <b>probability of a sample $x_i$ being class 0</b>. The second (<span style=\"color:rgb(213,123,80)\">orange</span>) line now indicates the <b>probability of a sample $x_i$ being class 1</b>. Note that now there is **no closed-form solution** as in linear regression anymore. Due to the logistic function, our model becomes <i>non-linear</i> and its optimal parameters can <b>only be found via numerical methods</b> (i.e., no analytical solution, we have to use some iterative method instead). In our case, we are using <b>Gradient Descent</b> and instead of the sigmoid function we are actually using the <b>softmax function</b>, which is a general form of the sigmoid function that is able to handle multiple classes $K$:\n",
    "\n",
    "$$\\sigma{}\\left(g\\left(x; w{}\\right)\\right)_i = \\frac{\\exp{}\\left(g\\left(x; w{}\\right)\\right)_i}{\\sum_{j=1}^{K}{\\exp{}\\left({g\\left(x; w\\right)}\\right)_j}}$$\n",
    "\n",
    "and instead of minimizing the MSE loss (function `minimize_mse`), we are now minimizing the <b>Cross Entropy</b> loss (function `minimize_ce(...)`), which is designed for classification and for handling the output such as the one produced by the above softmax function (i.e., probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>What happens if you change the number of iterations and the learning rate?</li>\n",
    "        <li>What happens if you change the number of samples?</li>\n",
    "        <li>What happens if you set the learning rate too high?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the gradient descent algorithm like <b>a ball rolling down a hill</b>:\n",
    "- The <i>height</i> of the ball indicates the <i>loss</i> of the model when using the respective parameter value.\n",
    "- The <i>inertia</i> of the ball indicates the <i>momentum</i>, whereas the (initial) <i>learning rate</i> can be seen as the <i>speed</i> of the ball.\n",
    "- The <i>space</i> is specified by our parameters. Each model parameter adds a new dimension to the parameter space.\n",
    "\n",
    "The loss landscape is typically unknown when training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"resources/loss_landscape.png\" alt=\"loss landscape\" width=\"25%\">\n",
    "    <img src=\"resources/loss_landscape_local_minimum.png\" alt=\"loss landscape\" width=\"25%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Logistic regression on 2D data</h2>\n",
    "\n",
    "We now go to a slightly more complex dataset <code>DataSet_LR_a.csv</code>, where the feature vector $\\boldsymbol{x}$ consists of two features $x_1$, $x_2$. The labels are again $0$ and $1$ (i.e., a classification task). So let's get started by plotting the new data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from <*.csv> file.\n",
    "dataset = u4.get_dataset_from_csv(path='resources/DataSet_LR_a.csv')\n",
    "\n",
    "sns.scatterplot(data=dataset, x=\"x0\", y=\"x1\", hue=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting a model to the freshly loaded dataset, we have to split it up into <i>two</i> different data sets:\n",
    "- A <i>training</i> set comprising $50\\%$ of the dataset.\n",
    "- A <i>test</i> set comprising the remaining $50\\%$ of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Split dataset into a taining and a test set.\n",
    "dataset_train = dataset.sample(frac=0.5, replace=False, axis=0)\n",
    "dataset_test = dataset.drop(dataset_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the trained model now to our training set (<code>dataset_train</code>) and our test set (<code>dataset_test</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize cross-entropy loss.\n",
    "coefficients = u4.minimize_ce(\n",
    "    dataset=dataset_train,\n",
    "    iterations=5000,\n",
    "    learning_rate=10,\n",
    "    momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data pairs as well as the predictions according to the defined logistic model.\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "# Plot ground truth labels.\n",
    "axes[0].set_title(\"ground truth\")\n",
    "sns.scatterplot(data=dataset_train, x=\"x0\", y=\"x1\", hue=\"y\", ax=axes[0])\n",
    "\n",
    "# Compute predictions of trained logistic regression model w.r.t. the training data.\n",
    "predictions = u4.predict_logistic(dataset_train.drop(columns=\"y\"), coefficients)\n",
    "\n",
    "# Plot predicted labels.\n",
    "axes[1].set_title(\"prediction\")\n",
    "sns.scatterplot(data=dataset_train.assign(y=predictions), x=\"x0\", y=\"x1\", hue=\"y\", ax=axes[1])\n",
    "plt.show()\n",
    "\n",
    "# Compute accuracy given the predicted and the true training labels.\n",
    "accuracy_train = (predictions == dataset_train[\"y\"]).mean()\n",
    "print(f\"Accuracy on training set: {accuracy_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Plot the test dataset. Does the resulting plot resemble the one of the training data?</li>\n",
    "        <li>Apply the resulting logistic regression model to the test data. How well does our model perform?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">PyTorch</h1>\n",
    "\n",
    "[PyTorch](https://pytorch.org) is a framework to support scientific work and develop corresponding applications. It is typically used to setup, train and apply <i>Artificial Neural Networks (ANNs)</i>. The power of PyTorch (as in several other machine learning centered frameworks) comes with <b>automatic differentation</b>. In this notebook, we will only need the concepts of <b>tensors</b> and of <b>automatic differentation</b>. For this purpose, let us perform some straighforward calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Generate 3 random numbers according to the uniform distribution U for the interval [0, 1).\n",
    "x = torch.rand(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the square of the previously generated random numbers.\n",
    "y = x ** 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the (natural) logarithm of the sum of two tensors.\n",
    "z = torch.log(x + y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch organizes sets of values in tensors. Tensors can have arbritary numbers of dimensions.\n",
    "- A tensor with $0$ dimensions is called a <i>scalar</i>.\n",
    "- A tensor with $1$ dimension is called a <i>vector</i>.\n",
    "- A tensor with $2$ dimensions is called a <i>matrix</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the shape of the resulting tensor.\n",
    "shape = (\n",
    "    2,  # number of elements in level 0; each element has the shape of the remaining levels; here: (3, 4)\n",
    "    3,  # number of elements in level 1; each element has the shape of the remaining levels; here: (4,)\n",
    "    4   # number of elements in level 3\n",
    ")\n",
    "\n",
    "# We could think of this 3D data as:\n",
    "#   level 0: number of matrices\n",
    "#   level 1: number of rows in each matrix\n",
    "#   level 2: number of elements in each row = number of columns in each matrix\n",
    "# So here, we create 2 matrices, each of size 3x4 (3 rows, 4 columns)\n",
    "\n",
    "# Generate a tensor of specified shape according to the normal distribution N(0, 1), i.e., 0 mean, 1 variance.\n",
    "x = torch.randn(shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and print the shape of the previously generated tensor.\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and print the data type of the previously generated tensor.\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and print the data type of an element of the previously generated tensor.\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scalar tensors (only 1 value): Extract the value from the tensor into a plain Python number.\n",
    "single_element_tensor = torch.rand(1)\n",
    "print(single_element_tensor)\n",
    "print(single_element_tensor.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor from existing data.\n",
    "x = torch.tensor([1.5, 0.4, 0.3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and print the device on which the previously generated tensor resides on.\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the previously generated tensor to the GPU with index 0 (if possible).\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to(\"cuda:0\")\n",
    "\n",
    "# Check the current device of the tensor.\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use PyTorch to automatically calculate gradients of a function with respect to some variable.\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        f\\left(x\\right) &= \\left(2\\cdot{}x\\right)^7 \\\\\n",
    "        f^\\prime{}\\left(x\\right) &= 7\\cdot{}\\left(2\\cdot{}x\\right)^6\\cdot{}2 \\\\\n",
    "        f^\\prime{}\\left(x\\right)\\vert{}_{x = 1} &= 896\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function (with type hints for the parameter and for the return value).\n",
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return (2 * x) ** 7\n",
    "\n",
    "# Create tensor comprising a single scalar of value 1.\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "print(f'x     = {x.item()}')\n",
    "\n",
    "# Apply the function from above to our created tensor \"x\".\n",
    "y = f(x)\n",
    "print(f'f(x)  = {y.item()}')\n",
    "\n",
    "# Compute the gradient of this function w.r.t. \"x\".\n",
    "dy = torch.autograd.grad(y, x)\n",
    "print(f\"f'(x) = {dy[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This automatic differentiation functionality will turn out as one of the most powerful properties of PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Implement the function $g\\left(x\\right) = 42\\cdot{}\\exp{}\\left(\\frac{1}{x}\\right)$ using PyTorch.</li>\n",
    "        <li>Compute the gradient of previously defined function $g\\left(x\\right)$ for $x\\in{}\\{1, 3, 42\\}$ using the automatic differentiation functionality of PyTorch.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">MNIST dataset</h1>\n",
    "\n",
    "The [MNIST database (Modified National Institute of Standards and Technology database)](http://yann.lecun.com/exdb/mnist/) is a large database of handwritten digits and is also widely used for training and testing in the field of machine learning. The MNIST database contains $60,000$ images for training and $10,000$ for testing. PyTorch [provides](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist) an already pre-processed and adapted version of the MNIST dataset via the <code>torchvision</code> module.\n",
    "\n",
    "Note: You probably also have to install the Jupyter Widgets, see here: https://ipywidgets.readthedocs.io/en/stable/user_install.html (might require a restart of Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(root=\"resources\", train=True, download=True, transform=None)\n",
    "mnist_test = MNIST(root=\"resources\", train=False, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to examine the <i>training</i> and the <i>test</i> sets a little bit closer. For this very reason we first check the number of items in each set (i.e. the amount of <i>samples</i>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mnist_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mnist_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now happily confirm that the loaded training set comprises $60,000$ items whereas the test set $10.000$ – exactly how the MNIST dataset is described. The next step is to look what these items actually are. For that, we print the first item in the MNIST training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two things:\n",
    "- A PIL image item within <...>.\n",
    "- A number.\n",
    "\n",
    "Hence, an item of the MNIST dataset is clearly a <i>compound data type</i>. To examine which one in particular, we can check the type of the item in question to get more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the compound type of question is a Python <code>tuple</code>. Python tuples present sequences and are <i>immutable</i> Python objects. Hence, they can't be modified in any way after creation (note: the tuples <i>themselves</i>, not necessarily the comprising <i>items</i>). We now use the Python variable assignment convention (i.e., <i>structured binding</i> in this case) to get the image and the integer into two separate Python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train, target_train = mnist_train[0]\n",
    "print(image_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As merely seeing <code>&lt;PIL.Image.Image image mode=L size=28x28 at 0x...&gt;</code> doesn't help much in understanding how the image part of an item looks like, we can make use <code>PIL.Image</code> specific properties to actually display the data in a <i>human-readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_train, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The displayed data shows an image which does indeed look like the handwritten number <i>five</i>. This is confirmed by checking the content of the accompanying variable <code>target_train</code>. The second part of the tuple therefore turns out to be the corresponding <i>label</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check another item of the MNIST training set, but this time we want to analyze the $42^{nd}$ entry of the test dataset (keep in mind that indexing in Python is <i>zero-based</i>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test, target_test = mnist_test[41]\n",
    "\n",
    "plt.imshow(image_test, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()\n",
    "\n",
    "print(target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same behaviour of a dataset item can be confirmed here. Moreover, let's have another look at the data type of both images: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Logistic regression on the MNIST dataset</h2>\n",
    "\n",
    "Before preparing the MNIST data sets and starting the training process we need to set a few hyperparameters:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Hyperparameter</th>\n",
    "        <th>Value (used in this notebook)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>batch_size</code></td>\n",
    "        <td>8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>epochs</code></td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>learning_rate</code></td>\n",
    "        <td>1e-2</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><code>momentum</code></td>\n",
    "        <td>5e-1</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Set hyperparameters to be used in the training process.\n",
    "# Note: The \"SimpleNamespace(...)\" wrapper is just for convenience so we can write \"hyperparameters.XYZ\", where \"XYZ\" are our specified variables.\n",
    "hyperparameters = SimpleNamespace(\n",
    "    batch_size=8,\n",
    "    epochs=1,\n",
    "    learning_rate=1e-2,\n",
    "    momentum=5e-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST datasets and encapsulate them in data loaders.\n",
    "train_loader, test_loader = u4.get_dataset_mnist(\n",
    "    batch_size=hyperparameters.batch_size\n",
    ")\n",
    "\n",
    "# Display the first batch of data and drop the labels (keep only the images themselves).\n",
    "images = next(islice(train_loader, 1))[0]\n",
    "# Transform the image shapes for visualization purposes.\n",
    "images = np.concatenate([img.squeeze() for img in images], axis=1)\n",
    "# Display the first batch of data.\n",
    "plt.imshow(images, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader.dataset.data.shape)\n",
    "print(test_loader.dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mnist = train_loader.dataset[41][0].squeeze(dim=0).numpy()\n",
    "input_size = image_mnist.shape[0] * image_mnist.shape[1]  # number of pixel values = 28*28 = 784\n",
    "output_size = len(train_loader.dataset.classes)  # number of classes = 10\n",
    "print(f'Input size: {input_size}')\n",
    "print(f'Output size: {output_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "u4.set_seed(seed=42)\n",
    "\n",
    "# Minimize cross-entropy loss.\n",
    "coefficients = u4.minimize_ce(\n",
    "    dataset=train_loader,\n",
    "    iterations=hyperparameters.epochs,\n",
    "    learning_rate=hyperparameters.learning_rate,\n",
    "    momentum=hyperparameters.momentum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all samples and targets of the MNIST data set and reshape them accordingly.\n",
    "samples = torch.stack([x for x, y in train_loader.dataset]).flatten(start_dim=1)\n",
    "targets = train_loader.dataset.targets\n",
    "\n",
    "# Compute predictions of trained logistic regression model w.r.t. the training data.\n",
    "predictions = u4.predict_logistic(samples, coefficients)\n",
    "\n",
    "# Compute accuracy given the predicted and the true training labels.\n",
    "accuracy_train = (predictions == targets.numpy()).mean()\n",
    "print(f\"Accuracy on training set: {accuracy_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Apply the resulting logistic regression model to the test data. How well does our model perform?</li>\n",
    "        <li>Modify the hyperparameters and re-train a corresponding model. Can you get a better performance on the test set?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Flipping the MNIST dataset</h2>\n",
    "\n",
    "We now flip the datasets <i>horizontally</i> and <i>vertically</i> and see how the flipping influences the performance of our model. Be sure, to use the <i>previously fitted</i> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST datasets and encapsulate them in data loaders.\n",
    "train_loader, test_loader = u4.get_dataset_mnist(\n",
    "    batch_size=hyperparameters.batch_size,\n",
    "    horizontal_flip_p=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first batch of data and drop the labels (keep only the images themselves).\n",
    "images = next(islice(train_loader, 1))[0]\n",
    "# Transform the image shapes for visualization purposes.\n",
    "images = np.concatenate([img.squeeze() for img in images], axis=1)\n",
    "# Display the first batch of data.\n",
    "plt.imshow(images, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all samples and targets of the MNIST data set and reshape them accordingly.\n",
    "samples = torch.stack([x for x, y in train_loader.dataset]).flatten(start_dim=1)\n",
    "targets = train_loader.dataset.targets\n",
    "\n",
    "# Compute predictions of trained logistic regression model w.r.t. the training data.\n",
    "predictions = u4.predict_logistic(samples, coefficients)\n",
    "\n",
    "# Compute accuracy given the predicted and the true training labels.\n",
    "accuracy_train = (predictions == targets.numpy()).mean()\n",
    "print(f'Accuracy on training set: {accuracy_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Apply the resulting logistic regression model to the test data. How well does our model perform?</li>\n",
    "        <li>Try <i>horizontal</i> and <i>vertical</i> flips with different probabilities. How does this influence the performance on the test set?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Inverting the test dataset</h2>\n",
    "\n",
    "We now <i>invert the colors</i> of the datasets (horizontally and vertically flipping is still possible). Be sure, to use the <i>previously fitted</i> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST datasets and encapsulate them in data loaders.\n",
    "train_loader, test_loader = u4.get_dataset_mnist(\n",
    "    batch_size=hyperparameters.batch_size,\n",
    "    invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first batch of data and drop the labels (keep only the images themselves).\n",
    "images = next(islice(train_loader, 1))[0]\n",
    "# Transform the image shapes for visualization purposes.\n",
    "images = np.concatenate([img.squeeze() for img in images], axis=1)\n",
    "# Display the first batch of data.\n",
    "plt.imshow(images, cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all samples and targets of the MNIST data set and reshape them accordingly.\n",
    "samples = torch.stack([x for x, y in train_loader.dataset]).flatten(start_dim=1)\n",
    "targets = train_loader.dataset.targets\n",
    "\n",
    "# Compute predictions of trained logistic regression model w.r.t. the training data.\n",
    "predictions = u4.predict_logistic(samples, coefficients)\n",
    "\n",
    "# Compute accuracy given the predicted and the true training labels.\n",
    "accuracy_train = (predictions == targets.numpy()).mean()\n",
    "print(f'Accuracy on training set: {accuracy_train:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <ul>\n",
    "        <li>Apply the resulting logistic regression model to the test data. How well does our model perform?</li>\n",
    "        <li>Try <i>additional</i> horizontal and vertical flips. How does this influence the performance on the test set?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
