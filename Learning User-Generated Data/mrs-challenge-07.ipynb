{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "from typing import Callable\n",
    "import cupy as cp\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import ndcg_score\n",
    "from joblib import Parallel, delayed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization variables\n",
    "# t_reduce_data_size responsible for reducing datasize and feasible for testing training and best hyperparameters training \n",
    "# reduce_data_size_n_times change the scale factor to reduce data size. Important: Some user ids for submission might be missing due to reduces size\n",
    "# t_to_save_models if you want to save your results make it true\n",
    "# t_to_load_models if you want to load your results make it true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_reduce_data_size = False\n",
    "reduce_data_size_n_times = 10\n",
    "t_to_save_models = False\n",
    "t_to_load_models = False\n",
    "path_to_save_model = r'C:\\Users\\azatv\\Jupyter\\JupyterProjects\\Learning User-Generated Data\\recommendations\\rec_models.pkl'\n",
    "path_to_load_model = r'C:\\Users\\azatv\\Jupyter\\JupyterProjects\\Learning User-Generated Data\\recommendations\\rec_models.pkl'\n",
    "path_to_user_ids = r\"C:\\Users\\azatv\\Jupyter\\JupyterProjects\\Learning User-Generated Data\\lfm-challenge\\test_indices.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - 2D np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - 2D np.ndarray, test interaction matrix for each user;\n",
    "    \n",
    "    returns - float, mean dcg score over all user;\n",
    "    \"\"\"\n",
    "    num_users = predictions.shape[0]\n",
    "    dcgs = np.zeros(num_users)\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        gains = np.zeros(topK)\n",
    "        \n",
    "        for rank in range(min(topK, predictions.shape[1])):\n",
    "            item_id = predictions[user_id, rank]\n",
    "            relevance = test_interaction_matrix[user_id, item_id]\n",
    "            # 1/log(rank+1)\n",
    "            if relevance == 1:\n",
    "                if rank == 0:\n",
    "                    gains[rank] = 1\n",
    "                else:\n",
    "                    gains[rank] = 1 / np.log2(rank + 1)\n",
    "            \n",
    "        dcgs[user_id] = np.sum(gains)\n",
    "\n",
    "    mean_dcg = np.mean(dcgs)\n",
    "    return mean_dcg\n",
    "\n",
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - np.ndarray, test interaction matrix for each user;\n",
    "    topK - int, topK recommendations should be evaluated;\n",
    "    \n",
    "    returns - float, average ndcg score over all users;\n",
    "    \"\"\"\n",
    "    num_users = predictions.shape[0]\n",
    "    ndcgs = np.zeros(num_users)\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        user_predictions = predictions[user_id, :topK]\n",
    "        relevances = test_interaction_matrix[user_id, user_predictions]\n",
    "\n",
    "        actual_topK = min(topK, len(user_predictions))\n",
    "        \n",
    "        if actual_topK > 1:\n",
    "            discounts = np.log2(np.arange(2, actual_topK + 1))\n",
    "            dcg = relevances[0] + np.sum(relevances[1:actual_topK] / discounts)\n",
    "        else:\n",
    "            dcg = relevances[0]\n",
    "\n",
    "        sorted_relevances = np.sort(test_interaction_matrix[user_id])[-actual_topK:][::-1]\n",
    "        if actual_topK > 1:\n",
    "            idcg = sorted_relevances[0] + np.sum(sorted_relevances[1:actual_topK] / discounts)\n",
    "        else:\n",
    "            idcg = sorted_relevances[0] \n",
    "        \n",
    "        if idcg == 0:\n",
    "            ndcgs[user_id] = 0\n",
    "        else:\n",
    "            ndcgs[user_id] = dcg / idcg\n",
    "    \n",
    "    average_ndcg = np.mean(ndcgs)\n",
    "    return average_ndcg\n",
    "\n",
    "\n",
    "# REC Class\n",
    "def inter_matr_implicit(users: pd.DataFrame,\n",
    "                        items: pd.DataFrame,\n",
    "                        interactions: pd.DataFrame,\n",
    "                        dataset_name: str,\n",
    "                        threshold=1) -> np.ndarray:\n",
    "    res = None\n",
    "\n",
    "    interactions = interactions.copy()\n",
    "\n",
    "    # getting number of users and items from the respective files to be on the safe side\n",
    "    n_users = len(users.index)\n",
    "    n_items = len(items.index)\n",
    "    print(f\"users.index: {users.index}\")\n",
    "    print(f\"items.index: {items.index}\")\n",
    "    # preparing the output matrix\n",
    "    res = np.zeros([n_users, n_items], dtype=np.int8)\n",
    "    print(f\"res shape: {np.shape(res)}\")\n",
    "    # for every interaction assign 1 to the respective element of the matrix\n",
    "    if dataset_name == 'lfm-ismir':\n",
    "        inter_column_name = 'listening_events'\n",
    "    elif dataset_name == 'ml-1m':\n",
    "        inter_column_name = 'rating'\n",
    "    elif dataset_name == 'lfm-tiny-tunes':\n",
    "        inter_column_name = 'listening_events'\n",
    "    elif dataset_name == 'lfm-challenge':\n",
    "        inter_column_name = 'count'\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataset name: {dataset_name} \")\n",
    "    print(f\"dataset name: {inter_column_name}\")\n",
    "    print(f\"user ids pd:{interactions['user_id'].head(10)}\")\n",
    "    print(f\"item ids pd:{interactions['item_id'].head(10)}\")\n",
    "    print(f\"listening_events pd:{interactions[inter_column_name].head(10)}\")\n",
    "    \n",
    "    row = interactions[\"user_id\"].to_numpy()\n",
    "    col = interactions[\"item_id\"].to_numpy()\n",
    "    print(f\"row user ids np:{row[:10]} shape: {np.shape(row)}\")\n",
    "    print(f\"col item ids np:{col[:10]} shape: {np.shape(col)}\")\n",
    "    data = interactions[inter_column_name].to_numpy()\n",
    "    print(f\"listening_events np:{data[:10]}\")\n",
    "\n",
    "    data[data < threshold] = 0\n",
    "    data[data >= threshold] = 1\n",
    "\n",
    "    \n",
    "    res[row, col] = data\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def recTopKPop(inter_matr: np.ndarray,\n",
    "               user: int,\n",
    "               top_k: int) -> np.array:\n",
    "    '''\n",
    "    inter_matr - np.ndarray, from the task 1;\n",
    "    user - int, user_id;\n",
    "    top_k - int, expected length of the resulting list;\n",
    "\n",
    "    returns - list/array, of top K popular items that the user has never seen\n",
    "              (sorted in the order of descending popularity);\n",
    "    '''\n",
    "\n",
    "\n",
    "    top_pop = None\n",
    "\n",
    "    item_pop = inter_matr.sum(axis=0)\n",
    "\n",
    "    items_seen = np.nonzero(inter_matr[user])\n",
    "\n",
    "    item_pop[items_seen] = 0\n",
    "\n",
    "    top_pop = np.full((top_k,), -1)\n",
    "\n",
    "    t_pop = (-item_pop).argsort()[:top_k]\n",
    "    top_pop[:len(t_pop)] = t_pop\n",
    "\n",
    "    return top_pop\n",
    "\n",
    "\n",
    "def svd_decompose(inter_matr: np.ndarray, f: int = 50) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    inter_matr - np.ndarray, interaction matrix to construct svd from;\n",
    "    f - int, expected size of embeddings;\n",
    "\n",
    "    returns - 2D np.ndarray, U_final &  2D np.ndarray, V_final (as above) user-/item-embeddings of given length f;\n",
    "    \"\"\"\n",
    "\n",
    "    U_final = None\n",
    "    V_final = None\n",
    "\n",
    "\n",
    "    U, s, Vh = np.linalg.svd(inter_matr, full_matrices=False)\n",
    "    U_final = U[:, :f] @ np.diag(s[:f] ** 0.5)  # users x features\n",
    "    V_final = (np.diag(s[:f] ** 0.5) @ Vh[:f, :]).T  # items x features\n",
    "\n",
    "    return U_final, V_final\n",
    "\n",
    "\n",
    "def svd_recommend_to_list(user_id: int, seen_item_ids: list, U: np.ndarray, V: np.ndarray, topK: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recommend with svd to selected users\n",
    "\n",
    "    user_id - int, id of target user;\n",
    "    seen_item_ids - list[int], ids of items already seen by the users (to exclude from recommendation);\n",
    "    U and V - 2D np.ndarray & 2D np.ndarray, user- and item-embeddings;\n",
    "    topK - int, number of recommendations per user to be returned;\n",
    "\n",
    "    returns - np.ndarray, list of ids of recommended items in the order of descending score\n",
    "                           use -1 as a place holder item index, when it is impossible to recommend topK items;\n",
    "    \"\"\"\n",
    "    recs = None\n",
    "\n",
    "    scores = U @ V.T\n",
    "    u_scores = scores[user_id]\n",
    "    u_scores[seen_item_ids] = -np.inf\n",
    "    m = min(topK, scores.shape[1])\n",
    "    recs = (-u_scores).argsort()[:m]\n",
    "\n",
    "    return np.array(recs)\n",
    "\n",
    "\n",
    "def jaccard_score(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    a, b - 1D np.ndarray, vectors of the same length corresponding to the two items;\n",
    "\n",
    "    returns - float, jaccard similarity score for a and b;\n",
    "    \"\"\"\n",
    "    score = None\n",
    "    c = a + b\n",
    "    intersection = np.zeros_like(c)\n",
    "    intersection[c > 1] = 1\n",
    "    union = np.zeros_like(c)\n",
    "    union[c >= 1] = 1\n",
    "\n",
    "    score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "def calculate_sim_scores(similarity_measure: Callable[[np.ndarray, np.ndarray], float],\n",
    "                         inter: np.ndarray,\n",
    "                         target_vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    similarity_measure - Callable, function that measures similarity, it gets called using your jaccard_score function from above - as always do not directly call your function, but use the passed parameter;\n",
    "    inter - np.ndarray, interaction matrix - calculate similarity between each item and the target item (see below);\n",
    "    target_vec - np.ndarray, target item vector;\n",
    "\n",
    "    returns - np.ndarray, similarities between every item from <inter> and <target_vec> in the respective order;\n",
    "    \"\"\"\n",
    "\n",
    "    item_similarities = None\n",
    "    item_similarities = np.zeros((inter.shape[1],))\n",
    "\n",
    "    for item in range(inter.shape[1]):\n",
    "        inter_items = inter[:, item]\n",
    "        item_similarities[item] = similarity_measure(inter_items, target_vec)\n",
    "\n",
    "    return np.array(item_similarities)\n",
    "\n",
    "\n",
    "def get_user_item_score(sim_scores_calculator: Callable[[Callable, np.array, np.array], np.array],\n",
    "                        inter: np.array,\n",
    "                        target_user: int,\n",
    "                        target_item: int,\n",
    "                        n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    sim_scores_calculator - Callable, function that calculates similarities, using calculate_sim_scores\n",
    "                                      from above, already defined in the next cell;\n",
    "    inter - np.ndarray, interaction matrix;\n",
    "    target_user - int, target user id;\n",
    "    target_item - int, target item id;\n",
    "    n - int, n closest neighbors to consider for the score prediction;\n",
    "\n",
    "    returns - float, mean of similarity scores = user-item 'fitness' score;\n",
    "    \"\"\"\n",
    "\n",
    "    item_similarities_mean = None\n",
    "    inter_pred = inter.copy()\n",
    "\n",
    "    # Get all items which were consumed by the user.\n",
    "    item_consumed_by_user = inter_pred[target_user, :] == 1\n",
    "    item_consumed_by_user[target_item] = False\n",
    "\n",
    "    # get column of the target_item.\n",
    "    inter_target_item = inter_pred[:, target_item]\n",
    "\n",
    "    # create a mask to remove the user from the interaction matrix.\n",
    "    not_user = np.full((inter_pred.shape[0],), True)\n",
    "    not_user[target_user] = False\n",
    "\n",
    "    # remove items not interacted with user\n",
    "    inter_pred = inter_pred[:, item_consumed_by_user]\n",
    "\n",
    "    # remove user\n",
    "    inter_pred = inter_pred[not_user]\n",
    "    inter_target_item = inter_target_item[not_user]\n",
    "\n",
    "    # get closest items to target_item, which is at the last indices.\n",
    "    scores = sim_scores_calculator(inter_pred, inter_target_item)\n",
    "\n",
    "    # get items with the highes scores.\n",
    "    scores_ids = np.argsort((- scores))\n",
    "    scores = scores[scores_ids]\n",
    "\n",
    "    scores = scores[:n]\n",
    "\n",
    "    if len(scores) > 0:\n",
    "        # calculate mean of normed scores.\n",
    "        item_similarities_mean = scores.mean()\n",
    "    else:\n",
    "        item_similarities_mean = 0.0\n",
    "\n",
    "    return item_similarities_mean\n",
    "\n",
    "\n",
    "def sim_score_calc(inter, target_vec): return calculate_sim_scores(jaccard_score, inter, target_vec)\n",
    "\n",
    "\n",
    "def user_item_scorer(inter, target_user, target_item, n): return get_user_item_score(sim_score_calc, inter,\n",
    "                                                                                     target_user, target_item, n)\n",
    "\n",
    "\n",
    "def _recTopK_base(user_item_scorer: Callable[[Callable, np.array, int, int], float],\n",
    "                  inter_matr: np.array,\n",
    "                  user: int,\n",
    "                  top_k: int,\n",
    "                  n: int) -> (np.array, np.array):\n",
    "    '''\n",
    "    user_item_scorer - Callable, wrapper function that calculates user-item score, using get_user_item_score function\n",
    "                                 from above, already defined in the next cell;\n",
    "    inter_matr - np.ndarray, interaction matrix;\n",
    "    user - int,  user_id;\n",
    "    top_k - int, expected length of the resulting list;\n",
    "    n - int, number of neighbors to consider;\n",
    "\n",
    "    returns - 1D np.ndarray, of recommendations (sorted in the order of descending scores) & 1D np.ndarray, of corresponding scores;\n",
    "    '''\n",
    "\n",
    "    top_rec = None\n",
    "    scores = None\n",
    "\n",
    "    \n",
    "\n",
    "    scores = np.zeros((inter_matr.shape[1],))\n",
    "\n",
    "    for item in range(inter_matr.shape[1]):\n",
    "        if inter_matr[user, item] == 0:\n",
    "            score = user_item_scorer(inter_matr, user, item, n)\n",
    "            scores[item] = score\n",
    "\n",
    "    top_rec = (- scores).argsort()[:top_k]\n",
    "    scores = scores[top_rec]\n",
    "\n",
    "    return np.array(top_rec), np.array(scores)\n",
    "\n",
    "\n",
    "def recTopK(inter_matr: np.array,\n",
    "            user: int,\n",
    "            top_k: int,\n",
    "            n: int) -> (np.array, np.array):\n",
    "    return _recTopK_base(user_item_scorer, inter_matr, user, top_k, n)[0]\n",
    "\n",
    "\n",
    "def reduce_matrix_size(matrix, reduction_factor=10):\n",
    "    n_rows, n_cols = matrix.shape\n",
    "    row_indices = np.random.choice(n_rows, n_rows // reduction_factor, replace=False)\n",
    "    col_indices = np.random.choice(n_cols, n_cols // reduction_factor, replace=False)\n",
    "    return matrix[np.ix_(row_indices, col_indices)]\n",
    "\n",
    "\n",
    "def get_recommendations_for_algorithms(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, already predefined below with name \"rec_dict\";\n",
    "    \"\"\"\n",
    "\n",
    "    #use this structure to return results\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            #Add your predictions here\n",
    "            \"predictions\": []\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"predictions\": []\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"predictions\": []\n",
    "        },\n",
    "    }}\n",
    "\n",
    "    # SVD \n",
    "    try:\n",
    "        U_final, V_final = svd_decompose(config['train_inter'], config['recommenders']['SVD']['n_factors'])\n",
    "        for user_id in tqdm (range(config['train_inter'].shape[0]), desc=\"SVD...\"):\n",
    "            seen_item_ids = np.where(config['train_inter'][user_id] > 0)[0] \n",
    "            recommendations = svd_recommend_to_list(user_id, seen_item_ids, U_final, V_final, config['top_k'])\n",
    "            rec_dict['recommenders']['SVD']['predictions'].append([recommendations])\n",
    "        rec_dict['recommenders']['SVD']['predictions'] = np.vstack(rec_dict['recommenders']['SVD']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"SVD Decomposition Failed:\", str(e))\n",
    "\n",
    "    # ItemKNN \n",
    "    try:\n",
    "        for user_id in tqdm (range(config['train_inter'].shape[0]), desc=\"ItemKNN...\"):\n",
    "            recommendations = recTopK(config['train_inter'], user_id, config['top_k'], config['recommenders']['ItemKNN']['n_neighbours'])\n",
    "            rec_dict['recommenders']['ItemKNN']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['ItemKNN']['predictions'] = np.vstack(rec_dict['recommenders']['ItemKNN']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"ItemKNN Recommendation Failed:\", str(e))\n",
    "\n",
    "    # TopPop \n",
    "    try:\n",
    "        for user_id in tqdm (range(config['train_inter'].shape[0]), desc=\"TopPop...\"):\n",
    "            recommendations = recTopKPop(config['train_inter'], user_id, config['top_k'])\n",
    "            rec_dict['recommenders']['TopPop']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['TopPop']['predictions'] = np.vstack(rec_dict['recommenders']['TopPop']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"TopPop Recommendation Failed:\", str(e))\n",
    "    if t_to_save_models: \n",
    "        with open(path_to_save_model, 'wb') as f:\n",
    "            pickle.dump(rec_dict['recommenders'], f)\n",
    "\n",
    "    return rec_dict\n",
    "\n",
    "# Loading the models when needed\n",
    "def load_models(filepath: str) -> dict:\n",
    "    with open(filepath, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "    return models\n",
    "\n",
    "\n",
    "# Load the list of user IDs from the provided file, skipping the header\n",
    "def load_test_user_ids(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        next(file)  # Skip the header line\n",
    "        test_user_ids = [int(line.strip()) for line in file]\n",
    "    return test_user_ids\n",
    "\n",
    "def save_submission_files(config: dict, recommendations: dict, test_user_ids: list, matr_num: str, name: str):\n",
    "    # Create the recommendations TSV file\n",
    "    tsv_filename = f\"rec_{matr_num}_{name}.tsv\"\n",
    "    with open(tsv_filename, 'w') as tsv_file:\n",
    "        for user_id in test_user_ids:\n",
    "            user_recommendations = recommendations['recommenders']['ItemKNN']['predictions'][user_id]\n",
    "            user_recommendations_str = ','.join(map(str, user_recommendations))\n",
    "            tsv_file.write(f\"{user_id}\\t{user_recommendations_str}\\n\")\n",
    "\n",
    "    print(f\"Submission files created: {tsv_filename}\")\n",
    "\n",
    "\n",
    "# def save_submission_files(config: dict, recommendations: dict, matr_num: str, name: str):\n",
    "#     # Get the list of test users from the config\n",
    "#     test_users = np.where(config['test_inter'].sum(axis=1) > 0)[0]\n",
    "\n",
    "#     # Select the best recommender (ItemKNN in this case)\n",
    "#     best_recommender = 'ItemKNN'\n",
    "    \n",
    "#     # Create the recommendations TSV file\n",
    "#     tsv_filename = f\"rec_{matr_num}_{name}.tsv\"\n",
    "#     with open(tsv_filename, 'w') as tsv_file:\n",
    "#         for user_id in test_users:\n",
    "#             user_recommendations = recommendations['recommenders'][best_recommender]['predictions'][user_id]\n",
    "#             user_recommendations_str = ','.join(map(str, user_recommendations))\n",
    "#             tsv_file.write(f\"{user_id}\\t{user_recommendations_str}\\n\")\n",
    "#     if t_to_create_txt_file:\n",
    "#         # Create the report TXT file\n",
    "#         txt_filename = f\"report_{matr_num}_{name}.txt\"\n",
    "#         with open(txt_filename, 'w') as txt_file:\n",
    "#             txt_file.write(f\"Matrix Number: {matr_num}\\nName: {name}\\n\\n\")\n",
    "#             txt_file.write(\"Approach:\\n\")\n",
    "#             txt_file.write(f\"{best_recommender}:\\n\")\n",
    "#             txt_file.write(f\"Hyperparameters: {config['recommenders'][best_recommender]}\\n\")\n",
    "#             txt_file.write(f\"Description: Description of the approach used for {best_recommender}.\\n\\n\")\n",
    "\n",
    "#     print(f\"Submission files created: {tsv_filename}\")\n",
    "\n",
    "# def read(dataset, file):\n",
    "#     return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_recommendations_for_svd(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, already predefined below with name \"rec_dict\";\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this structure to return results\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            # Add your predictions here\n",
    "            \"predictions\": [],\n",
    "            \"n_factors\": config['recommenders']['SVD']['n_factors']\n",
    "        }\n",
    "    }}\n",
    "\n",
    "    # SVD \n",
    "    try:\n",
    "        U_final, V_final = svd_decompose(config['train_inter'], config['recommenders']['SVD']['n_factors'])\n",
    "        for user_id in tqdm(range(config['train_inter'].shape[0]), desc=\"SVD...\"):\n",
    "            seen_item_ids = np.where(config['train_inter'][user_id] > 0)[0]\n",
    "            recommendations = svd_recommend_to_list(user_id, seen_item_ids, U_final, V_final, config['top_k'])\n",
    "            rec_dict['recommenders']['SVD']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['SVD']['predictions'] = np.array(rec_dict['recommenders']['SVD']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"SVD Decomposition Failed:\", str(e))\n",
    "\n",
    "    with open(f\"rec_svd_{config['recommenders']['SVD']['n_factors']}.pkl\", 'wb') as f:\n",
    "        pickle.dump(rec_dict['recommenders'], f)\n",
    "\n",
    "    return rec_dict\n",
    "\n",
    "def evaluate_svd(config: dict) -> float:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - float, nDCG score for the SVD recommender;\n",
    "    \"\"\"\n",
    "\n",
    "    svd_predictions = config['recommenders']['SVD']['predictions']\n",
    "    svd_ndcg = get_ndcg_score(svd_predictions, config['test_inter'], config['top_k'])\n",
    "    return svd_ndcg\n",
    "\n",
    "def find_best_svd_factors(train_inter, test_inter, factors_list, top_k):\n",
    "    best_factors = None\n",
    "    best_ndcg = -1\n",
    "    results = []\n",
    "\n",
    "    for n_factors in factors_list:\n",
    "        print(f\"Evaluating SVD with n_factors={n_factors}\")\n",
    "        config_predict_svd = {\n",
    "            \"train_inter\": train_inter,\n",
    "            \"top_k\": top_k,\n",
    "            \"recommenders\": {\n",
    "                \"SVD\": {\n",
    "                    \"n_factors\": n_factors\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        recommendations = get_recommendations_for_svd(config_predict_svd)\n",
    "        config_test = {\n",
    "            \"top_k\": top_k,\n",
    "            \"test_inter\": test_inter,\n",
    "            \"recommenders\": recommendations['recommenders']\n",
    "        }\n",
    "        ndcg = evaluate_svd(config_test)\n",
    "        results.append((n_factors, ndcg))\n",
    "\n",
    "        if ndcg > best_ndcg:\n",
    "            best_ndcg = ndcg\n",
    "            best_factors = n_factors\n",
    "\n",
    "    print(f\"Best n_factors: {best_factors} with nDCG: {best_ndcg}\")\n",
    "    for n_factors, ndcg in results:\n",
    "        print(f\"n_factors: {n_factors}, nDCG: {ndcg}\")\n",
    "\n",
    "    return best_factors, best_ndcg\n",
    "\n",
    "\n",
    "def score_ndcg(recs, g_truth):\n",
    "    predicted_scores = np.zeros(g_truth.shape[1])\n",
    "\n",
    "    for i, rec in enumerate(recs):\n",
    "        predicted_scores[rec] = len(recs) - i\n",
    "\n",
    "    return ndcg_score(g_truth, predicted_scores.reshape(1, -1), k=len(recs))\n",
    "\n",
    "\n",
    "def evaluate_algorithm(predictions, g_truth, top_k):\n",
    "    \"\"\"\n",
    "    Evaluate a single algorithm using the given predictions and ground truth.\n",
    "\n",
    "    predictions - np.ndarray, predicted item rankings for each user\n",
    "    g_truth - np.ndarray, ground truth interaction matrix\n",
    "    top_k - int, number of top recommendations to evaluate\n",
    "\n",
    "    returns - float, nDCG score for the given algorithm\n",
    "    \"\"\"\n",
    "    # Calculate the average nDCG score over all users\n",
    "    ndcg_scores = []\n",
    "    for user_id in range(g_truth.shape[0]):\n",
    "        recs = predictions[user_id]\n",
    "        ground_truth = g_truth[user_id].reshape(1, -1)\n",
    "        ndcg = score_ndcg(recs, ground_truth)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def evaluate_algorithms(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, { Recommender Key from input dict: { \"ndcg\": float - ndcg from evaluation for this recommender} };\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"SVD\": {\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Calculate nDCG for SVD\n",
    "    svd_predictions = config['recommenders']['SVD']['predictions']\n",
    "    svd_ndcg = evaluate_algorithm(svd_predictions, config['test_inter'], config['top_k'])\n",
    "    metrics[\"SVD\"][\"ndcg\"] = svd_ndcg\n",
    "\n",
    "    # Calculate nDCG for ItemKNN\n",
    "    itemknn_predictions = config['recommenders']['ItemKNN']['predictions']\n",
    "    itemknn_ndcg = evaluate_algorithm(itemknn_predictions, config['test_inter'], config['top_k'])\n",
    "    metrics[\"ItemKNN\"][\"ndcg\"] = itemknn_ndcg\n",
    "\n",
    "    # Calculate nDCG for TopPop\n",
    "    toppop_predictions = config['recommenders']['TopPop']['predictions']\n",
    "    toppop_ndcg = evaluate_algorithm(toppop_predictions, config['test_inter'], config['top_k'])\n",
    "    metrics[\"TopPop\"][\"ndcg\"] = toppop_ndcg\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "def recTopK_parallel(inter_matr, user_id, top_k, n_neighbours):\n",
    "    return recTopK(inter_matr, user_id, top_k, n_neighbours)\n",
    "\n",
    "def get_recommendations_for_itemknn(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, already predefined below with name \"rec_dict\";\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this structure to return results\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"ItemKNN\": {\n",
    "            # Add your predictions here\n",
    "            \"predictions\": [],\n",
    "            \"n_neighbours\": config['recommenders']['ItemKNN']['n_neighbours']\n",
    "        }\n",
    "    }}\n",
    "\n",
    "    # ItemKNN\n",
    "    try:\n",
    "        num_cores = os.cpu_count()\n",
    "        print(f\"Using {num_cores} cores for parallel processing.\")\n",
    "\n",
    "        # Parallel processing of recommendations\n",
    "        recommendations = Parallel(n_jobs=num_cores)(\n",
    "            delayed(recTopK_parallel)(config['train_inter'], user_id, config['top_k'], config['recommenders']['ItemKNN']['n_neighbours'])\n",
    "            for user_id in tqdm(range(config['train_inter'].shape[0]), desc=\"ItemKNN...\")\n",
    "        )\n",
    "        rec_dict['recommenders']['ItemKNN']['predictions'] = np.array(recommendations)\n",
    "    except Exception as e:\n",
    "        print(\"ItemKNN Recommendation Failed:\", str(e))\n",
    "\n",
    "    with open(f\"rec_itemknn_{config['recommenders']['ItemKNN']['n_neighbours']}.pkl\", 'wb') as f:\n",
    "        pickle.dump(rec_dict['recommenders'], f)\n",
    "\n",
    "    return rec_dict\n",
    "\n",
    "def evaluate_itemknn(config: dict) -> float:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - float, nDCG score for the ItemKNN recommender;\n",
    "    \"\"\"\n",
    "\n",
    "    itemknn_predictions = config['recommenders']['ItemKNN']['predictions']\n",
    "    itemknn_ndcg = get_ndcg_score(itemknn_predictions, config['test_inter'], config['top_k'])\n",
    "    return itemknn_ndcg\n",
    "\n",
    "def find_best_itemknn_neighbours(train_inter, test_inter, neighbours_list, top_k):\n",
    "    best_neighbours = None\n",
    "    best_ndcg = -1\n",
    "    results = []\n",
    "\n",
    "    for n_neighbours in neighbours_list:\n",
    "        print(f\"Evaluating ItemKNN with n_neighbours={n_neighbours}\")\n",
    "        config_predict_itemknn = {\n",
    "            \"train_inter\": train_inter,\n",
    "            \"top_k\": top_k,\n",
    "            \"recommenders\": {\n",
    "                \"ItemKNN\": {\n",
    "                    \"n_neighbours\": n_neighbours\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        recommendations = get_recommendations_for_itemknn(config_predict_itemknn)\n",
    "        config_test = {\n",
    "            \"top_k\": top_k,\n",
    "            \"test_inter\": test_inter,\n",
    "            \"recommenders\": recommendations['recommenders']\n",
    "        }\n",
    "        ndcg = evaluate_itemknn(config_test)\n",
    "        results.append((n_neighbours, ndcg))\n",
    "\n",
    "        if ndcg > best_ndcg:\n",
    "            best_ndcg = ndcg\n",
    "            best_neighbours = n_neighbours\n",
    "\n",
    "    print(f\"Best n_neighbours: {best_neighbours} with nDCG: {best_ndcg}\")\n",
    "    for n_neighbours, ndcg in results:\n",
    "        print(f\"n_neighbours: {n_neighbours}, nDCG: {ndcg}\")\n",
    "\n",
    "    return best_neighbours, best_ndcg\n",
    "\n",
    "\n",
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialization of interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users.index: RangeIndex(start=0, stop=2795, step=1)\n",
      "items.index: RangeIndex(start=0, stop=4178, step=1)\n",
      "res shape: (2795, 4178)\n",
      "dataset name: count\n",
      "user ids pd:0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: user_id, dtype: int64\n",
      "item ids pd:0     341\n",
      "1     393\n",
      "2     331\n",
      "3     343\n",
      "4     326\n",
      "5     345\n",
      "6     342\n",
      "7     332\n",
      "8     340\n",
      "9    2622\n",
      "Name: item_id, dtype: int64\n",
      "listening_events pd:0    7\n",
      "1    2\n",
      "2    2\n",
      "3    6\n",
      "4    2\n",
      "5    5\n",
      "6    5\n",
      "7    3\n",
      "8    8\n",
      "9    2\n",
      "Name: count, dtype: int64\n",
      "row user ids np:[100 100 100 100 100 100 100 100 100 100] shape: (75860,)\n",
      "col item ids np:[ 341  393  331  343  326  345  342  332  340 2622] shape: (75860,)\n",
      "listening_events np:[7 2 2 6 2 5 5 3 8 2]\n",
      "users.index: RangeIndex(start=0, stop=2795, step=1)\n",
      "items.index: RangeIndex(start=0, stop=4178, step=1)\n",
      "res shape: (2795, 4178)\n",
      "dataset name: count\n",
      "user ids pd:0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: user_id, dtype: int64\n",
      "item ids pd:0     333\n",
      "1     357\n",
      "2     329\n",
      "3     349\n",
      "4     324\n",
      "5     133\n",
      "6    3015\n",
      "7     132\n",
      "8     358\n",
      "9     363\n",
      "Name: item_id, dtype: int64\n",
      "listening_events pd:0     2\n",
      "1     3\n",
      "2    18\n",
      "3     4\n",
      "4    10\n",
      "5     4\n",
      "6     2\n",
      "7    12\n",
      "8     5\n",
      "9     2\n",
      "Name: count, dtype: int64\n",
      "row user ids np:[100 100 100 100 100 100 100 100 100 100] shape: (25695,)\n",
      "col item ids np:[ 333  357  329  349  324  133 3015  132  358  363] shape: (25695,)\n",
      "listening_events np:[ 2  3 18  4 10  4  2 12  5  2]\n"
     ]
    }
   ],
   "source": [
    "users = read('lfm-challenge','user')\n",
    "items = read('lfm-challenge','item')\n",
    "train_inters = read('lfm-challenge','inter_train')\n",
    "test_inters = read('lfm-challenge','inter_test')\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=train_inters,\n",
    "                                               dataset_name=\"lfm-challenge\")\n",
    "test_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=test_inters,\n",
    "                                              dataset_name=\"lfm-challenge\")\n",
    "\n",
    "if t_reduce_data_size:\n",
    "    # Reduce the size of the matrices by 1/10\n",
    "    train_interaction_matrix = reduce_matrix_size(train_interaction_matrix, reduction_factor=reduce_data_size_n_times)\n",
    "    test_interaction_matrix = reduce_matrix_size(test_interaction_matrix, reduction_factor=reduce_data_size_n_times)\n",
    "\n",
    "    # Output the shapes of the reduced matrices to verify\n",
    "    print(f\"Reduced train interaction matrix shape: {train_interaction_matrix.shape}\")\n",
    "    print(f\"Reduced test interaction matrix shape: {test_interaction_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs\n",
    "# config_predict responsible for hyperparameters tweaking \n",
    "# config_predict_svd for testing particularly svd\n",
    "# config_test includes predictions of svd, itemknn and topPop recomenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_predict = {\n",
    "    #interaction matrix\n",
    "    \"train_inter\": train_interaction_matrix,\n",
    "    #topK parameter used for all algorithms\n",
    "    \"top_k\": 10,\n",
    "    #specific parameters for all algorithms\n",
    "    \"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            \"n_factors\": 300\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"n_neighbours\": 6\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config_predict_svd = {\n",
    "    #interaction matrix\n",
    "    \"train_inter\": train_interaction_matrix,\n",
    "    #topK parameter used for all algorithms\n",
    "    \"top_k\": 10,\n",
    "    #specific parameters for all algorithms\n",
    "    \"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            \"n_factors\": 300\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config_test = {\n",
    "    \"top_k\": 10,\n",
    "    \"test_inter\": test_interaction_matrix,\n",
    "    \"recommenders\": {}  # here you can access the recommendations from get_recommendations_for_algorithms\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SVD, ItemKNN and TopPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 2795/2795 [03:44<00:00, 12.47it/s]\n",
      "ItemKNN...:   0%|          | 4/2795 [00:45<8:47:06, 11.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mget_recommendations_for_algorithms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m recommendations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m recommendations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(recommendations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray)\n",
      "Cell \u001b[1;32mIn[63], line 372\u001b[0m, in \u001b[0;36mget_recommendations_for_algorithms\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m tqdm (\u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_inter\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItemKNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 372\u001b[0m         recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecTopK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_inter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecommenders\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mItemKNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_neighbours\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m         rec_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(recommendations)\n\u001b[0;32m    374\u001b[0m     rec_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(rec_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommenders\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[63], line 327\u001b[0m, in \u001b[0;36mrecTopK\u001b[1;34m(inter_matr, user, top_k, n)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecTopK\u001b[39m(inter_matr: np\u001b[38;5;241m.\u001b[39marray,\n\u001b[0;32m    324\u001b[0m             user: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    325\u001b[0m             top_k: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    326\u001b[0m             n: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (np\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39marray):\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_recTopK_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_item_scorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minter_matr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[63], line 314\u001b[0m, in \u001b[0;36m_recTopK_base\u001b[1;34m(user_item_scorer, inter_matr, user, top_k, n)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inter_matr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inter_matr[user, item] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 314\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43muser_item_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minter_matr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m         scores[item] \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m    317\u001b[0m top_rec \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m scores)\u001b[38;5;241m.\u001b[39margsort()[:top_k]\n",
      "Cell \u001b[1;32mIn[63], line 285\u001b[0m, in \u001b[0;36muser_item_scorer\u001b[1;34m(inter, target_user, target_item, n)\u001b[0m\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muser_item_scorer\u001b[39m(inter, target_user, target_item, n): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_user_item_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim_score_calc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mtarget_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 244\u001b[0m, in \u001b[0;36mget_user_item_score\u001b[1;34m(sim_scores_calculator, inter, target_user, target_item, n)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03msim_scores_calculator - Callable, function that calculates similarities, using calculate_sim_scores\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m                                  from above, already defined in the next cell;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03mreturns - float, mean of similarity scores = user-item 'fitness' score;\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m item_similarities_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m inter_pred \u001b[38;5;241m=\u001b[39m \u001b[43minter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Get all items which were consumed by the user.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m item_consumed_by_user \u001b[38;5;241m=\u001b[39m inter_pred[target_user, :] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "recommendations = get_recommendations_for_algorithms(config_predict)\n",
    "\n",
    "assert \"SVD\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"SVD\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"SVD\"][\"predictions\"], np.ndarray)\n",
    "assert \"ItemKNN\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"ItemKNN\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"ItemKNN\"][\"predictions\"], np.ndarray)\n",
    "assert \"TopPop\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"TopPop\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"TopPop\"][\"predictions\"], np.ndarray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2795 entries, 0 to 2794\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   user_id              2795 non-null   int64 \n",
      " 1   country              2264 non-null   object\n",
      " 2   age_at_registration  2795 non-null   int64 \n",
      " 3   gender               2789 non-null   object\n",
      " 4   registration_date    2795 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 109.3+ KB\n",
      "None\n",
      "   user_id country  age_at_registration gender    registration_date\n",
      "0        0     NaN                   -1      n  2012-01-17 18:42:44\n",
      "1        1     NaN                   -1      n  2011-03-24 13:27:26\n",
      "2        2      US                   -1      m  2011-12-29 06:46:36\n",
      "3        3     NaN                   -1      n  2012-04-16 11:21:04\n",
      "4        4     NaN                   -1      n  2012-01-18 19:01:26\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4178 entries, 0 to 4177\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   item_id     4178 non-null   int64 \n",
      " 1   artist      4177 non-null   object\n",
      " 2   song        4177 non-null   object\n",
      " 3   album_name  4177 non-null   object\n",
      " 4   genre       4177 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 163.3+ KB\n",
      "None\n",
      "   item_id               artist                       song  \\\n",
      "0        0           Bucks Fizz        Making Your Mind Up   \n",
      "1        1               Delain                    Invidia   \n",
      "2        2  Death Cab for Cutie  Pictures in an Exhibition   \n",
      "3        3             Maroon 5                        How   \n",
      "4        4           Anne Clark      Sleeper In Metropolis   \n",
      "\n",
      "                             album_name  \\\n",
      "0                            Bucks Fizz   \n",
      "1                            April Rain   \n",
      "2  You Can Play These Songs With Chords   \n",
      "3               Hands All Over (Deluxe)   \n",
      "4                       Changing Places   \n",
      "\n",
      "                                               genre  \n",
      "0  ['pop', 'eurovision', 'disco', 'rock', 'uk pop...  \n",
      "1  ['symphonic metal', 'gothic metal', 'metal', '...  \n",
      "2  ['indie rock', 'emo', 'rock', 'indie pop', 'al...  \n",
      "3  ['pop', 'rock', 'pop rock', 'alternative rock'...  \n",
      "4  ['new wave', 'dark wave', 'electro', 'synthpop...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75860 entries, 0 to 75859\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   user_id  75860 non-null  int64\n",
      " 1   item_id  75860 non-null  int64\n",
      " 2   count    75860 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 1.7 MB\n",
      "None\n",
      "   user_id  item_id  count\n",
      "0      100      341      7\n",
      "1      100      393      2\n",
      "2      100      331      2\n",
      "3      100      343      6\n",
      "4      100      326      2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25695 entries, 0 to 25694\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   user_id  25695 non-null  int64\n",
      " 1   item_id  25695 non-null  int64\n",
      " 2   count    25695 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 602.4 KB\n",
      "None\n",
      "   user_id  item_id  count\n",
      "0      100      333      2\n",
      "1      100      357      3\n",
      "2      100      329     18\n",
      "3      100      349      4\n",
      "4      100      324     10\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('lfm-challenge/lfm-challenge.user', sep='\\t')\n",
    "\n",
    "print(users.info())\n",
    "print(users.head())\n",
    "\n",
    "items = pd.read_csv('lfm-challenge/lfm-challenge.item', sep='\\t')\n",
    "\n",
    "print(items.info())\n",
    "print(items.head())\n",
    "\n",
    "train_inters = pd.read_csv('lfm-challenge/lfm-challenge.inter_train', sep='\\t')\n",
    "\n",
    "print(train_inters.info())\n",
    "print(train_inters.head())\n",
    "\n",
    "test_inters = pd.read_csv('lfm-challenge/lfm-challenge.inter_test', sep='\\t')\n",
    "\n",
    "print(test_inters.info())\n",
    "print(test_inters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add predictions to config_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config_test = {\n",
    "#     \"top_k\": 10,\n",
    "#     \"test_inter\": test_interaction_matrix,\n",
    "#     \"recommenders\": recommendations['recommenders']  # Access the recommendations from get_recommendations_for_algorithms\n",
    "# }\n",
    "# add dictionary with recommendations to config dictionary\n",
    "config_test.update(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate based on the sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = evaluate_algorithms(config_test)\n",
    "\n",
    "\n",
    "if t_to_load_models:\n",
    "    loaded_models = load_models(path_to_load_model)\n",
    "    for recommender in loaded_models.keys():\n",
    "        print(f\"Loaded {recommender} model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD ndcg: 0.008928682201060007\n",
      "ItemKNN ndcg: 0.001138426731611974\n",
      "TopPop ndcg: 0.00619646805273084\n"
     ]
    }
   ],
   "source": [
    "# Output the evaluations\n",
    "for recommender in evaluations.keys():\n",
    "    print(f\"{recommender} ndcg: {evaluations[recommender]['ndcg']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save .tsv submissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 288 is out of bounds for axis 0 with size 279",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m test_user_ids \u001b[38;5;241m=\u001b[39m load_test_user_ids(path_to_user_ids)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Save the submission files\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43msave_submission_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecommendations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_user_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatr_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 411\u001b[0m, in \u001b[0;36msave_submission_files\u001b[1;34m(config, recommendations, test_user_ids, matr_num, name)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tsv_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tsv_file:\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m test_user_ids:\n\u001b[1;32m--> 411\u001b[0m         user_recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mrecommendations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecommenders\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mItemKNN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    412\u001b[0m         user_recommendations_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, user_recommendations))\n\u001b[0;32m    413\u001b[0m         tsv_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_recommendations_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 288 is out of bounds for axis 0 with size 279"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "matr_num = \"k00000000\"  # Your matriculation number\n",
    "name = \"Joe_Biden\"  # Your name\n",
    "\n",
    "# Load test user IDs from the provided file\n",
    "test_user_ids = load_test_user_ids(path_to_user_ids)\n",
    "\n",
    "# Save the submission files\n",
    "save_submission_files(config_test, recommendations, test_user_ids, matr_num, name)\n",
    "\n",
    "\n",
    "# if you get IndexError, turn off reduce_data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding best hyperparameter for SVD and ItemKNN\n",
    "## IMPORTANT RUN ONLY IF YOU WANNA FIND BEST SVD, ItemKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVD with n_factors=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 1397/1397 [00:30<00:00, 46.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVD with n_factors=250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 1397/1397 [00:31<00:00, 44.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVD with n_factors=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 1397/1397 [00:35<00:00, 39.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVD with n_factors=350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 1397/1397 [00:37<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVD with n_factors=400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD...: 100%|██████████| 1397/1397 [00:40<00:00, 34.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_factors: 300 with nDCG: 0.0038533284949486156\n",
      "n_factors: 200, nDCG: 0.0028261210720671366\n",
      "n_factors: 250, nDCG: 0.0031378753300206658\n",
      "n_factors: 300, nDCG: 0.0038533284949486156\n",
      "n_factors: 350, nDCG: 0.0032704443259702373\n",
      "n_factors: 400, nDCG: 0.0030534877575233983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "factors_list = [ 200, 250, 300, 350, 400]\n",
    "top_k = 10\n",
    "\n",
    "best_factors, best_ndcg = find_best_svd_factors(train_interaction_matrix, test_interaction_matrix, factors_list, top_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recommenders': {'SVD': {'predictions': array([[  95,  932, 2698, ..., 2243, 3215,  247],\n",
       "          [ 193, 1553,  233, ..., 1318,   11, 2786],\n",
       "          [ 473, 1171,  599, ..., 1088,  957, 2053],\n",
       "          ...,\n",
       "          [1978, 2315,  187, ..., 1435, 2279, 2697],\n",
       "          [2170, 2662, 3305, ..., 1975,   75,  907],\n",
       "          [2052,  204, 1192, ..., 1590, 2298, 1260]], dtype=int64)},\n",
       "  'ItemKNN': {'predictions': array([[3215, 3505, 2245, ..., 1918, 2243, 1739],\n",
       "          [2786, 1992, 3489, ...,  233, 2327, 1968],\n",
       "          [ 473, 1731, 2191, ..., 1171, 1839,   77],\n",
       "          ...,\n",
       "          [1978, 2315, 3111, ..., 2310,  445, 2594],\n",
       "          [4027, 2170, 2662, ..., 2344, 3087, 1665],\n",
       "          [2530, 1757,   51, ..., 2259, 2254, 3620]], dtype=int64)},\n",
       "  'TopPop': {'predictions': array([[2108, 1667, 1376, ...,  323, 1059,  803],\n",
       "          [2108, 1667, 1376, ...,  323,  324, 1616],\n",
       "          [2108, 1667,  892, ...,  301, 1616, 1059],\n",
       "          ...,\n",
       "          [2108, 1667,  892, ...,  301,  324, 1055],\n",
       "          [2108, 1667, 1376, ...,  301, 1616, 1055],\n",
       "          [2108, 1667,  892, ...,  803, 1059,  324]])}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    U_final, V_final = svd_decompose(config_predict['train_inter'], config_predict['recommenders']['SVD']['n_factors'])\n",
    "    for user_id in tqdm (range(config_predict['train_inter'].shape[0]), desc=\"SVD...\"):\n",
    "        seen_item_ids = np.where(config_predict['train_inter'][user_id] > 0)[0] \n",
    "        get_recommendations_for_svd = svd_recommend_to_list(user_id, seen_item_ids, U_final, V_final, config_predict['top_k'])\n",
    "        recommendations['recommenders']['SVD']['predictions'].append([get_recommendations_for_svd])\n",
    "    recommendations['recommenders']['SVD']['predictions'] = np.vstack(recommendations['recommenders']['SVD']['predictions'])\n",
    "except Exception as e:\n",
    "    print(\"SVD Decomposition Failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users.index: RangeIndex(start=0, stop=2795, step=1)\n",
      "items.index: RangeIndex(start=0, stop=4178, step=1)\n",
      "res shape: (2795, 4178)\n",
      "dataset name: count\n",
      "user ids pd:0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: user_id, dtype: int64\n",
      "item ids pd:0     341\n",
      "1     393\n",
      "2     331\n",
      "3     343\n",
      "4     326\n",
      "5     345\n",
      "6     342\n",
      "7     332\n",
      "8     340\n",
      "9    2622\n",
      "Name: item_id, dtype: int64\n",
      "listening_events pd:0    7\n",
      "1    2\n",
      "2    2\n",
      "3    6\n",
      "4    2\n",
      "5    5\n",
      "6    5\n",
      "7    3\n",
      "8    8\n",
      "9    2\n",
      "Name: count, dtype: int64\n",
      "row user ids np:[100 100 100 100 100 100 100 100 100 100] shape: (75860,)\n",
      "col item ids np:[ 341  393  331  343  326  345  342  332  340 2622] shape: (75860,)\n",
      "listening_events np:[7 2 2 6 2 5 5 3 8 2]\n",
      "users.index: RangeIndex(start=0, stop=2795, step=1)\n",
      "items.index: RangeIndex(start=0, stop=4178, step=1)\n",
      "res shape: (2795, 4178)\n",
      "dataset name: count\n",
      "user ids pd:0    100\n",
      "1    100\n",
      "2    100\n",
      "3    100\n",
      "4    100\n",
      "5    100\n",
      "6    100\n",
      "7    100\n",
      "8    100\n",
      "9    100\n",
      "Name: user_id, dtype: int64\n",
      "item ids pd:0     333\n",
      "1     357\n",
      "2     329\n",
      "3     349\n",
      "4     324\n",
      "5     133\n",
      "6    3015\n",
      "7     132\n",
      "8     358\n",
      "9     363\n",
      "Name: item_id, dtype: int64\n",
      "listening_events pd:0     2\n",
      "1     3\n",
      "2    18\n",
      "3     4\n",
      "4    10\n",
      "5     4\n",
      "6     2\n",
      "7    12\n",
      "8     5\n",
      "9     2\n",
      "Name: count, dtype: int64\n",
      "row user ids np:[100 100 100 100 100 100 100 100 100 100] shape: (25695,)\n",
      "col item ids np:[ 333  357  329  349  324  133 3015  132  358  363] shape: (25695,)\n",
      "listening_events np:[ 2  3 18  4 10  4  2 12  5  2]\n",
      "Reduced train interaction matrix shape: (1397, 2089)\n",
      "Reduced test interaction matrix shape: (1397, 2089)\n"
     ]
    }
   ],
   "source": [
    "t_reduce_data_size = True\n",
    "reduce_data_size_n_times = 2\n",
    "\n",
    "users = read('lfm-challenge','user')\n",
    "items = read('lfm-challenge','item')\n",
    "train_inters = read('lfm-challenge','inter_train')\n",
    "test_inters = read('lfm-challenge','inter_test')\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=train_inters,\n",
    "                                               dataset_name=\"lfm-challenge\")\n",
    "test_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=test_inters,\n",
    "                                              dataset_name=\"lfm-challenge\")\n",
    "\n",
    "if t_reduce_data_size:\n",
    "    # Reduce the size of the matrices by 1/10\n",
    "    train_interaction_matrix = reduce_matrix_size(train_interaction_matrix, reduction_factor=reduce_data_size_n_times)\n",
    "    test_interaction_matrix = reduce_matrix_size(test_interaction_matrix, reduction_factor=reduce_data_size_n_times)\n",
    "\n",
    "    # Output the shapes of the reduced matrices to verify\n",
    "    print(f\"Reduced train interaction matrix shape: {train_interaction_matrix.shape}\")\n",
    "    print(f\"Reduced test interaction matrix shape: {test_interaction_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=1\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:30<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=2\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:50<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=3\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:55<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=4\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:53<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=5\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:52<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=6\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:54<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=7\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:54<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=8\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:57<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=9\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:55<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=10\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:54<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=11\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:57<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=12\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:49<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=13\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:46<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=14\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:48<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ItemKNN with n_neighbours=15\n",
      "Using 20 cores for parallel processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ItemKNN...: 100%|██████████| 1397/1397 [08:41<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbours: 6 with nDCG: 0.003258857416320537\n",
      "n_neighbours: 1, nDCG: 0.002987049209126641\n",
      "n_neighbours: 2, nDCG: 0.0029626966599032556\n",
      "n_neighbours: 3, nDCG: 0.0027025609902983877\n",
      "n_neighbours: 4, nDCG: 0.0028199246381583843\n",
      "n_neighbours: 5, nDCG: 0.0029703049197391833\n",
      "n_neighbours: 6, nDCG: 0.003258857416320537\n",
      "n_neighbours: 7, nDCG: 0.002867639456667905\n",
      "n_neighbours: 8, nDCG: 0.00289280664622744\n",
      "n_neighbours: 9, nDCG: 0.002839967264984375\n",
      "n_neighbours: 10, nDCG: 0.0027056353541941277\n",
      "n_neighbours: 11, nDCG: 0.002670809820431408\n",
      "n_neighbours: 12, nDCG: 0.0026562277480182746\n",
      "n_neighbours: 13, nDCG: 0.002739895269630877\n",
      "n_neighbours: 14, nDCG: 0.002629719462819059\n",
      "n_neighbours: 15, nDCG: 0.0025896448478315705\n",
      "Best ItemKNN neighbours: 6 with nDCG: 0.003258857416320537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "neighbours_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "top_k = 10\n",
    "\n",
    "best_neighbours, best_ndcg = find_best_itemknn_neighbours(train_interaction_matrix, test_interaction_matrix, neighbours_list, top_k)\n",
    "print(f\"Best ItemKNN neighbours: {best_neighbours} with nDCG: {best_ndcg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
