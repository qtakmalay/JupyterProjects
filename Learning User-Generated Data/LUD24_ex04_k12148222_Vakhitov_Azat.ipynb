{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef6227e",
   "metadata": {},
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2024*\n",
    "# Exercise 4: Evaluation\n",
    "\n",
    "In this exercise we evaluate accuracy of three different RecSys we already implemented. First we implement DCG and nDCG metrics, then we create a simple evaluation framework to compare the three recommenders in terms of nDCG. The implementations for the three recommender systems are provided in a file rec.py and are imported later in the notebook.\n",
    "Please consult the lecture slides and the presentation from UE Session 4 for a recap.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD24_ex04_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD24_ex04_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, as before, you are required to write a number of functions. Only implemented functions are graded. Insert your implementations into the templates provided. Please don't change the templates even if they are not pretty. Don't forget to test your implementation for correctness and efficiency. **Make sure to try your implementations on toy examples and sanity checks.**\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fe8120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35c1c9",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 1/2</font>: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9af194",
   "metadata": {},
   "source": [
    "Implement DCG and nDCG in the corresponding templates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6360ca",
   "metadata": {},
   "source": [
    "### DCG Score\n",
    "Implement DCG following the input/output convention:\n",
    "#### Input:\n",
    "* predictions - (not an interaction matrix!) numpy array with recommendations. Row index corresponds to User_id, column index corresponds to the rank of the item mentioned in the cell. Every cell (i,j) contains **item id** recommended to the user (i) on the position (j) in the list. For example:\n",
    "\n",
    "The following predictions structure [[12, 7, 99], [0, 97, 6]] means that the user with id==1 (second row) got recommended item **0** on the top of the list, item **97** on the second place and item **6** on the third place.\n",
    "\n",
    "* test_interaction_matrix - (plain interaction matrix format as before!) interaction matrix constructed from interactions held out as a test set, rows - users, columns - items, cells - 0 or 1\n",
    "\n",
    "* topK - integer - top \"how many\" to consider for the evaluation. By default top 10 items are to be considered\n",
    "\n",
    "#### Output:\n",
    "* DCG score\n",
    "\n",
    "Don't forget, DCG is calculated for every user separately and then the average is returned.\n",
    "\n",
    "\n",
    "<font color='red'>**Attention!**</font> Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d607a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - 2D np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - 2D np.ndarray, test interaction matrix for each user;\n",
    "    \n",
    "    returns - float, mean dcg score over all user;\n",
    "    \"\"\"\n",
    "    num_users = predictions.shape[0]\n",
    "    dcgs = np.zeros(num_users)\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        gains = np.zeros(topK)\n",
    "        \n",
    "        for rank in range(min(topK, predictions.shape[1])):\n",
    "            item_id = predictions[user_id, rank]\n",
    "            relevance = test_interaction_matrix[user_id, item_id]\n",
    "            # 1/log(rank+1)\n",
    "            if relevance == 1:\n",
    "                if rank == 0:\n",
    "                    gains[rank] = 1\n",
    "                else:\n",
    "                    gains[rank] = 1 / np.log2(rank + 1)\n",
    "            \n",
    "        dcgs[user_id] = np.sum(gains)\n",
    "\n",
    "    mean_dcg = np.mean(dcgs)\n",
    "    return mean_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376794a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "dcg_score = get_dcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(dcg_score, 1), \"1 expected\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afdf65",
   "metadata": {},
   "source": [
    "* Can DCG score be higher than 1?\n",
    "* Can the average DCG score be higher than 1?\n",
    "* Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef924fee",
   "metadata": {},
   "source": [
    "### nDCG Score\n",
    "\n",
    "Following the same parameter convention as for DCG implement nDCG metric.\n",
    "\n",
    "<font color='red'>**Attention!**</font> Remember that ideal DCG is calculated separately for each user and depends on the number of tracks held out for them as a Test set! Use logarithm with base 2 for discounts! Remember that the top1 recommendation shouldn't get discounted!\n",
    "\n",
    "<font color='red'>**Note:**</font> nDCG is calculated for **every user separately** and then the average is returned. You do not necessarily need to use the function you implemented above. Writing nDCG from scratch might be a good idea as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f2ec6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray, predictions of the recommendation algorithm for each user;\n",
    "    test_interaction_matrix - np.ndarray, test interaction matrix for each user;\n",
    "    topK - int, topK recommendations should be evaluated;\n",
    "    \n",
    "    returns - float, average ndcg score over all users;\n",
    "    \"\"\"\n",
    "    num_users = predictions.shape[0]\n",
    "    ndcgs = np.zeros(num_users)\n",
    "\n",
    "    for user_id in range(num_users):\n",
    "        user_predictions = predictions[user_id, :topK]\n",
    "        relevances = test_interaction_matrix[user_id, user_predictions]\n",
    "\n",
    "        actual_topK = min(topK, len(user_predictions))\n",
    "        \n",
    "        if actual_topK > 1:\n",
    "            discounts = np.log2(np.arange(2, actual_topK + 1))\n",
    "            dcg = relevances[0] + np.sum(relevances[1:actual_topK] / discounts)\n",
    "        else:\n",
    "            dcg = relevances[0]\n",
    "\n",
    "        sorted_relevances = np.sort(test_interaction_matrix[user_id])[-actual_topK:][::-1]\n",
    "        if actual_topK > 1:\n",
    "            idcg = sorted_relevances[0] + np.sum(sorted_relevances[1:actual_topK] / discounts)\n",
    "        else:\n",
    "            idcg = sorted_relevances[0] \n",
    "        \n",
    "        if idcg == 0:\n",
    "            ndcgs[user_id] = 0\n",
    "        else:\n",
    "            ndcgs[user_id] = dcg / idcg\n",
    "    \n",
    "    average_ndcg = np.mean(ndcgs)\n",
    "    return average_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db41b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([[0, 1, 2, 3], [3, 2, 1, 0]])\n",
    "test_interaction_matrix = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "\n",
    "ndcg_score = get_ndcg_score(predictions, test_interaction_matrix, topK=4)\n",
    "\n",
    "assert np.isclose(ndcg_score, 1), \"ndcg score is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34472609",
   "metadata": {},
   "source": [
    "* Can nDCG score be higher than 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22655bbf",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 2/2</font>: Evaluation\n",
    "Use provided rec.py (see imports below) to build a simple evaluation framework. It should be able to evaluate POP, ItemKNN and SVD.\n",
    "\n",
    "\n",
    "*Make sure to place provided rec.py next to your notebook for the imports to work.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222a425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rec import svd_decompose, svd_recommend_to_list  #SVD\n",
    "from rec import inter_matr_implicit\n",
    "from rec import recTopK  #ItemKNN\n",
    "from rec import recTopKPop  #TopPop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32d6a9b7",
   "metadata": {},
   "source": [
    "Load the users, items and both the train interactions and test interactions\n",
    "from the **new version of the lfm-tiny-tunes dataset** provided with the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "276fea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "# TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "users = read('lfm-tiny-tunes','user')\n",
    "items = read('lfm-tiny-tunes','item')\n",
    "train_inters = read('lfm-tiny-tunes','inter_train')\n",
    "test_inters = read('lfm-tiny-tunes','inter_test')\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=train_inters,\n",
    "                                               dataset_name=\"lfm-tiny-tunes\")\n",
    "test_interaction_matrix = inter_matr_implicit(users=users, items=items, interactions=test_inters,\n",
    "                                              dataset_name=\"lfm-tiny-tunes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e414bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Recommendations\n",
    "\n",
    "Implement the function below to get recommendations from all 3 recommender algorithms. Make sure you use the provided config dictionary and pay attention to the structure for the output dictionary - we will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47964245",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_predict = {\n",
    "    #interaction matrix\n",
    "    \"train_inter\": train_interaction_matrix,\n",
    "    #topK parameter used for all algorithms\n",
    "    \"top_k\": 10,\n",
    "    #specific parameters for all algorithms\n",
    "    \"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            \"n_factors\": 50\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"n_neighbours\": 5\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52b7a07a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations_for_algorithms(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, already predefined below with name \"rec_dict\";\n",
    "    \"\"\"\n",
    "\n",
    "    #use this structure to return results\n",
    "    rec_dict = {\"recommenders\": {\n",
    "        \"SVD\": {\n",
    "            #Add your predictions here\n",
    "            \"predictions\": np.array([])\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "            \"predictions\": np.array([])\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "            \"predictions\": np.array([])\n",
    "        },\n",
    "    }}\n",
    "\n",
    "    # SVD \n",
    "    try:\n",
    "        U_final, V_final = svd_decompose(config['train_inter'], config['recommenders']['SVD']['n_factors'])\n",
    "        for user_id in range(config['train_inter'].shape[0]):\n",
    "            seen_item_ids = np.where(config['train_inter'][user_id] > 0)[0]  # Items the user has interacted with\n",
    "            recommendations = svd_recommend_to_list(user_id, seen_item_ids, U_final, V_final, config['top_k'])\n",
    "            rec_dict['recommenders']['SVD']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['SVD']['predictions'] = np.vstack(rec_dict['recommenders']['SVD']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"SVD Decomposition Failed:\", str(e))\n",
    "\n",
    "    # ItemKNN \n",
    "    try:\n",
    "        for user_id in range(config['train_inter'].shape[0]):\n",
    "            recommendations = recTopK(config['train_inter'], user_id, config['top_k'], config['recommenders']['ItemKNN']['n_neighbours'])\n",
    "            rec_dict['recommenders']['ItemKNN']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['ItemKNN']['predictions'] = np.vstack(rec_dict['recommenders']['ItemKNN']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"ItemKNN Recommendation Failed:\", str(e))\n",
    "\n",
    "    # TopPop \n",
    "    try:\n",
    "        for user_id in range(config['train_inter'].shape[0]):\n",
    "            recommendations = recTopKPop(config['train_inter'], user_id, config['top_k'])\n",
    "            rec_dict['recommenders']['TopPop']['predictions'].append(recommendations)\n",
    "        rec_dict['recommenders']['TopPop']['predictions'] = np.vstack(rec_dict['recommenders']['TopPop']['predictions'])\n",
    "    except Exception as e:\n",
    "        print(\"TopPop Recommendation Failed:\", str(e))\n",
    "\n",
    "    return rec_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b78eab7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Decomposition Failed: 'numpy.ndarray' object has no attribute 'append'\n",
      "ItemKNN Recommendation Failed: 'numpy.ndarray' object has no attribute 'append'\n",
      "TopPop Recommendation Failed: 'numpy.ndarray' object has no attribute 'append'\n"
     ]
    }
   ],
   "source": [
    "recommendations = get_recommendations_for_algorithms(config_predict)\n",
    "\n",
    "assert \"SVD\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"SVD\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"SVD\"][\"predictions\"], np.ndarray)\n",
    "assert \"ItemKNN\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"ItemKNN\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"ItemKNN\"][\"predictions\"], np.ndarray)\n",
    "assert \"TopPop\" in recommendations[\"recommenders\"] and \"predictions\" in recommendations[\"recommenders\"][\"TopPop\"]\n",
    "assert isinstance(recommendations[\"recommenders\"][\"TopPop\"][\"predictions\"], np.ndarray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26752d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1215 entries, 0 to 1214\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   user_id              1215 non-null   int64 \n",
      " 1   country              972 non-null    object\n",
      " 2   age_at_registration  1215 non-null   int64 \n",
      " 3   gender               1213 non-null   object\n",
      " 4   registration_date    1215 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 47.6+ KB\n",
      "None\n",
      "   user_id country  age_at_registration gender    registration_date\n",
      "0        0      RU                   25      m  2006-06-12 13:25:12\n",
      "1        1      US                   23      m  2005-08-18 15:25:41\n",
      "2        2      FR                   25      m  2006-02-26 22:39:03\n",
      "3        3      DE                    2      m  2007-02-28 10:12:13\n",
      "4        4      UA                   23      n  2007-10-09 15:21:20\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 394 entries, 0 to 393\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   artist   394 non-null    object\n",
      " 1   track    394 non-null    object\n",
      " 2   item_id  394 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 9.4+ KB\n",
      "None\n",
      "                artist                                   track  item_id\n",
      "0           Black Flag                              Rise Above        0\n",
      "1                 Blur  For Tomorrow - 2012 Remastered Version        1\n",
      "2          Damien Rice                            Moody Mooday        2\n",
      "3                 Muse                            Feeling Good        3\n",
      "4  My Bloody Valentine                                    Soon        4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9178 entries, 0 to 9177\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   user_id           9178 non-null   int64\n",
      " 1   item_id           9178 non-null   int64\n",
      " 2   listening_events  9178 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 215.2 KB\n",
      "None\n",
      "   user_id  item_id  listening_events\n",
      "0      510       50                 3\n",
      "1      510      324                 5\n",
      "2      510       80                 4\n",
      "3      510      266                 3\n",
      "4      510      152                 2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2878 entries, 0 to 2877\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   user_id           2878 non-null   int64\n",
      " 1   item_id           2878 non-null   int64\n",
      " 2   listening_events  2878 non-null   int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 67.6 KB\n",
      "None\n",
      "   user_id  item_id  listening_events\n",
      "0      510        0                 6\n",
      "1      510       23                 2\n",
      "2      699       54                22\n",
      "3      699       55                 4\n",
      "4      699       43                 3\n"
     ]
    }
   ],
   "source": [
    "users = pd.read_csv('lfm-tiny-tunes/lfm-tiny-tunes.user', sep='\\t')\n",
    "\n",
    "print(users.info())\n",
    "print(users.head())\n",
    "\n",
    "items = pd.read_csv('lfm-tiny-tunes/lfm-tiny-tunes.item', sep='\\t')\n",
    "\n",
    "print(items.info())\n",
    "print(items.head())\n",
    "\n",
    "train_inters = pd.read_csv('lfm-tiny-tunes/lfm-tiny-tunes.inter_train', sep='\\t')\n",
    "\n",
    "print(train_inters.info())\n",
    "print(train_inters.head())\n",
    "\n",
    "test_inters = pd.read_csv('lfm-tiny-tunes/lfm-tiny-tunes.inter_test', sep='\\t')\n",
    "\n",
    "print(test_inters.info())\n",
    "print(test_inters.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf7a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluate Recommendations\n",
    "\n",
    "Implement the function such that it evaluates the previously generated recommendations. Make sure you use the provided config dictionary. **DO NOT** load it directly from the *config_test*. Pay attention to the structure for the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7478da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_test = {\n",
    "    \"top_k\": 10,\n",
    "    \"test_inter\": test_interaction_matrix,\n",
    "    \"recommenders\": {}  # here you can access the recommendations from get_recommendations_for_algorithms\n",
    "\n",
    "}\n",
    "# add dictionary with recommendations to config dictionary\n",
    "config_test.update(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c0fd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithms(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    config - dict, configuration as defined above;\n",
    "\n",
    "    returns - dict, { Recommender Key from input dict: { \"ndcg\": float - ndcg from evaluation for this recommender} };\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"SVD\": {\n",
    "        },\n",
    "        \"ItemKNN\": {\n",
    "        },\n",
    "        \"TopPop\": {\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36d4b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating Every Algorithm\n",
    "Make sure everything works.\n",
    "We expect KNN to outperform other algorithms on our small data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f672c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m evaluate_algorithms(config_test)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItemKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopPop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopPop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(evaluations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopPop\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndcg\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluations = evaluate_algorithms(config_test)\n",
    "\n",
    "assert \"SVD\" in evaluations and \"ndcg\" in evaluations[\"SVD\"] and isinstance(evaluations[\"SVD\"][\"ndcg\"], float)\n",
    "assert \"ItemKNN\" in evaluations and \"ndcg\" in evaluations[\"ItemKNN\"] and isinstance(evaluations[\"ItemKNN\"][\"ndcg\"], float)\n",
    "assert \"TopPop\" in evaluations and \"ndcg\" in evaluations[\"TopPop\"] and isinstance(evaluations[\"TopPop\"][\"ndcg\"], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bfaee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for recommender in evaluations.keys():\n",
    "    print(f\"{recommender} ndcg: {evaluations[recommender]['ndcg']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191839c",
   "metadata": {},
   "source": [
    "## Questions and Potential Future Work\n",
    "* How would you try improve performance of all three algorithms?\n",
    "* What other metrics would you consider to compare these recommender systems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ad182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
