{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Azat Vakhitov | 12148222 | 23.01.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI I</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 6 (Assignment) &ndash; Convolutional Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> Brandstetter, Schäfl, Schlüter, Rumetshofer, Schörgenhumer<br>\n",
    "<b>Date:</b> 09-01-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI I\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which require your contribution (in form of code, plain text, ...). Most/All of the supplied functions are imported from the file <code>u6_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u6_utils.py</code> need to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> When specifying a seed for the sources of randomness, use the <code>u6.set_seed(seed=XYZ)</code> function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5768\\3106679905.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Required packages and the u6_utils file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mu6_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mu6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\JupyterProjects\\U6\\u6_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Required packages and the u6_utils file\n",
    "import u6_utils as u6\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "u6.check_module_versions()\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u6.setup_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Similar to the Sobel filter presented in the lecture, apply two other operators (<b>Prewitt</b> and <b>Scharr</b>) to an image for the horizontal and the vertical approximations of the derivatives of the image intensity function. Perfom the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.1. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Load a picture of a road (<code>white_lines.jpg</code>), convert it to grayscale and plot the grayscale image to see what it looks like.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread(\"resources/white_lines.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "u6.show_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.2. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define the two Prewitt operators as numpy arrays:\n",
    "\n",
    "\\begin{equation}P_x = \\left[\n",
    "    \\begin{array}{rrr}                                \n",
    "        1 & 0 & -1 \\\\\n",
    "        1 & 0 & -1 \\\\\n",
    "        1 & 0 & -1 \\\\\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}P_y = \\left[\n",
    "    \\begin{array}{rrr}                                \n",
    "        1 & 1 & 1 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        -1 & -1 & -1 \\\\\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prewitt_x = np.array([\n",
    "    [ 1,  0, -1],\n",
    "    [ 1,  0, -1],\n",
    "    [ 1,  0, -1]\n",
    "])\n",
    "prewitt_y = np.array([\n",
    "    [ 1, 1, 1],\n",
    "    [ 0, 0, 0],\n",
    "    [ -1, -1, -1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.3. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Apply the two Prewitt operators $P_x$ and $P_y$ to the grayscale version of the above image and plot the results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_image = cv2.filter2D(image, -1, prewitt_x)  # add [1:-1, 1:-1] to extract the unpadded data\n",
    "u6.show_image(filtered_image)\n",
    "filtered_image = cv2.filter2D(image, -1, prewitt_y)  # add [1:-1, 1:-1] to extract the unpadded data\n",
    "u6.show_image(filtered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.4. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Which parts of the resulting images above are highlighted? Do you observe any differences between the two Prewitt operators?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to a grayscale image, the resulting image emphasizes the edges of the image. The Prewitt_x filter finds x-axis edges and the Prewitt_y filter finds y-axis edges. For street grayscale images, Prewitt_x emphasizes horizontal edges, while Prewitt_y de-emphasizes them to capture vertical edges that are not present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.5. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define the two Scharr operators as numpy arrays:\n",
    "\n",
    "\\begin{equation}S_x = \\left[\n",
    "    \\begin{array}{rrr}                                \n",
    "        47 & 0 & -47 \\\\\n",
    "        162 & 0 & -162 \\\\\n",
    "        47 & 0 & -47 \\\\\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}S_y = \\left[\n",
    "    \\begin{array}{rrr}                                \n",
    "        47 & 162 & 47 \\\\\n",
    "        0 & 0 & 0 \\\\\n",
    "        -47 & -162 & -47 \\\\\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scharr_x = np.array([\n",
    "    [ 47,  0, -47],\n",
    "    [ 162,  0, -162],\n",
    "    [ 47,  0, -47]\n",
    "])\n",
    "scharr_y = np.array([\n",
    "    [ 47, 162, 47],\n",
    "    [ 0, 0, 0],\n",
    "    [ -47, -162, -47]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.6. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Apply the two operators $S_x$ and $S_y$ to the grayscale version of the above image and plot the results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_image = cv2.filter2D(image, -1, scharr_x)  # add [1:-1, 1:-1] to extract the unpadded data\n",
    "u6.show_image(filtered_image)\n",
    "filtered_image = cv2.filter2D(image, -1, scharr_y)  # add [1:-1, 1:-1] to extract the unpadded data\n",
    "u6.show_image(filtered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.7. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Which parts of the resulting images above are highlighted? Do you observe any differences between the two Scharr operators?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image produced by the Scharr_x operator is more sensitive on edges in the horizontal direction, and the image produced by the Scharr_y  is more sensitive on the edges in the vertical axis. The Scharr is usually sensetive in the diagonal edges than the Sobel operator, thus, you can see more details in the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.8. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Do you observe any differences between the Prewitt and the Scharr operators?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scharr operator seems to make more of the image visible, including diagonal edges, while the Prewitt operator looks like it only highlights very little and only some distinct objects. Depending on the task, one operator may be more suitable than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 2</h2>\n",
    "\n",
    "Up until now, custom filters were defined and applied on images, but as you certainly noticed, no neural network was involed so far. The gist of this exercise is to change that and get more insight into how <b>convolutional neural networks</b> operate. For this reason, perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.1. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define a custom filter as a numpy array:\n",
    "\n",
    "\\begin{equation}F = \\left[\n",
    "    \\begin{array}{rrrrr}                                \n",
    "        1 &    1 &    1 &     1 &  0 \\\\\n",
    "        1 & 0.75 &  0.5 &     0 & -1 \\\\\n",
    "        1 &  0.5 &    0 &  -0.5 & -1 \\\\\n",
    "        1 &    0 & -0.5 & -0.75 & -1 \\\\\n",
    "        0 &   -1 &   -1 &    -1 & -1\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}</li>\n",
    "        <li>Create three additional copies of the custom filter and apply the following transformations: negation, rotation by $90$ degrees, negation + rotation by $90$ degrees.</li>\n",
    "        <li>Visualize all four filters.</li>\n",
    "        <li>Hint: You can rotate a numpy array by $90$ degrees with <code>np.rot90(some_array)</code>.</li>\n",
    "        <li>Hint: Your final array of filters should be of the form $\\left[F, -F, F_{rot}, -F_{rot}\\right]$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filter 1\n",
    "filter_1 = np.array([\n",
    "    [1 , 1 , 1 , 1 , 0], \n",
    "    [1 , 0.75 , 0.5 , 0 , -1], \n",
    "    [1 , 0.5 , 0 , -0.5 , -1], \n",
    "    [1 , 0 , -0.5 , -0.75 , -1], \n",
    "    [0 , -1 , -1 , -1 , -1]\n",
    "])\n",
    "filter_2 = -filter_1\n",
    "filter_3 = np.rot90(filter_1)\n",
    "filter_4 = -filter_3\n",
    "filters = np.array([filter_1, filter_2, filter_3, filter_4])\n",
    "\n",
    "u6.visualize_filters(filters)\n",
    "print(f\"Filter shape: {filter_1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.2. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Which structures do you think these filters might detect?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These custom filters and different variations of it can are used detect different structures in the image such as trees, lights, cars, roads, lanes, edges, buildings, signs, poles, and shadows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.3. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create and initialize a neural network of type <code>u6.InitializedNet</code> using the four filters.</li>\n",
    "        <li>Use ReLU as activation function.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = u6.InitializedNet(weights=filters, activation=torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.4. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Apply the neural network to the grayscale version of the image above to retrieve the layer outputs.</li>\n",
    "        <li>In contrast to the previous tasks, use <code>get_grayscale_image_tensor</code> to load the image (again, <code>white_lines.jpg</code>).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = u6.get_grayscale_image_tensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.5. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Plot the output of the convolutional layer and the output of the activation layer.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer, activated_layer,  _  = model(image_tensor)\n",
    "# Visualize the outputs of these layers\n",
    "u6.visualize_cnn_layer(conv_layer, title=\"Output of Convolutional Layer\")\n",
    "u6.visualize_cnn_layer(activated_layer, title=\"Output of Activation Layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.6. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Which parts of the image are highlighted? Do you observe any differences between the filters (with respect to each other) as well as between the two layers?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main difference between filters is the rotation between the two layers. The difference  between the convolutional layer and the activation layer, is not significant, except the color. The convolutional layer highlights the structures in the image, and the activation layer further emphasizes the highlighted structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Exercise 3</h2>\n",
    "\n",
    "In this exercise, we want to compare the output of a max pooling layer with the output of a strided convolution. Perform the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.1. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create and initialize four neural networks of type <code>u6.InitializedNet</code> using the four filters from exercise 2.</li>\n",
    "        <li>Use ReLU as activation function.</li>\n",
    "        <li>Additionally, configure the following hyperparameters:</li>\n",
    "        <ul>\n",
    "            <li>Model 1: <b>max pooling</b> (parameter <code>max_pool_size</code>) with height/width = $2$, don't define a stride value.</li>\n",
    "            <li>Model 2: <b>stride</b> (parameter <code>filter_stride</code>) = $2$, don't define a max pooling size.</li>\n",
    "            <li>Model 3: <b>max pooling</b> with height/width = $8$, don't define a stride value.</li>\n",
    "            <li>Model 4: <b>stride</b> = $8$, don't define a max pooling size.</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = u6.InitializedNet(weights=filters, max_pool_size=(2,2), activation=torch.nn.ReLU())\n",
    "model_2 = u6.InitializedNet(weights=filters, filter_stride=2, activation=torch.nn.ReLU())\n",
    "model_3 = u6.InitializedNet(weights=filters, max_pool_size=(8, 8), activation=torch.nn.ReLU())\n",
    "model_4 = u6.InitializedNet(weights=filters, filter_stride = 8, activation=torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.2. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Apply the four neural networks to the grayscale version of the image above to retrieve the layer outputs.</li>\n",
    "        <li>Use <code>get_grayscale_image_tensor</code> to load the image (again, <code>white_lines.jpg</code>).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = u6.get_grayscale_image_tensor(image)\n",
    "\n",
    "conv_layer1, activated_layer1,  _  = model_1(image_tensor)\n",
    "u6.visualize_cnn_layer(conv_layer1, title=\"Output of Convolutional Layer 1\")\n",
    "u6.visualize_cnn_layer(activated_layer1, title=\"Output of Activation Layer 1\")\n",
    "\n",
    "conv_layer2, activated_layer2,  _  = model_2(image_tensor)\n",
    "u6.visualize_cnn_layer(conv_layer2, title=\"Output of Convolutional Layer 2\")\n",
    "u6.visualize_cnn_layer(activated_layer2, title=\"Output of Activation Layer 2\")\n",
    "\n",
    "conv_layer3, activated_layer3,  _  = model_3(image_tensor)\n",
    "u6.visualize_cnn_layer(conv_layer3, title=\"Output of Convolutional Layer 3\")\n",
    "u6.visualize_cnn_layer(activated_layer3, title=\"Output of Activation Layer 3\")\n",
    "\n",
    "conv_layer4, activated_layer4,  _  = model_4(image_tensor)\n",
    "u6.visualize_cnn_layer(conv_layer4, title=\"Output of Convolutional Layer 4\")\n",
    "u6.visualize_cnn_layer(activated_layer4, title=\"Output of Activation Layer 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.3. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Kernel size $2$ comparison: Plot the output of the max pooling layer of model 1 and the output of the activation layer of model 2.</li>\n",
    "        <li>When plotting, set <code>clip=True</code>. This cuts off values larger than 255 to yield a more brighter looking output. Note that this is purely for visualization purposes, the original values in the CNN are unchanged.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer, activated_layer, pooling_layer = model_1(image_tensor)\n",
    "conv_layer2, activated_layer2, pooling_layer2 = model_2(image_tensor)\n",
    "u6.visualize_cnn_layer(pooling_layer, title=\"Max pooling layer of model 1\", clip=True)\n",
    "u6.visualize_cnn_layer(activated_layer2, title=\"Max pooling layer of model 2\", clip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.4. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Kernel size $8$ comparison: Plot the output of the max pooling layer of model 3 and the output of the activation layer of model 4.</li>\n",
    "        <li>When plotting, set <code>clip=True</code>. This cuts off values larger than 255 to yield a more brighter looking output. Note that this is purely for visualization purposes, the original values in the CNN are unchanged.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer3, activated_layer3, pooling_layer3 = model_3(image_tensor)\n",
    "conv_layer4, activated_layer4, pooling_layer4 = model_4(image_tensor)\n",
    "u6.visualize_cnn_layer(pooling_layer3, title=\"Max pooling layer of model 1\", clip=True)\n",
    "u6.visualize_cnn_layer(activated_layer4, title=\"Max pooling layer of model 2\", clip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.5. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Which parts of the image are highlighted? Do you observe any differences between striding and max pooling? Do you observe any differences between the two pooling sizes and the two stride values?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out image show that the stride highlight is more than the max poling, and the max pooling with kernel size  2 and stride  2 has  more information than the max pooling with kernel size = 8 and stride = 8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Exercise 4</h2>\n",
    "\n",
    "As you are now an expert in defining and applying custom kernels, we will now move on to learning kernels to create more expressive systems. \n",
    "\n",
    "Following the instruction given in the lecture notebook, perform the tasks below, but this time, using the <b>CIFAR10</b> dataset (more information about the dataset can be found here: https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "To load the CIFAR10 dataset and take a look at a preview of $10$ samples, run the cell below. The CIFAR10 dataset contains RGB images with a resolution of $32 \\times 32$ pixels from $10$ different classes. \n",
    "\n",
    "**Important:** The first time you run this, it will download the dataset. You may see a <code>UserWarning: The given NumPy array is not writeable</code>. This can be safely ignored. The download itself might take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset with custom batch size\n",
    "train_loader, valid_loader, test_loader = u6.get_dataset(\n",
    "    variant=\"CIFAR10\",\n",
    "    root=\"resources\",\n",
    "    batch_size=10,\n",
    "    valid_size=0.1\n",
    ")\n",
    "\n",
    "# load the first batch of data (set seed for reproducibility)\n",
    "u6.set_seed(22)\n",
    "images, labels = next(iter(train_loader))\n",
    "# transform the image shapes and normalize their values ([0, 1]) for visualization purposes\n",
    "images = np.concatenate([img.squeeze() for img in images], axis=2).transpose(1, 2, 0)\n",
    "images = (images - images.min()) / (images.max() - images.min())\n",
    "\n",
    "# display the first batch of data\n",
    "with plt.style.context({\"axes.grid\": False, \"xtick.bottom\": False}):\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    plt.imshow(images)\n",
    "    plt.xticks(16 + np.arange(len(labels)) * 32, labels.numpy())\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.1. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Reload the CIFAR10 dataset with a batch size of $32$ and considering $15\\%$ of the samples as validation set.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = u6.get_dataset(\n",
    "    variant=\"CIFAR10\",\n",
    "    root=\"resources\",\n",
    "    batch_size=32,\n",
    "    valid_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.2. [13 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create and train a model according to the MNIST example provided in the notebook from the lecture. Keep the structure (ordering and number of layers) and hyperparameters if not stated otherwise. Change the model in a way that:</li>\n",
    "        <ul>\n",
    "            <li>it can process the RGB images from the CIFAR10 dataset as input</li>\n",
    "            <li>its convolutional layer applies kernels with size $5$ and outputs $12$ feature maps</li>\n",
    "            <li>max pooling applies kernels with size $2$</li>\n",
    "            <li>the number of inputs of the linear layer is adapted correctly after flattening the feature maps</li>\n",
    "            <li>its first fully connected layer consists of $100$ neurons</li>\n",
    "        </ul>\n",
    "        <li>Define the loss function.</li>\n",
    "        <li>Then, train the model with the following hyperparameters: iterations = $4$; momentum = $0.1$; and learning rate = $0.1$. Use the function <code>run_gradient_descent()</code> from <code>u6_utils.py</code>.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22). It must be set both before defining the model and before the optimization (in which random samples are drawn). Otherwise, changes in the model would change the train/validation split samples, since both steps use randomization.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader, valid_loader, test_loader = u6.get_dataset(variant=\"MNIST\", root=\"resources\", batch_size=20, valid_size=0.1)\n",
    "u6.set_seed(seed=22)\n",
    "\n",
    "# Reload the CIFAR10 dataset with a batch size of  32  and considering  15%  of the samples as validation set.\n",
    "# train_loader, valid_loader, test_loader = u6.get_dataset(\n",
    "#     variant=\"MNIST\",\n",
    "#     root=\"resources\",\n",
    "#     batch_size=32,\n",
    "#     valid_size=0.15\n",
    "# )\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5),  # conv layer: size changes from (3, 32, 32) to (12, 28, 28); 28 because kernel with size 5 must fit 2 pixels left+right (top+bottom)\n",
    "    torch.nn.ReLU(),                                                 # nonlinearity\n",
    "    torch.nn.MaxPool2d(kernel_size=2),                               # max pooling layer: size changes from (12, 28, 28) to (12, 14, 14)\n",
    "    torch.nn.Flatten(),                                              # flatten activation maps: size changes from (12, 14, 14) to 1D shape of size (3024)\n",
    "    torch.nn.Linear(in_features=12 * 14 * 14, out_features=100),                                # first fully connected layer\n",
    "    torch.nn.ReLU(),                                                 # nonlinearity\n",
    "    torch.nn.Linear(in_features=100, out_features=10)                                         # output layer\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Minimize the given loss for our CNN model (set seed for reproducibility)\n",
    "u6.set_seed(seed=22)\n",
    "losses = u6.run_gradient_descent(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    training_set=train_loader,\n",
    "    valid_set=valid_loader,\n",
    "    iterations=4,\n",
    "    learning_rate=0.1,\n",
    "    momentum=0.1,\n",
    "    use_cuda_if_available=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.3. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Plot the training and validation losses and print the accuracy on the test set.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=losses)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(u6.evaluate_model(model, train_loader, loss=loss, accuracy=u6.multiclass_accuracy))\n",
    "print(\"Test set:\")\n",
    "print(u6.evaluate_model(model, test_loader, loss=loss, accuracy=u6.multiclass_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.4. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Plot the filters from the convolutional layer (layer 0).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u6.visualize_cnn_filters(model[0], ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.5. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>What kind of different structures (or textures) in an image might your CNN detect with these filters?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters 1,4,5,7,8 appears to find darker colors, filter one found the darkest colors, and filter 10 has the brightest ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 5</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.1. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Reload the CIFAR10 dataset with $15\\%$ validation data (the choice of other parameters is up to you).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = u6.get_dataset(\n",
    "    vertical_flip_p=0.1,\n",
    "    variant=\"CIFAR10\",\n",
    "    root=\"resources\",\n",
    "    batch_size=20,\n",
    "    valid_size=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.2. [20 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a model similar to the model from exercise 4. The architecture design is completely up to you.</li>\n",
    "        <li>Define the loss function.</li>\n",
    "        <li>Train the model by using the function <code>run_gradient_descent()</code> from <code>u6_utils.py</code> and choose some appropriate hyperparameters.</li>\n",
    "        <li>Can you optimize the model in order to achieve an accuracy on the test set > $70\\%$? Plot the training and validation losses to show that your model does not overfit to the training data and print out the accuracy on the test to show that it is better than $70\\%$.</li>\n",
    "        <li>For optimization, you can vary several hyperparameters, e.g.: batch size, iterations, learning rate, momentum, number of layers, number of kernels/neurons, and type of non-linearity. Also different types of layers are allowed, e.g., <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\"><code>torch.nn.Dropout()</code></a>. You may also try randomly flipping training images to perform data augmentation (provided by <code>u6.get_dataset()</code>). Do <b>not</b> vary the validation set size, as that would change the training set size.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22). It must be set both before defining the model and before the optimization (in which random samples are drawn). Otherwise, changes in the model would change the train/validation split samples, since both steps use randomization.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u6.set_seed(seed=22)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5),  # conv layer: size changes from (3, 32, 32) to (12, 28, 28); 28 because kernel with size 5 must fit 2 pixels left+right (top+bottom)\n",
    "    torch.nn.ReLU(),                                                 # nonlinearity\n",
    "    torch.nn.MaxPool2d(kernel_size=2),                               # max pooling layer: size changes from (12, 28, 28) to (12, 14, 14)\n",
    "    torch.nn.Flatten(),                                              # flatten activation maps: size changes from (12, 14, 14) to 1D shape of size (3024)\n",
    "    torch.nn.Linear(in_features=12 * 14 * 14, out_features=150),                                # first fully connected layer\n",
    "    torch.nn.Dropout(),                                                # nonlinearity\n",
    "    torch.nn.Linear(150, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=50, out_features=10)                                         # output layer\n",
    ")\n",
    "\n",
    "# Define the loss function\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Minimize the given loss for our CNN model (set seed for reproducibility)\n",
    "u6.set_seed(seed=22)\n",
    "losses = u6.run_gradient_descent(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    training_set=train_loader,\n",
    "    valid_set=valid_loader,\n",
    "    iterations=20,\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.1,\n",
    "    use_cuda_if_available=True\n",
    ")\n",
    "sns.lineplot(data=losses)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training set:\")\n",
    "print(u6.evaluate_model(model, train_loader, loss=loss, accuracy=u6.multiclass_accuracy))\n",
    "print(\"Test set:\")\n",
    "print(u6.evaluate_model(model, test_loader, loss=loss, accuracy=u6.multiclass_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
