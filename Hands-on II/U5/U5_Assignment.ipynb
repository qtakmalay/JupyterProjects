{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Azat Vakhitov | 12148222 | 25.05.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 5 – Language Modeling with LSTM (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 16-05-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.10 (✓)\n",
      "Installed numpy version: 1.23.5 (✓)\n",
      "Installed pandas version: 1.5.3 (✓)\n",
      "Installed PyTorch version: 1.12.1 (✓)\n"
     ]
    }
   ],
   "source": [
    "import u5_utils as u5\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u5.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u5.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Model Training and Evaluation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Setup the data set using the same parameter settings as in the main exercise notebook but with the changes mentioned below.</li>\n",
    "            <li>Change the batch size in the initial parameters to $64$ and observe its effect on the created batches. Explain how the corpora are transformed into batches.</li>\n",
    "            <li>Use a seed of $23$.</li>\n",
    "            <li>For a specific sequence in <code>val_data_splits</code> (e.g., index $15$), print the corresponding words of its first 25 wordIDs.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = \"model.pt\" # path to save the final model\n",
    "\n",
    "# Training & evaluation parameters\n",
    "train_batch_size = 64 # batch size for training\n",
    "eval_batch_size = 64 # batch size for validation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "# Random seed to facilitate reproducibility\n",
    "torch.manual_seed(23)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_corpus = u5.Corpus(os.path.join(data_path, \"train.txt\"))\n",
    "valid_corpus = u5.Corpus(os.path.join(data_path, \"valid.txt\"))\n",
    "test_corpus = u5.Corpus(os.path.join(data_path, \"test.txt\"))\n",
    "\n",
    "dictionary = u5.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print(f\"Number of tokens in dictionary {ntokens}\")\n",
    "\n",
    "# Some samples in the dictionary ...\n",
    "print(f\"wordID of a word in the dictionary: {dictionary.word2idx['book']}\")\n",
    "print(f\"A word in the dictionary based on its wordID: '{dictionary.idx2word[854]}'\")\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print(f\"Train data: number of tokens {len(train_data)}\")\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print(f\"Validation data: number of tokens {len(valid_data)}\")\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print(f\"Test data: number of tokens {len(test_data)}\")\n",
    "\n",
    "print()\n",
    "train_data_splits = u5.batchify(train_data, train_batch_size, device)\n",
    "print(f\"Train data split shape: {train_data_splits.shape}\")\n",
    "\n",
    "val_data_splits = u5.batchify(valid_data, eval_batch_size, device)\n",
    "print(f\"Validation data split shape: {val_data_splits.shape}\")\n",
    "\n",
    "test_data_splits = u5.batchify(test_data, eval_batch_size, device)\n",
    "print(f\"Test data batchified shape: {test_data_splits.shape}\")\n",
    "\n",
    "word_ids = val_data_splits[15, :25]\n",
    "words = list()\n",
    "for i_d in word_ids:\n",
    "    words.append(dictionary.idx2word[i_d])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data split [29049, 32] \n",
    "\n",
    "Validation data split [2305, 32] \n",
    "\n",
    "Test data batchified [2575, 32] \n",
    "\n",
    "train, validation, and test data shapes have changed, the first and second dimentions doubled \n",
    "\n",
    "tensor([  27, 2458,   43,  109,   49,   35, 1961, 1416, 5711,   79, 5501,  153,\n",
    "          25,  132,   99,   43,   41,   33, 5827,  502,   27,   36,   35,   25,\n",
    "          25], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2. [20 Points]</b>\n",
    "        <ul>\n",
    "            <li>Copy the implementation of <code>LM_LSTMModel</code> from the main exercise notebook but make the following changes:</li>\n",
    "            <ul>\n",
    "                <li>Add an integer parameter to <code>LM_LSTMModel</code>'s initialization, called <code>num_layers</code> which indicates the number of (vertically) stacked LSTM blocks. Hint: PyTorch's LSTM implementation directly supports this, so you simply have to set it when creating the LSTM instance (see parameter <code>num_layers</code> in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">documentation</a>).</li>\n",
    "                <li>Add a new bool parameter to <code>LM_LSTMModel</code>'s initialization, called <code>tie_weights</code>. Extend the implementation of <code>LM_LSTMModel</code> such that if <code>tie_weights</code> is set to <code>True</code>, the model ties/shares the parameters of <code>encoder</code> with the ones of <code>decoder</code>. Consider that <code>encoder</code> and <code>decoder</code> still remain separate components but their parameters are now the same (shared). This process is called <i>weight tying</i>. Feel free to search the internet for relevant resources and implementation hints.</li>\n",
    "            </ul>\n",
    "            <li>Create four models:</li>\n",
    "            <ul>\n",
    "                <li>1 layer and without weight tying</li>\n",
    "                <li>1 layer and with weight tying</li>\n",
    "                <li>2 layers and without weight tying</li>\n",
    "                <li>2 layers and with weight tying</li>\n",
    "            </ul>\n",
    "            <li>Compare the number of parameters of the models and report your observations.</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhid, num_layers, tie_weights: bool):\n",
    "        super().__init__()\n",
    "        self.tie_weights = tie_weights\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp) # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid, num_layers = num_layers)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken) # matrix U in the figure\n",
    "        if tie_weights: self.decoder.weight = self.encoder.weight\n",
    "    \n",
    "    def forward(self, input, hidden=None, return_logs=True):\n",
    "        #ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "        \n",
    "        return y_hat, last_hidden\n",
    "    \n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "#     1 layer and without weight tying\n",
    "#     1 layer and with weight tying\n",
    "#     2 layers and without weight tying\n",
    "#     2 layers and with weight tying\n",
    "\n",
    "model1 = LM_LSTMModel(ntokens, emsize, nhid, 1, False)\n",
    "model1.to(device)\n",
    "\n",
    "print(f\"Model-1: {model1}\")\n",
    "print(f\"Model-1 total trainable parameters: {sum(p.numel() for p in model1.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model2 = LM_LSTMModel(ntokens, emsize, nhid, 1, True)\n",
    "model2.to(device)\n",
    "\n",
    "print(f\"Model-2: {model2}\")\n",
    "print(f\"Model-2 total trainable parameters: {sum(p.numel() for p in model2.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model3 = LM_LSTMModel(ntokens, emsize, nhid, 2, False)\n",
    "model3.to(device)\n",
    "\n",
    "print(f\"Model-3: {model3}\")\n",
    "print(f\"Model-3 total trainable parameters: {sum(p.numel() for p in model3.parameters() if p.requires_grad)}\")\n",
    "\n",
    "model4 = LM_LSTMModel(ntokens, emsize, nhid, 2, True)\n",
    "model4.to(device)\n",
    "\n",
    "print(f\"Model-4: {model4}\")\n",
    "print(f\"Model-4 total trainable parameters: {sum(p.numel() for p in model4.parameters() if p.requires_grad)}\")\n",
    "\n",
    "models = [model1, model2, model3, model4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the number of trainable parameters reduces due to tying weights which decreses processing time and tying weights can help with overfitting as well as help increase accuracy due to simmilarity of the weights and words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3. [30 Points]</b>\n",
    "    <ul>\n",
    "        <li>Using the same setup as in the main lecture/exercise notebook, train all four models for $5$ epochs.</li>\n",
    "        <li>Using <code>ipdb</code>, look inside the <code>forward</code> function of <code>LM_LSTMModel</code> during training. Check the forward process from input to output particularly by looking at the shapes of tensors. Report the shape of all tensors used in <code>forward</code>. Try to translate the numbers into batches $B$ and sequence length $L$. For instance, if we know that the batch size is $B=32$, a tensor of shape $(32, 128, 3)$ can be interpreted as a batch of $32$ sequences with $3$ channels of size $L=128$. Thus, this tensor can be translated into $(32, 128, 3) \\rightarrow (B, L, 3)$. Look at the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">official documentation</a> to understand the order of the dimensions.</li>\n",
    "        <li>Evaluate the models. Compare the performances of all four models on the train, validation and test set (for the test set, use the best model according to the respective validation set performance), and report your observations. To do so, create a plot showing the following curves:</li>\n",
    "        <ul>\n",
    "            <li>Loss on each current training batch before every model update step as function of epochs</li>\n",
    "            <li>Loss on the validation set at every epoch</li>\n",
    "        </ul>\n",
    "        <li>Comment on the results!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_AFTER_BATCHES = 200  # JUST FOR DEBUGGING: cut the loop after these number of batches. Set to -1 to ignore\n",
    "\n",
    "def plot_losses(model_losses, model_names):\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    for model_loss, model_name in zip(model_losses, model_names):\n",
    "        plt.plot(model_loss[0], label=f'{model_name} Training Loss')\n",
    "        plt.plot(model_loss[1], label=f'{model_name} Validation Loss')\n",
    "\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u5.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
    "          clipping: float, learning_rate: float, print_interval: int, epoch: int,\n",
    "          criterion: torch.nn.Module = torch.nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = None\n",
    "    n_batches = (train_data_splits.size(0) - 1) // max_seq_len\n",
    "    \n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = u5.get_batch(train_data_splits, i, max_seq_len)\n",
    "        #ipdb.set_trace()\n",
    "        \n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        if start_hidden is not None:\n",
    "            start_hidden = u5.repackage_hidden(start_hidden)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "        \n",
    "        # Loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = criterion(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "        \n",
    "        # Clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        \n",
    "        # Updating parameters using SGD\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            throughput = elapsed * 1000 / print_interval\n",
    "            print(f\"| epoch {epoch:3d} | {batch_i:5d}/{n_batches:5d} batches | lr {learning_rate:02.2f} | ms/batch {throughput:5.2f} \"\n",
    "                  f\"| loss {cur_loss:5.2f} | perplexity {math.exp(cur_loss):8.2f}\")\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "        # Cuts the loop (only for debugging)\n",
    "        if (CUT_AFTER_BATCHES != -1) and (batch_i >= CUT_AFTER_BATCHES):\n",
    "            print(f\"WARNING: Training is interrupted after {batch_i} batches\")\n",
    "            break\n",
    "model_losses = []\n",
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\"]            \n",
    "for model in models:\n",
    "    epochs = 5  # total number of training epochs\n",
    "    print_interval = 25  # print report statistics every x batches\n",
    "    lr = 20  # initial learning rate\n",
    "    clipping = 0.25  # gradient clipping\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = None\n",
    "\n",
    "    # Loop over epochs.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "        val_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s\"\n",
    "              f\"| valid loss {val_loss:5.2f} | valid perplexity {math.exp(val_loss):8.2f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "            for g in optimizer.param_groups:\n",
    "                g[\"lr\"] = lr\n",
    "    model_losses.append((train_losses, val_losses))\n",
    "plot_losses(model_losses, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Language Generation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4. [30 Points]</b>\n",
    "    <p>\n",
    "    Copy the language generation code from the main exercise notebook and perform the following tasks:\n",
    "    </p>\n",
    "        <ul>\n",
    "            <li>Compare all four previous models by generating $12$ words that append the starting word <tt>\"despite\"</tt>.</li>\n",
    "            <li>For each model, retrieve the top $10$ wordIDs with the highest probabilities from the generated probability distribution (<code>prob_dist</code>) following the starting word <tt>\"despite\"</tt>. Fetch the corresponding words of these wordIDs. Do you observe any specific linguistic characteristic common between these words?</li>\n",
    "            <li>The implementation in the main exercise notebook is based on sampling. Implement a second deterministic variant based on the <i>top-1</i> approach. In this particular variant, the generated word is the word with the highest probability in the predicted probability distribution. Repeat the same procedure as before (i.e., generate $12$ words that append the starting word <tt>\"despite\"</tt>).</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_LENGTH = 12\n",
    "START_WORD = \"despite\"\n",
    "\n",
    "for model_idx, model in enumerate(models):\n",
    "    print(f\"Model {model_idx + 1}\")\n",
    "    \n",
    "    start_hidden = None\n",
    "    wordid_input = dictionary.word2idx[START_WORD.lower()]\n",
    "    generated_text = START_WORD\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(GENERATION_LENGTH):\n",
    "            data = torch.tensor([wordid_input]).unsqueeze(1).to(device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden)\n",
    "            prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "            wordid_input = prob_dist.sample().item()\n",
    "            generated_text += \" \" + dictionary.idx2word[wordid_input]\n",
    "            start_hidden = last_hidden\n",
    "\n",
    "    print(f\"Generated text (probabilistic): {generated_text}\")\n",
    "\n",
    "    top10_wordids = y_hat_probs.squeeze().topk(10).indices.tolist()\n",
    "    top10_words = [dictionary.idx2word[wordid] for wordid in top10_wordids]\n",
    "    print(f\"top 10 wordIDs with the highest probabilities: {top10_words}\")\n",
    "    \n",
    "    start_hidden = None\n",
    "    wordid_input = dictionary.word2idx[START_WORD.lower()]\n",
    "    generated_text = START_WORD\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(GENERATION_LENGTH):\n",
    "            data = torch.tensor([wordid_input]).unsqueeze(1).to(device)\n",
    "            \n",
    "            y_hat_probs, last_hidden = model(data, start_hidden)\n",
    "            wordid_input = y_hat_probs.argmax().item()\n",
    "            generated_text += \" \" + dictionary.idx2word[wordid_input]\n",
    "            start_hidden = last_hidden\n",
    "\n",
    "    print(f\"Generated text (deterministic): {generated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
