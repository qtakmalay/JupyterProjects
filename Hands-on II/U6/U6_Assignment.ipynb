{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bfffa4-a151-4f0d-9269-3897da73a4b1",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Azat Vakhitov | 12148222 | 22.06.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a9280",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 6 &ndash; Introduction to Reinforcement Learning (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170adbf1",
   "metadata": {},
   "source": [
    "<b>Authors:</b> B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, M. Abbass, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 13-06-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e3bb8",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which require your contribution (in form of code, plain text, ...). Most/All of the supplied functions are imported from the file <code>u6_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u6_utils.py</code> need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f21ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.10 (✓)\n",
      "Installed numpy version: 1.23.5 (✓)\n",
      "Installed pandas version: 1.5.3 (✓)\n",
      "Installed PyTorch version: 2.0.1+cpu (✓)\n",
      "Installed matplotlib version: 3.7.0 (✓)\n",
      "Installed seaborn version: 0.12.2 (✓)\n",
      "Installed gym version: 0.28.1 (✓)\n"
     ]
    }
   ],
   "source": [
    "# Import pre-defined utilities specific to this notebook.\n",
    "import u6_utils as u6\n",
    "\n",
    "# Import additional utilities needed in this notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython import display\n",
    "from typing import Tuple\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u6.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u6.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0136de7",
   "metadata": {},
   "source": [
    "<h2>Dissection of an Environment</h2>\n",
    "<p>All exercises in this assignment are referring to the <i>FrozenLake</i> environment as described <a href=\"https://gymnasium.farama.org/environments/toy_text/frozen_lake/\">here</a>.\n",
    "<center>\n",
    "    <cite>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly <span style=\"color:rgb(0,255,0)\">frozen</span>, but there are a few <span style=\"color:rgb(255,0,0)\">holes</span> where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the <span style=\"color:rgb(255,0,255)\">disc</span>. However, the ice is slippery, so you won't always move in the direction you intend.</cite>\n",
    "    </center></p>\n",
    "\n",
    "\n",
    "<p>There are <i>four</i> types of surfaces described in this environment:\n",
    "<ul>\n",
    "    <li><code>S</code> $\\rightarrow$ starting point (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>F</code> $\\rightarrow$ frozen surface (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>H</code> $\\rightarrow$ hole (<span style=\"color:rgb(255,0,0)\"><i>fall to your doom</i></span>)</li>\n",
    "    <li><code>G</code> $\\rightarrow$ goal (<span style=\"color:rgb(255,0,255)\"><i>frisbee location</i></span>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2cb2b2d-7fef-4fd0-8ea1-fa822404e49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility function for creating a default FrozenLake environment.\n",
    "def FrozenLakeEnv(slippery: bool = False, max_episode_steps: int = 100_000) -> gym.Env:\n",
    "    return gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=slippery, max_episode_steps=max_episode_steps, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8694c6f-7cac-411d-b00f-90642761af92",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (3037371846.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Given the function \\( f(x_1, x_2) = \\sin(x_1) \\cdot \\cos(x_2) \\), the Hessian matrix \\( H_f \\) is:\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "Given the function \\( f(x_1, x_2) = \\sin(x_1) \\cdot \\cos(x_2) \\), the Hessian matrix \\( H_f \\) is:\n",
    "\\[\n",
    "H_f = \\begin{bmatrix}\n",
    "0 & \\cos(x_1) \\cdot \\sin(x_2) \\\\\n",
    "\\cos(x_1) \\cdot \\sin(x_2) & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Evaluating the Hessian matrix at the stationary point \\( (0, 0) \\):\n",
    "\\[\n",
    "H_f(0, 0) = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "Next, evaluating the Hessian matrix at the stationary points \\( (\\sqrt{2}, 0) \\) and \\( (-\\sqrt{2}, 0) \\):\n",
    "\\[\n",
    "H_f(\\sqrt{2}, 0) = H_f(-\\sqrt{2}, 0) = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\textbf{Analysis of Stationary Points:}\n",
    "\n",
    "1. For \\( (0, 0) \\):\n",
    "\\[\n",
    "H_f(0, 0) = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "This matrix is indefinite. Using the theorem, \\( (0, 0) \\) is not an extreme point of \\( f \\).\n",
    "\n",
    "2. For \\( (\\sqrt{2}, 0) \\) and \\( (-\\sqrt{2}, 0) \\):\n",
    "\\[\n",
    "H_f(\\sqrt{2}, 0) = H_f(-\\sqrt{2}, 0) = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "This matrix is also indefinite. Thus, \\( (\\sqrt{2}, 0) \\) and \\( (-\\sqrt{2}, 0) \\) are not extreme points of \\( f \\).\n",
    "\n",
    "\\textbf{Conclusion:}\n",
    "The function \\( f(x_1, x_2) = \\sin(x_1) \\cdot \\cos(x_2) \\) has several stationary points, but none of them are extreme points (neither local maxima nor local minima). They are all saddle points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44614f3b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a <code>FrozenLakeEnv</code> with <code>slippery = False</code> and set the seed to $23$. Use this environment in the subsequent tasks if not specified otherwise.</li>\n",
    "        <li>Gather and print the amount of different <i>actions</i> as well as <i>states</i> of the <code>FrozenLakeEnv</code> instance. Discuss the results.</li>\n",
    "        <li>Display the <i>reward table entry</i> for the current state. Discuss the different elements of the resulting dictionary.</li>\n",
    "        <li>Perform $25$ different random actions using <code>env_lake.action_space.sample()</code>. Print the action number, state (observation), reward, whether the episode is done (either by being terminated or being truncted), and render the corresponding environment. Hint: If the episode is done, reset the environment or you are stuck.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ece15e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code/answer goes here\n",
    "\n",
    "frozenlake = FrozenLakeEnv(slippery = False)\n",
    "u6.set_environment_seed(environment=frozenlake, seed=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26aa3cc-9c5b-43a9-8f0e-2d3b7feca0c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the amount of different actions:  4\n",
      "states of the FrozenLakeEnv instance 64\n",
      "Reward table entry: 0: {0: [(1.0, 0, 0.0, False)], 1: [(1.0, 8, 0.0, False)], 2: [(1.0, 1, 0.0, False)], 3: [(1.0, 0, 0.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "print(\"the amount of different actions: \", frozenlake.action_space.n)\n",
    "print(\"states of the FrozenLakeEnv instance\", frozenlake.observation_space.n)\n",
    "current_state = frozenlake.s\n",
    "reward_table_current_state = frozenlake.P[current_state]\n",
    "print(f\"Reward table entry: {current_state}: {reward_table_current_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a35dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 1, taken: 0, next state: 0, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 2, taken: 2, next state: 1, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 3, taken: 1, next state: 9, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 4, taken: 2, next state: 10, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 5, taken: 1, next state: 18, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 6, taken: 0, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 7, taken: 2, next state: 18, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 8, taken: 0, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 9, taken: 0, next state: 16, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 10, taken: 2, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 11, taken: 2, next state: 18, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 12, taken: 3, next state: 10, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 13, taken: 1, next state: 18, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 14, taken: 0, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 15, taken: 0, next state: 16, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 16, taken: 0, next state: 16, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 17, taken: 0, next state: 16, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 18, taken: 2, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 19, taken: 3, next state: 9, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 20, taken: 1, next state: 17, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 21, taken: 2, next state: 18, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 22, taken: 1, next state: 26, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 23, taken: 2, next state: 27, reward: 0.0, terminated: False, turncated: False\n",
      "Number: 24, taken: 1, next state: 35, reward: 0.0, terminated: True, turncated: False\n",
      "Number: 25, taken: 1, next state: 8, reward: 0.0, terminated: False, turncated: False\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    # Take a random action\n",
    "    action = frozenlake.action_space.sample()\n",
    "    results = frozenlake.step(action)\n",
    "\n",
    "    next_state, reward, terminated, truncated, info = results\n",
    "    frozenlake.render()\n",
    "    print(f\"Number: {i + 1}, taken: {action}, next state: {next_state}, reward: {reward}, terminated: {terminated}, turncated: {truncated}\")\n",
    "\n",
    "\n",
    "    if terminated or truncated:\n",
    "        frozenlake.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f4d0b",
   "metadata": {},
   "source": [
    "<h2>Tackling the Environment with Random Exploration</h2>\n",
    "<p>In the exercise, we talked about solving this kind of tasks in a na&#xEF;ve way by simply applying <i>brute force</i>: using <i>random search</i>. So far, we analyzed the <i>action</i> as well as the <i>state space</i> and came to the conclusion that such an approach is feasible. Here is the outline of such an approach:\n",
    "<ul>\n",
    "    <li><code>I</code> $\\rightarrow$ choose a random <i>action</i> with respect to the <i>current</i> state.</li>\n",
    "    <li><code>II</code> $\\rightarrow$ execute previously chosen <i>action</i> and transition into a <i>new</i> state.</li>\n",
    "    <li><code>III</code> $\\rightarrow$ if the episode is finished but the goal is not reached, <i>reset</i> the <i>environment</i>.</li>\n",
    "</ul>\n",
    "\n",
    "This procedure is repeated as long as the task is not solved or a defined <i>maximum number of steps</i> is reached, whatever triggers first (<code>IV</code>). Adapt the function <code>apply_random_search</code> as discussed during the exercise. Mark the corresponding sections of the code using <code>I</code>, <code>II</code>, <code>III</code> and <code>IV</code>. Note that our <i>random search</i> is <i>not</i> guaranteed to find  the solution of a task in <i>finite time</i>, hence an upper bound on the <i>runtime</i> is often applied as a safety net (in our case the <i>maximum number of steps</i>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cae41d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.1 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement the <i>random search</i> algorithm as outlined above (equivalently to the one discussed during the exercise).</li>\n",
    "        <li>Apply your random search implementation on a freshly $23$-seeded <code>FrozenLakeEnv</code> instance, with an animation delay of $0.01$ and a maximum number of steps of $750$.</li>\n",
    "        <li>Was the goal reached, how many steps were taken and how often did an involuntary dive happen?</li>\n",
    "        <li>Repeat the experiment and find parameters (random seed, maximum number of steps) that lead to a successful run (you do not need to animate this second experiment).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4259e297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_random_search(environment: gym.Env, animate: bool = False,\n",
    "                        delay: float = 0.01) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying random search.\n",
    "    \n",
    "    :param environment: the environment on which to apply random search\n",
    "    :param animate: animate the random search process\n",
    "    :param delay: the minimum delay in milliseconds between each rendered frame (ignored if not animated)\n",
    "    :return: amount of steps performed, penalties inflicted and final reward\n",
    "    \"\"\"\n",
    "    num_steps, num_penalties, final_reward = 0, 0, 0\n",
    "    \n",
    "    # <III>: repeat random search procedure as long as the episode is still ongoing.\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # <I>: choose a random action with respect to the current state.\n",
    "        current_action = environment.action_space.sample()\n",
    "        \n",
    "        # <II>: execute previously chosen action and transition into a new state.\n",
    "        current_state, current_reward, terminated, truncated, _ = environment.step(current_action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Update counter for inflicted penalties.\n",
    "        final_reward += current_reward\n",
    "        if current_reward == -10:\n",
    "            num_penalties += 1\n",
    "        num_steps += 1\n",
    "        \n",
    "        # Optionally display current state.\n",
    "        if animate:\n",
    "            display.clear_output(wait=True)\n",
    "            print(environment.render())\n",
    "            print(f'Step No.: {num_steps}'\n",
    "                  f'\\nState ID: {current_state}'\n",
    "                  f'\\nAction ID: {current_action}'\n",
    "                  f'\\nReward: {current_reward}')\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return num_steps, num_penalties, final_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcbc6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Step No.: 24\n",
      "State ID: 35\n",
      "Action ID: 1\n",
      "Reward: 0.0\n",
      "24 steps taken and 0 penalties inflicted during randomly searching the goal, with a final reward of 0.0.\n"
     ]
    }
   ],
   "source": [
    "u6.set_environment_seed(environment=frozenlake, seed=23)\n",
    "num_steps, num_penalties, final_reward = apply_random_search(frozenlake, animate=True, delay=0.01)\n",
    "\n",
    "print(f'{num_steps} steps taken and {num_penalties} penalties inflicted'\n",
    "      f' during randomly searching the goal, with a final reward of {final_reward}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916ff17",
   "metadata": {},
   "source": [
    "<p>To drill down on the drawbacks of plain <i>random search</i>, we are designing the following experimental setup (<i>Hint</i>: it is actually the same experimental setup as already discussed during the exercise, so you might orient yourself on the implementation presented during class):\n",
    "<ul>\n",
    "    <li>Repeat the previous <i>random search</i> procedure a specified amount of times.</li>\n",
    "    <li>Aggregate the results of each run for later analysis.</li>\n",
    "    <li>Visualize the aggregated results using <i>box-</i> and <i>strip-plots</i> (or <i>swarm-plots</i>).</li>\n",
    "</ul>\n",
    "Once again, we are setting the <i>random seed</i>, but take care of setting it <i>outside</i> the loop, otherwise the same result is reported with each iteration (and an aggregation of the results would not give us any more insights).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abf5d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.2 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Conduct a <i>random search experiment</i> as outlined above, using $100$ repetitions and the random seed set to $23$. Set the maximum number of steps to $10,000$.</li>\n",
    "        <li>Create a plot showing the results and interpret the visualization.</li>\n",
    "        <li>In comparison with the <code>Taxi</code> environment, what might be the problem with <code>FrozenLakeEnv</code> w.r.t. random exploration?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbe8f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u6.set_environment_seed(environment=frozenlake, seed=23)\n",
    "num_steps_total, num_penalties_total, final_reward_total = [], [], []\n",
    "num_repetitions = 100\n",
    "\n",
    "# Collect information over multiple repetitions.\n",
    "for repetition in range(num_repetitions):\n",
    "    frozenlake.reset()\n",
    "    num_steps, num_penalties, final_reward = apply_random_search(environment=frozenlake)\n",
    "    num_steps_total.append(num_steps)\n",
    "    num_penalties_total.append(num_penalties)\n",
    "    final_reward_total.append(final_reward)\n",
    "\n",
    "# Combine collected information to a data frame for further downstream analysis.\n",
    "collected_experiment_info = pd.DataFrame({\n",
    "    'Steps performed': num_steps_total,\n",
    "    'Penalties inflicted': num_penalties_total,\n",
    "    'Final reward': final_reward_total\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de916b2a-89b0-4a29-bb4f-1b6665057f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Visualize aggregated results of the random search procedure.\n",
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "ax.set_xscale('symlog')\n",
    "sns.boxplot(data=collected_experiment_info, ax=ax, orient='h')\n",
    "sns.stripplot(data=collected_experiment_info, ax=ax, orient='h', linewidth=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d2c5d",
   "metadata": {},
   "source": [
    "<h2>Tackling the Environment with $Q$-Learning</h2>\n",
    "<p>In a simplified version of $Q$-learning, the <b>$\\boldsymbol{Q}$-value</b>\n",
    "\\begin{equation}\n",
    "    Q(s,a)\n",
    "\\end{equation}</p>\n",
    "\n",
    "<p>is the expected future reward of being in state $s$ and taking action $a$. Intuitively, if the $Q$-values are learned correctly, a good policy would be to take the action which maximizes the expected future reward. This is what $Q$-learning is doing. $Q$-learning lets the agent <b>use the environment's rewards to learn</b>, over time, the best action to take in a given state. $Q$-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the $Q$-values are updated using the equation:\n",
    "\\begin{equation}\n",
    "    Q(s_t,a_t) \\leftarrow (1 - \\alpha) \\cdot Q(s_t,a_t) + \\alpha \\cdot \\left( r_t + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\\right)\n",
    "\\end{equation}</p>\n",
    "\n",
    "<p>We are assigning $\\leftarrow$, or updating, the $Q$-value of the agent's current state and action, denoted as $Q(s_t,a_t)$ with $\\alpha$ as the learning rate, i.e the extent to which our $Q$-values are being updated in every iteration.</p>\n",
    "\n",
    "<p>The <b>$\\boldsymbol{Q}$-table</b> is a matrix where we have a row for every state and a column for every action: $64$ and $4$, respectively, when referring to the current <i>FrozenLake</i> example. It's first initialized to $0$, and then values are updated during training.</p>\n",
    "\n",
    "<p>Previously, we talked about solving this task in a na&#xEF;ve way by simply applying <i>brute force</i> using <i>random search</i>. This time, we want to apply a more sophisticated algorithm, namely $Q$-learning:\n",
    "<ul>\n",
    "    <li><code>I</code> $\\rightarrow$ Choose action $a_t$.\n",
    "    <li><code>II</code> $\\rightarrow$ Go from state $s_t$ to state $s_{t+1}$ by taking action $a_{t}$.\n",
    "    <li><code>III</code> $\\rightarrow$ For all possible $Q$-values from the state $s_{t+1}$, select the highest.\n",
    "    <li><code>IV</code> $\\rightarrow$ Update $Q$-table values using the equation from above.\n",
    "    <li><code>V</code> $\\rightarrow$ Set the next state as the current state and go back to <code>I</code> until a final state is reached (end of episode).\n",
    "</ul>\n",
    "\n",
    "This procedure is repeated for as many episodes as specified (<code>VI</code>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0e987",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    The following code snippet is taken from the accompanying exercise notebook. You do not need to modify it for this assignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ad6e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_q_table(q_table: np.ndarray, title: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Visualize Q-table using a heatmap plot.\n",
    "    \n",
    "    :param q_table: Q-table to visualize\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(20, 7))\n",
    "    sns.heatmap(data=q_table, ax=ax)\n",
    "    ax.set(xlabel='Action', ylabel='State', title=title)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def apply_q_learning(environment: gym.Env, num_episodes: int = 1000, alpha: float = 0.1, gamma: float = 1.0,\n",
    "                     animate: bool = False, delay_steps: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying Q-learning.\n",
    "    \n",
    "    :param environment: the environment on which to apply Q-learning\n",
    "    :param num_episodes: the total amount of episodes used to adapt the Q-table\n",
    "    :param alpha: the learning rate to be applied by Q-learning\n",
    "    :param gamma: the discount factor of future rewards\n",
    "    :param animate: animate the Q-learning process\n",
    "    :param delay_steps: the steps between each Q-table visualization (ignored if not animated)\n",
    "    :return: adapted Q-table\n",
    "    \"\"\"\n",
    "    q_table = np.zeros(shape=(environment.observation_space.n, environment.action_space.n))\n",
    "    \n",
    "    # <VI>: repeat Q-learning as long as the total amount of episodes is not yet reached.\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = environment.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # <I>: choose next action according to current Q-table.\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "            # <II>: go from the current state to the next by applying chosen action.\n",
    "            next_state, reward, terminated, truncated, _ = environment.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # <III>: from all possible Q-values w.r.t. the new state, select the highest.\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            # <IV>: update the Q-table accordingly.\n",
    "            old_value = q_table[state, action]\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            \n",
    "            # <V>: update the next step with the current one.\n",
    "            state = next_state\n",
    "        \n",
    "        # Optionally visualize the current Q-table.\n",
    "        if animate and any(((episode + 1) % delay_steps == 0, (episode + 1) == num_episodes)):\n",
    "            visualize_q_table(q_table=q_table, title=f'Episode {episode + 1}')\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f8850-194e-4687-8d76-3f1250048c80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.1 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement <i>$Q$-learning</i> as outlined above (equivalently to the one discussed during the exercise).</li>\n",
    "        <li>Apply $Q$-learning on a freshly $23$-seeded <code>FrozenLakeEnv</code> instance for $500$ episodes, with $10$ delay steps, a discount factor $\\gamma=0.99$ and $\\alpha=0.1$. Important: Make sure that you create a new <code>FrozenLakeEnv</code> instance where <code>max_episode_steps</code> is set to $100$ (only in this exercise), since otherwise, the code execution might take very long.</li>\n",
    "        <li>Interpret the visualization of the resulting $Q$-table. What do you observe? Why do you observe this effect?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9774f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FrozenLakeEnv(slippery: bool = False, max_episode_steps: int = 500) -> gym.Env:\n",
    "    return gym.make('FrozenLake-v1', desc=None, map_name='8x8', is_slippery=slippery, max_episode_steps=max_episode_steps, render_mode='ansi')\n",
    "\n",
    "def apply_q_learning(environment: gym.Env, num_episodes: int = 500, alpha: float = 0.1, gamma: float = 0.99, animate: bool = False, delay_steps: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying Q-learning with exploration.\n",
    "    \n",
    "    :param environment: the environment on which to apply Q-learning\n",
    "    :param num_episodes: the total amount of episodes used to adapt the Q-table\n",
    "    :param alpha: the learning rate to be applied by Q-learning\n",
    "    :param gamma: the discount factor of future rewards\n",
    "    :param epsilon: the exploration rate (probability of choosing a random action)\n",
    "    :param animate: animate the Q-learning process\n",
    "    :param delay_steps: the steps between each Q-table visualization (ignored if not animated)\n",
    "    :return: adapted Q-table\n",
    "    \"\"\"\n",
    "    q_table = np.zeros(shape=(environment.observation_space.n, environment.action_space.n))\n",
    "    \n",
    "    # <VI>: repeat Q-learning as long as the total amount of episodes is not yet reached.\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = environment.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # <I>: choose next action according to current Q-table.\n",
    "            #================================\n",
    "            if np.random.uniform(0, 1) < 0.1:\n",
    "                action = np.argmax(q_table[state])\n",
    "            else:\n",
    "                action = environment.action_space.sample()  # Random action\n",
    "            #================================\n",
    "            # <II>: go from the current state to the next by applying chosen action.\n",
    "            next_state, reward, terminated, truncated, _ = environment.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # <III>: from all possible Q-values w.r.t. the new state, select the highest.\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            # <IV>: update the Q-table accordingly.\n",
    "            old_value = q_table[state, action]\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            \n",
    "            # <V>: update the next step with the current one.\n",
    "            state = next_state\n",
    "        \n",
    "        # Optionally visualize the current Q-table.\n",
    "        if animate and any(((episode + 1) % delay_steps == 0, (episode + 1) == num_episodes)):\n",
    "            visualize_q_table(q_table=q_table, title=f'Episode {episode + 1}')\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "environment_frozen_lake = FrozenLakeEnv(max_episode_steps=100)\n",
    "u6.set_environment_seed(environment=environment_frozen_lake, seed=23)\n",
    "\n",
    "q_table = apply_q_learning(\n",
    "    environment=environment_frozen_lake,\n",
    "    num_episodes=500,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    animate=True,\n",
    "    delay_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85c9a4",
   "metadata": {},
   "source": [
    "<p>Very likely, the $Q$-table of the previous experiment looked a little bit odd. Try to add exploration to your algorithm by adapting your $Q$-learning implementation:\n",
    "    <ul>\n",
    "        <li>Get a random uniform number between $0$ and $1$.</li>\n",
    "        <li>If the number is $< 0.1$, choose your action as usual (according to current Q-table).</li>\n",
    "        <li>Otherwise, sample a random action.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e4097",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.2 [20 Points]</b>\n",
    "    <ul>\n",
    "        <li>Modify the <i>$Q$-learning</i> implementation from the previous tasks as outlined above (mark the corresponding code sections).</li>\n",
    "        <li>Apply $Q$-learning on a freshly $23$-seeded <code>FrozenLakeEnv</code> instance for $40,000$ episodes, with $1,000$ delay steps, discount factor $\\gamma=0.99$ and $\\alpha=0.1$.</li>\n",
    "        <li>Interpret the visualization of the resulting $Q$-table. What do you observe (compare with the previous visualization)?</li>\n",
    "        <li>Use this $Q$-learning strategy to also tackle the <code>FrozenLakeEnv</code> with <code>slippery = True</code>. Compare both $Q$-tables and discuss the differences.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a44c763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_q_learning\u001b[39m(environment: \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mEnv, num_episodes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, alpha: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, gamma: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m      2\u001b[0m                      animate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, delay_steps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Solve specified environment by applying Q-learning.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    :return: adapted Q-table\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_q_learning(environment: gym.Env, num_episodes: int = 1000, alpha: float = 0.1, gamma: float = 1.0,\n",
    "                     animate: bool = False, delay_steps: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying Q-learning.\n",
    "    \n",
    "    :param environment: the environment on which to apply Q-learning\n",
    "    :param num_episodes: the total amount of episodes used to adapt the Q-table\n",
    "    :param alpha: the learning rate to be applied by Q-learning\n",
    "    :param gamma: the discount factor of future rewards\n",
    "    :param animate: animate the Q-learning process\n",
    "    :param delay_steps: the steps between each Q-table visualization (ignored if not animated)\n",
    "    :return: adapted Q-table\n",
    "    \"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = environment.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # <I>: choose next action according to current Q-table.\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "            # <II>: go from the current state to the next by applying chosen action.\n",
    "            next_state, reward, terminated, truncated, _ = environment.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # <III>: from all possible Q-values w.r.t. the new state, select the highest.\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            # <IV>: update the Q-table accordingly.\n",
    "            old_value = q_table[state, action]\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            \n",
    "            # <V>: update the next step with the current one.\n",
    "            state = next_state\n",
    "        \n",
    "        # Optionally visualize the current Q-table.\n",
    "        if animate and any(((episode + 1) % delay_steps == 0, (episode + 1) == num_episodes)):\n",
    "            visualize_q_table(q_table=q_table, title=f'Episode {episode + 1}')\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48a37b-12a6-446e-af47-1571fe462f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code/answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a4788",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.3 [15 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a function for applying a pre-trained $Q$-table on a <code>FrozenLakeEnv</code> instance (like discussed during class).</li>\n",
    "        <li>For both <code>FrozenLakeEnv</code> instances (with <code>slippery = False</code> and <code>slippery = True</code>), conduct a guided search using the corresponding $Q$-table on a freshly $23$-seeded instance, with an animation delay of $0.1$.</li>\n",
    "        <li>Answer the following question for both settings: How many steps are necessary to reach the goal at least once and how often did an involuntary dive happen?</li>\n",
    "        <li>Compare the corresponding policies of each $Q$-table. Which property of the environment is exploited in the slippery policy to avoid involuntary dives?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d1020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code/answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460686dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.4 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Conduct a <i>$Q$-table guided search experiment</i> on the non-slippery environment, as outlined previously, using $100$ repetitions and the random seed set to $23$.</li>\n",
    "        <li>Create a plot showing the results and interpret the visualization.</li>\n",
    "        <li>In comparison with the <i>random search</i> experiment, how does the $Q$-table guided search perform? Discuss the results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc93d00-55a4-4935-a8b9-ce1fa3bdfb27",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.5 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Repeat the <i>$Q$-table guided search experiment</i> using $100$ repetitions and the random seed set to $23$. This time we would like to compare both policies (learned for the slippery and non-slippery environment) on the slippery environment. Hence, you have to apply the non-slippery $Q$-table to the slippery environment and also the slippery $Q$-table to the same environment. Remember to reset the seed.</li>\n",
    "        <li>Compare the number of performed steps as well as the performed dives for both policy results using appropirate visualizations (e.g., box and strip plots). Discuss your observations.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc832e4-38bb-42b6-9510-20f43999a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code/answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4234b5-cfa6-4279-aff9-b6b135188248",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.6 [5 Points]</b>\n",
    "    <ul>\n",
    "        <li>Assume that you are given new non-slippery <i>FrozenLake</i> environments where holes and frozen tiles are completely randomized (the grid size, start and goal position are the same). How would your learned non-slippery policy from above perform?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e22f1-8105-45c8-b109-1c31b72bd0af",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1067ee20f23cf75b48768bdb5f7ec1d4c21e1831c972d070ed1f98bb55bb7e57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
