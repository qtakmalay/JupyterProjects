{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e2c663-475e-4887-b512-4108fad1cdda",
   "metadata": {
    "tags": []
   },
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Firstname Lastname | 01234567 | 11.05.2022, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd530a2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 4 &ndash; Recurrent Neural Networks (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f035a0",
   "metadata": {},
   "source": [
    "<b>Authors:</b> B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler, A. Schörgenhumer<br>\n",
    "<b>Date:</b> 02-05-2023\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9313369",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which require your contribution (in form of code, plain text, ...). Most/All of the supplied functions are imported from the file <code>u4_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u4_utils.py</code> need to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c68373-2a37-45d4-b193-b161db846292",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> Set the random seed with <code>u4.set_seed(23)</code> to enable reproducible results in all tasks that incorporate randomness (e.g., t-SNE, splitting data intro train and test sets, initializing weights of a neural network, running the model optimization with random batches, etc.). You must use <code>23</code> as seed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19be49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pre-defined utilities specific to this notebook.\n",
    "import u4_utils as u4\n",
    "\n",
    "# Import additional utilities needed in this notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u4.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u4.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e58951",
   "metadata": {},
   "source": [
    "<a name=\"exercise-latch-sequence-set\"></a><h2>The Latch Sequence Data Set</h2>\n",
    "<p>In the accompanying excercise class, the <i>latch task</i> was presented. You'll be working with the same data set in this assignment. The original latch task was introduced by Hochreiter and Mozer:\n",
    "<center>\n",
    "    <cite>Sepp Hochreiter, Michael Mozer, 2001. A discrete probabilistic memory model for discovering dependencies in time. Artificial Neural Networks -- ICANN 2001, 13, pp.661-668.</cite>\n",
    "</center></p>\n",
    "\n",
    "<p>The essence of this task is that a sequence of inputs is presented, beginning with one of two symbols, <b>A</b> or <b>B</b>, and after a variable number of time steps, the model has to output a corresponding symbol. Thus, the task requires memorizing the original input over time. It has to be noted that in the <i>original</i> task desription, both class-defining symbols must only appear at the first position of an instance.</p>\n",
    "\n",
    "<p>The modified version of this task used in this assignment is identical to the one discussed during the accompanying exercise, with the difference of a higher amount of possible targets. Defining arguments are:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Argument</th>\n",
    "        <th>Value (used in this assignment)</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_samples</code></th>\n",
    "        <th>5000</th>\n",
    "        <th>Amount of samples of the full dataset.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_instances</code></th>\n",
    "        <th>56</th>\n",
    "        <th>Amount of instances per sample (sample length).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_characters</code></th>\n",
    "        <th>20</th>\n",
    "        <th>Amount of different characters (size of the one-hot encoded vector).</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>num_targets</code></th>\n",
    "        <th>19</th>\n",
    "        <th>Amount of different characters used as possible targets.</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th><code>seed</code></th>\n",
    "        <th>23</th>\n",
    "        <th>Random seed used to generate the samples of the data set.</th>\n",
    "    </tr>\n",
    "</table></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0085e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.1. [8 Points]</b>\n",
    "    <ul>\n",
    "        <li>Generate a <i>latch sequence</i> data set with the properties as described in the table <i>above</i>.</li>\n",
    "        <li>Visualize the last sequence of the data set in <i>tabular</i> form, with all $1$ in <b style=\"color:green\">bold green</b> and all $0$ in <span style=\"font-weight: lighter\">lighter</span> default font color.</li>\n",
    "        <li>Visualize the <i>first</i> $20$ samples in a heatmap, once <i>without</i> and once <i>with</i> a corresponding prefix-mask.</li>\n",
    "        <li>Interpreting the previous visualizations, which <i>character</i> of the chosen alphabet determines the <i>prefix</i>?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29c06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ca060",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625c842",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.2. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Linearly split the data set into a <i>training</i> and a <i>test</i> set in a ratio of $4 : 1$ (use a <code>SubsetRandomSampler</code> and a batch size of $48$).</li>\n",
    "        <li>Compute and print the <i>amount of samples</i> of each of the respective sets and verify the $4 : 1$ split.</li>\n",
    "        <li>Visualize the <i>character counts</i> of the <i>first</i> training minibatch appropriately. What is the count of the <i>prefix</i> character? Provide a formula as a function of the <i>batch_size</i> to compute the count of the <i>prefix</i> character.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafeff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3471aa",
   "metadata": {},
   "source": [
    "<a name=\"exercise-latch-cnn\"></a><h2>Tackling Sequence Data with CNNs</h2>\n",
    "<p>During the accompanying exercise class, a <i>dense feed-forward</i> network was presented as some kind of baseline. Afterwards, recurrent architectures were applied. In this exercise, you'll be tasked with implementing a <i>convolutional</i> architecture for handling sequence data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26283c0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    The following code snippet is taken from the accompanying exercise notebook. You do not need to modify it for this assignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9c822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TheMightyDice(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Dice roll \"network\" tailored to deliver random outcomes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size: int):\n",
    "        super().__init__()\n",
    "        self.__output_size = output_size\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.rand(size=(x.shape[0], self.__output_size), device=x.device)\n",
    "\n",
    "\n",
    "def count_parameters(model: torch.nn.Module, only_trainable: bool = True) -> int:\n",
    "    \"\"\"\n",
    "    Count (trainable) paramaters of specified model.\n",
    "    \n",
    "    :param model: model for which to compute the amount of (trainable) parameters\n",
    "    :param only_trainable: only include trainable parameters in total count\n",
    "    :return: amount of (trainable) parameters of the specified model\n",
    "    \"\"\"\n",
    "    return sum(parameter.numel() for parameter in model.parameters() if any(\n",
    "        (not only_trainable, only_trainable and parameter.requires_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94da7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.1. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Test a <code>TheMightyDice</code> instance on the latch sequence <i>test</i> set. Do you expect this result? Comment on your answer.</li>\n",
    "        <li>Assume <i>uniformly</i> distributed targets. If a model would <i>always</i> predict the same class, what would the accuracy be?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36967d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6efab-e46b-4969-9ffb-b563b8020ce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a004f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.2. [15 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a class <code>CNN</code> with the following architecture:</li>\n",
    "    </ul>\n",
    "    <table style=\"text-align:center;vertical-align:middle\">\n",
    "        <th>Position</th>\n",
    "        <th>Element</th>\n",
    "        <th>Comment</th>\n",
    "        <tr>\n",
    "            <td>0</td>\n",
    "            <td>input</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\">1D convolution</a></td>\n",
    "            <td>$192$ output channels and a configurable kernel size (specified as an argument to <code>__init__</code>)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>ReLU</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\">1D convolution</a></td>\n",
    "            <td>$192$ output channels and the same kernel size as the <i>1D convolution</i> at position $1$</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>4</td>\n",
    "            <td>ReLU</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>5</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">fully connected</a></td>\n",
    "            <td><code>num_targets</code> output features (as specified during the data set creation)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>6</td>\n",
    "            <td>output</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <ul>\n",
    "        <li>Train a <code>CNN</code> network for $12$ epochs (use <code>Adam</code> optimizer with a learning rate of $0.01$), print the training accuracy as well as the loss per epoch and report the final test set loss and accuracy as well as the number of trainable parameters (use <code>count_parameters</code> from above). Use a <i>kernel size</i> of $1$.</li>\n",
    "        <li>Repeat the same procedure with a second <code>CNN</code> but a <i>kernel size</i> of $3$.</li>\n",
    "        <li>Hints:</li>\n",
    "        <ul>\n",
    "            <li>With the parameterized kernel size, you cannot hard-code the flattened number of elements for the linear layer input. Instead, you have to calculate it using the length of the input (<code>data_latch.num_instances</code>), which might be a good idea to pass into <code>__init__</code> as well. Each convolutional layer then reduces this length by <code>kernel_size - 1</code>.</li>\n",
    "            <li><code>Conv1d</code> expects the minibatch input shape <code>(N, C, L)</code>, but the latch data has a minibatch shape of <code>(N, L, C)</code>. Thus, in your <code>forward</code> method, you will first have to transform the input minibtach into the correct shape before you can apply the network architecture. There are several ways to do this, e.g., <a href=\"https://pytorch.org/docs/stable/generated/torch.transpose.html\">transposing</a> or <a href=\"https://pytorch.org/docs/stable/generated/torch.movedim.html\">moving dimensions</a>.</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcebd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167c8d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.3. [12 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a class <code>PoolCNN</code> with the following architecture:</li>\n",
    "    </ul>\n",
    "    <table style=\"text-align:center;vertical-align:middle\">\n",
    "        <th>Position</th>\n",
    "        <th>Element</th>\n",
    "        <th>Comment</th>\n",
    "        <tr>\n",
    "            <td>0</td>\n",
    "            <td>input</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\">1D convolution</a></td>\n",
    "            <td>$16$ output channels and a kernel size of $3$</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>ReLU</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool1d.html\">1D adaptive max pool</a></td>\n",
    "            <td>output size of $1$</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>4</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">fully connected</a></td>\n",
    "            <td><code>num_targets</code> output features (as specified during the data set creation)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>5</td>\n",
    "            <td>output</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <ul>\n",
    "        <li>Train a <code>PoolCNN</code> network for $12$ epochs (use <code>Adam</code> optimizer with a learning rate of $0.01$), print the training accuracy as well as the loss per epoch and report the final test set loss and accuracy as well as the number of trainable parameters.\n",
    "        <li>Compare the results of the <code>PoolCNN</code> network with those of the two <code>CNN</code> networks. What do you observe? Interpret and discuss your results regarding all three networks.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493506b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f54d57",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc18cf5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    The following code snippet is taken from the accompanying exercise notebook. You do not need to modify it for this assignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c77aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_gradients(model: torch.nn.Module, loader: torch.utils.data.DataLoader) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Auxiliary function for collecting gradient magnitudes of a corresponding model w.r.t. the network input.\n",
    "    \n",
    "    :param model: model instance to be used for collecting gradients\n",
    "    :param device: device to use for gradient collection\n",
    "    :param loader: data loader supplying the samples used for collecting gradients\n",
    "    :return: data frame comprising the gradient magnitudes of the loss function w.r.t. each input element\n",
    "    \"\"\"\n",
    "    model_state = model.training\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Set random seed for reproducibility (data iteration).\n",
    "    u4.set_seed(23)\n",
    "    \n",
    "    # Iterating over the data set and computing the corresponding gradients.\n",
    "    device, gradients = next(model.parameters())[0].device, []\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for data, target in loader:\n",
    "        data, target = data.float().to(device), target.long().to(device)\n",
    "        \n",
    "        # Prepare network input for gradient recording.\n",
    "        data.requires_grad_(True)\n",
    "        data.register_hook(lambda x: gradients.append(x.cpu().abs()))\n",
    "\n",
    "        # One forward\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # Reset model to its original state and return averaged collected gradients.\n",
    "    model.train(mode=model_state)\n",
    "    return pd.DataFrame(torch.cat(gradients, dim=0).mean(dim=2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197d6a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.4. [8 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a <i>fresh</i> instance of <code>CNN</code> using a <i>kernel size</i> of $3$ and collect its gradients w.r.t. the network input using the latch sequence training set.</li>\n",
    "        <li>Visualize the collected gradients accordingly. What do you observe? Comment on your results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f626f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b828e41",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d71d487",
   "metadata": {},
   "source": [
    "<a name=\"exercise-latch-lstm\"></a><h2>Tackling Sequence Data with LSTMs</h2>\n",
    "<p>During the accompanying exercise class, the <i>Long Short-Term Memory (LSTM)</i> was presented as a quite prominent and often used architecture in the recurrent case. It was designed and published by <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf\">Hochreiter and Schmidhuber</a>:\n",
    "    <center>\n",
    "        <cite>\n",
    "            Hochreiter, S. and Schmidhuber, J., 1997. Long short-term memory. Neural computation, 9(8), pp.1735-1780.\n",
    "        </cite>\n",
    "    </center></p>\n",
    "\n",
    "<p>It has to be noted, that the most crucial part of the LSTM, the <i>constant error carousel (CEC)</i>, was already discussed during <a href=\"https://people.idsia.ch//~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\">Hochreiter's diploma thesis</a> (in German):\n",
    "    <center>\n",
    "            <cite>\n",
    "                Hochreiter, S., 1991. Untersuchungen zu dynamischen neuronalen Netzen. Diploma, Technische Universität München, 91(1).\n",
    "            </cite>\n",
    "    </center></p>\n",
    "\n",
    "<p>In contrast to most other recurrent architectures like the Elman RNN, the <i>LSTM</i> is a bit more complex, but equally more powerful:\n",
    "    <center>\n",
    "        \\begin{equation}\n",
    "            \\begin{split}\n",
    "                i_{t} &= \\sigma{\\left(W_{ii}x_{t} + b_{ii} + W_{hi}h_{t-1} + b_{hi}\\right)} \\\\\n",
    "                \\color{red}{f_{t}} &\\color{red}{= \\sigma{\\left(W_{if}x_{t} + b_{if} + W_{hf}h_{t-1} + b_{hf}\\right)}} \\\\\n",
    "                g_{t} &= \\tanh{\\left(W_{ig}x_{t} + b_{ig} + W_{hg}h_{t-1} + b_{hg}\\right)} \\\\\n",
    "                o_{t} &= \\sigma{\\left(W_{io}x_{t} + b_{io} + W_{ho}h_{t-1} + b_{ho}\\right)} \\\\\n",
    "                c_{t} &= \\color{red}{f_{t}\\odot{}}c_{t-1} + i_{t}\\odot{}g_{t} \\\\\n",
    "                h_{t} &= o_{t}\\odot{}\\tanh{\\left(c_{t}\\right)}\n",
    "            \\end{split}\n",
    "        \\end{equation}\n",
    "    </center></p>\n",
    "\n",
    "<p>We are using the implementation provided by PyTorch, more information may be found in the <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM\">official documentation</a>. It has to be noted, the the <i>original</i> formulation did <i>not</i> contain an additional <i>forget gate</i> $f_{t}$ (see equations above), as this completely destroys the <i>constant error carousel</i> – it was introduced by <a href=\"https://www.mitpressjournals.org/doi/pdfplus/10.1162/089976600300015015\">Gers et al.</a>:\n",
    "<center>\n",
    "    <cite>Gers, F.A., Schmidhuber, J. and Cummins, F., 1999. Learning to forget: Continual prediction with LSTM.\n",
    "    </cite>\n",
    "</center></p>\n",
    "<p>Nonetheless, for <i>some</i> tasks, the forget gate seems to provide a useful addition. Hence, in this exercise you'll be tasked with activating the <i>forget gate</i> and interpreting the results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117707f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.1. [13 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a class <code>LSTM</code> with the following architecture:</li>\n",
    "    </ul>\n",
    "    <table style=\"text-align:center;vertical-align:middle\">\n",
    "        <th>Position</th>\n",
    "        <th>Element</th>\n",
    "        <th>Comment</th>\n",
    "        <tr>\n",
    "            <td>0</td>\n",
    "            <td>input</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\">LSTM</a></td>\n",
    "            <td>$48$ memory cells and a configurable initial forget gate bias (specified as an argument to <code>__init__</code>)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\">fully connected</a></td>\n",
    "            <td><code>num_targets</code> output features (as specified during the data set creation)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td>output</td>\n",
    "            <td>-</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <ul>\n",
    "        <li>Train an <code>LSTM</code> network for $12$ epochs (use <code>Adam</code> optimizer with a learning rate of $0.01$), print the training accuracy as well as the loss per epoch and report the final test set loss and accuracy as well as the number of trainable parameters. Use an initial forget gate bias of $0.0$. Do you expect the resulting performance?</li>\n",
    "        <li>Hint: You now want to use the forget gate, so do <i>not</i> disable the gradient computation!</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ce94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581aefb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.2. [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a <i>fresh</i> instance of <code>LSTM</code> and collect its gradients w.r.t. the network input using the latch sequence training set.</li>\n",
    "        <li>Visualize the collected gradients accordingly. What do you observe? Comment on your results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46ba25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd3141",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6dcbe1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.3. [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Use the already trained <code>LSTM</code> instance from the beginning of this section for collecting its gradients w.r.t. the network input using the latch sequence training set, i.e., collect gradients one more time but now using the trained model.</li>\n",
    "        <li>Visualize (in a single plot) the the newly collected gradients together with the previous gradients of the <i>freshly</i> created <code>LSTM</code> instance. What do you observe? Comment on your results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9610c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1393d9",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b285b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.4. [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Train an <code>LSTM</code> network for $12$ epochs (use <code>Adam</code> optimizer with a learning rate of $0.01$), print the training accuracy as well as the loss per epoch and report the final test set loss and accuracy as well as the number of trainable parameters. Use an initial forget gate bias of $1.0$. Do you expect the resulting performance?</li>\n",
    "        <li>Analogous to above, visualize (in a single plot) the gradients of both a <i>freshly</i> created <code>LSTM</code> instance and the trained model. What do you observe? Comment on your results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440dad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4bfb1",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1067ee20f23cf75b48768bdb5f7ec1d4c21e1831c972d070ed1f98bb55bb7e57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
