{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Azat Vakhitov | 12148222 | 09.01.2023, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI I</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 5 &ndash; Your First Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> Brandstetter, Sch√§fl, Winter, Schl√ºter, Parada-Cabaleiro, Sch√∂rgenhumer<br>\n",
    "<b>Date:</b> 05-12-2022\n",
    "\n",
    "This file is part of the \"Hands-on AI I\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which require your contribution (in form of code, plain text, ...). Most/All of the supplied functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> When specifying a seed for the sources of randomness, use the <code>u5.set_seed(seed=XYZ)</code> function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.9 (‚úì)\n",
      "Installed numpy version: 1.21.5 (‚úì)\n",
      "Installed pandas version: 1.4.2 (‚úì)\n",
      "Installed scikit-learn version: 1.0.2 (‚úì)\n",
      "Installed matplotlib version: 3.5.1 (‚úì)\n",
      "Installed seaborn version: 0.11.2 (‚úì)\n",
      "Installed scipy version: 1.7.3 (‚úì)\n",
      "Installed torch version: 1.13.0 (‚úì)\n",
      "Installed tqdm version: 4.64.0 (‚úì)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .output_png {\n",
       "            display: table-cell;\n",
       "            text-align: center;\n",
       "            vertical-align: middle;\n",
       "        }\n",
       "        .jp-RenderedImage {\n",
       "            display: table-cell;\n",
       "            text-align: center;\n",
       "            vertical-align: middle;\n",
       "        }\n",
       "    </style>\n",
       "    <p>Setting up notebook ... finished.</p>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required packages and the u5_utils file\n",
    "import u5_utils as u5\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "u5.check_module_versions()\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u5.setup_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 1</h2>\n",
    "\n",
    "Following the instructions given in the lecture notebook, perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.1. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a dataset by sampling $100$ values for $x$ and computing $y = -0.5 - 2.35 \\cdot x + 0.7 \\cdot x^2$, then adding some noise (variance $0.45$) to $y$. For this, consider the function <code>get_dataset()</code> from <code>u5_utils.py</code>.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_dataset() got an unexpected keyword argument 'x_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m u5\u001b[38;5;241m.\u001b[39mset_seed(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Generate dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mu5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_variance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_dataset() got an unexpected keyword argument 'x_min'"
     ]
    }
   ],
   "source": [
    "u5.set_seed(seed=22)\n",
    "#ùë¶=0.241+0.422‚ãÖùë•\n",
    "# Generate dataset\n",
    "dataset = u5.get_dataset(\n",
    "    num_pairs=100,  # number of data points to create\n",
    "    coefficients=(-0.5, -2.35, 0.7),  # coefficients for computing y from x\n",
    "    variance=0.45  # variance of the noise of the observations\n",
    ")\n",
    "\n",
    "# Display the dataset.\n",
    "sns.scatterplot(data=dataset, x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.2. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define a model with three coefficients $b$, $w_1$ and $w_2$ of the form $\\hat{y} = b + w_1 \\cdot x + w_2 \\cdot x^2$ (polynomial of degree $2$), with which we will approximate the relation between $x$ and $y$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.3. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define <b>Mean Squared Error (MSE)</b> as the loss function.</li>\n",
    "        <li>Using the most elegant method from the lecture, find the optimum $b$, $w_1$ and $w_2$ minimizing the loss and print out the loss.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1.4. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Plot the resulting linear model along with the data.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.1. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a dataset with two variables, $x$ and $y$, where $y = 1$ for $x > 0.55$ and $y = 0$ otherwise. For this, use the function <code>get_dataset_logistic()</code> from <code>u5_utils.py</code>, with $100$ data points and variance $0.15$. Then, plot the dataset.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.2. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define a <b>logistic</b> model with two coefficients $b$ and $w$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.3. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Define a suitable loss and corresponding gradient function.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.4. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Visualize the loss landscape <b>including the gradient arrows</b>. For this, use the function <code>plot_loss_landscape()</code> from <code>u5_utils.py</code> for $b$ and $w$ in a range from $-10$ to $10$ each.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.5. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Optimize the parameters $b$ and $w$ of the model using Gradient Descent. For this, use the function <code>plot_gradient_descent()</code> from <code>u5_utils.py</code>.</li>\n",
    "        <li>As a starting point, use $b=7.5$, $w=3$, use $500$ iterations (steps), a step size (learning rate) of $0.9$ and a momentum of $0.0$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.6. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Print out the found, optimized values for $b$ and $w$ as well as the loss.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 3</h2>\n",
    "\n",
    "Continuing with the dataset and the logistic model from Exercise 2, we will now replace the loss function with the Mean Squared Error (MSE) loss to see what happens. Since we also need its gradient, we prepared this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss and gradient of MSE loss for a logistic model\n",
    "def loss(dataset, b, w):\n",
    "    predictions = model(dataset.x.values, b, w)\n",
    "    targets = dataset.y.values\n",
    "    return np.mean((predictions - targets)**2, axis=-1)\n",
    "\n",
    "def loss_grad(dataset, b, w):\n",
    "    predictions = model(dataset.x.values, b, w)\n",
    "    targets = dataset.y.values\n",
    "    delta = 2 * (predictions - targets)  # grad of (predictions - targets)**2\n",
    "    delta *= predictions * (1 - predictions)  # grad of sigmoid()\n",
    "    b_grad = np.mean(delta, axis=-1)\n",
    "    w_grad = np.mean(dataset.x.values * delta, axis=-1)\n",
    "    return b_grad, w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.1. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Visualize the loss landscape <b>including the gradient arrows</b>. For this, use the function <code>plot_loss_landscape()</code> from <code>u5_utils.py</code>, for $b$ and $w$ in a range from $-10$ to $10$ each.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.2. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Optimize the parameters $b$ and $w$ of the model using Gradient Descent. Again, use the function <code>plot_gradient_descent()</code> from <code>u5_utils.py</code> to perform the optimization.</li>\n",
    "        <li>Again, start from $b=7.5$, $w=3$, do $500$ iterations (steps), use a step size (learning rate) of $0.9$ and a momentum of $0.0$.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.3. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Print out the found, optimized values for $b$ and $w$ as well as the loss.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.4. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Redo the optimization but change the starting position for $b$ and $w$ such that it finds a loss smaller than $0.1$.\n",
    "        <li>Hint: Look at the loss landscape to pick the position. Remember that gradient descent can be compared to a ball rolling down a hill.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.5. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Print out the found, optimized values for $b$ and $w$ as well as the loss.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 4</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.1. [5 Points]</b>\n",
    "    <ul>\n",
    "        <li>Considering again the dataset from Exercises 2 and 3, implement a logistic regression model in <code>PyTorch</code> and define a suitable loss function and a gradient descent optimizer.</li>\n",
    "        <li>For the optimizer, set learning rate = $0.15$ and momentum = $0.0$.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 4.2. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Run the optimization (get predictions, calculate loss, compute loss gradient, perform update step) until the loss is $\\leq 0.3$.</li>\n",
    "        <li>Print out the final loss as well as the initial (randomly chosen) $b$ (bias) and $w$ (weight) and those achieved after optimization.</li>\n",
    "        <li>Hint: Follow the lecture notebook to convert the input data and target to torch tensors for the optimization.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 5</h2>\n",
    "\n",
    "With the dataset defined and plotted below (as given in the next code cell), perform the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "u5.set_seed(seed=22)\n",
    "\n",
    "# create dataset consisting of random (x, y) pairs\n",
    "dataset = u5.get_dataset_blob2d(\n",
    "    num_samples=700,  # number of data points to create\n",
    "    variance=0.05,    # variance of the noise of the observations    \n",
    "    threshold=1.1,    # radius of the circle for class 1\n",
    "    offset=(1, 0.1),  # center of the circle\n",
    ")\n",
    "\n",
    "# display the dataset\n",
    "sns.scatterplot(data=dataset, x=\"x1\", y=\"x2\", hue=\"y\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> For reproducibility, for each of the following tasks, set a fixed seed (seed=22).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.1. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a similar model as in Exercise 4, i.e., a logistic regression model in <code>PyTorch</code>, considering Binary Cross-Entropy (BCE) as the loss funtion and Gradient Descent as the optimization method.</li>\n",
    "        <li>For the optimization, set learning rate = $0.15$ and momentum = $0.9$. Then, run the optimization for $4000$ steps and print out the loss.</li>\n",
    "        <li>Hint: This dataset is two-dimensional, i.e., it has $2$ input features called \"x1\" and \"x2\". They need to be combined into a single torch tensor for the optimization. You can concatenate two torch vectors into a matrix with <a href=\"https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack\"><code>torch.stack(..., dim=1)</code></a>, or you can directly assign the entire feature vector matrix <code>dataset[[\"x1\", \"x2\"]].values</code> to a tensor.</li>\n",
    "        <li>Hint: Your model will now need $2$ input nodes to handle $2$ features but still $1$ output node for binary classification.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> Display the predictions to answer the following questions. The following code assumes that the predictions from above are stored into a variable called <code>preds</code>. Adapt this if needed (or simply name your predictions above to <code>preds</code>).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"prediction\"] = (preds.detach().sigmoid().numpy() > 0.5).astype(float)\n",
    "sns.scatterplot(data=dataset, x=\"x1\", y=\"x2\", hue=\"prediction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.2. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Does the model manage to separate the 2 classes?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.3. [2 Points]</b>\n",
    "    <ul>\n",
    "        <li>Does it look like it tried to separate them with a straight line?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 5.4. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a more complex model by adding a hidden layer with two nodes (the loss function and the Gradient Descent optimizer parameters remain the same).</li>\n",
    "        <li>Run the optimization for $4000$ steps, print out the loss and visualize the predictions of the model with the code provided by us.</li>\n",
    "        <li>Increase the number of nodes in the hidden layer, retrain the model and visualize the predictions until the classes are separated properly.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Important:</b> Display the predictions with the more complex model. The following code assumes that the predictions from above are stored into a variable called <code>preds</code>. Adapt this if needed (or simply name your predictions above to <code>preds</code>).\n",
    "    Repeat step <b>5.4.</b> until the model properly separates the two classes (a visual check is sufficient).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"prediction\"] = (preds.detach().sigmoid().numpy() > 0.5).astype(float)\n",
    "sns.scatterplot(data=dataset, x=\"x1\", y=\"x2\", hue=\"prediction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 6</h2>\n",
    "\n",
    "Following the instruction given in the lecture notebook, perform the following tasks, but this time, considering the <b>Fashion-MNIST</b> dataset. More information about the dataset can be found in this publication:\n",
    "\n",
    "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf (2017). [arXiv:1708.07747](https://arxiv.org/abs/1708.07747)\n",
    "\n",
    "To load the Fashion-MNIST dataset and take a look at a preview of $5$ samples, run the cell below (provided by us).\n",
    "\n",
    "**Important:** The first time you run this, it will download the dataset. You may see a <code>UserWarning: The given NumPy array is not writeable</code>. This can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset with custom batch size\n",
    "train_loader, valid_loader, test_loader = u5.get_dataset_mnist(\n",
    "    root=\"resources\",\n",
    "    variant=\"FashionMNIST\",\n",
    "    batch_size=8,\n",
    "    valid_size=0.10\n",
    ")\n",
    "\n",
    "# load the first batch of data (set seed for reproducibility)\n",
    "u5.set_seed(seed=22)\n",
    "images, labels = next(iter(train_loader))\n",
    "# transform the image shapes for visualization purposes\n",
    "images = np.concatenate([img.squeeze() for img in images], axis=1)\n",
    "\n",
    "# display the first batch of data\n",
    "with plt.style.context({\"axes.grid\": False, \"xtick.bottom\": False}):\n",
    "    plt.imshow(images, cmap=\"binary\")\n",
    "    plt.xticks(14 + np.arange(len(labels)) * 28, labels.numpy())\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 6.1. [4 Points]</b>\n",
    "    <ul>\n",
    "        <li>Like the MNIST dataset, the Fashion-MNIST has input images of $28 \\times 28 = 784$ pixels and $10$ classes. Considering this, define a Neural Network model with one hidden layer consisting of $16$ nodes.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 6.2. [7 Points]</b>\n",
    "    <ul>\n",
    "        <li>Reload the dataset considering $20\\%$ of the samples as validation set and train the model with the following hyperparameters: batch size = $32$, iterations = $4$, momentum = $0.9$ and learning rate = $0.05$. Use the function <code>run_gradient_descent()</code> from <code>u5_utils.py</code>.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22) before the running optimization.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 6.3. [3 Points]</b>\n",
    "    <ul>\n",
    "        <li>Plot the training and validation losses and print out the accuracy on the test set.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 6.4. [13 Points]</b>\n",
    "    <ul>\n",
    "        <li>Keeping the partitioning as before, can you optimize the model in order to achieve an accuracy on the test set > $86\\%$ (there are various ways to achieve this)? Plot the training and validation losses to show that your model does not overfit to the training data and print out the accuracy on test to show that it is better than $86\\%$.</li>\n",
    "        <li>For reproducibility, set a fixed seed (seed=22). It must be set both before defining the model and before the optimization (in which random samples are drawn). Otherwise, changes in the model would change the train/validation split samples, since both steps use randomization.</li>\n",
    "        <li>Hint: For optimization, vary the following hyperparameters: batch size, iterations, learning rate, momentum, number of layers, number of nodes and type of non-linearity. You may also try randomly flipping training images to perform data augmentation (provided by <code>get_dataset_mnist()</code>). Do <b>not</b> vary the validation set size, as that would increase the training set size.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
